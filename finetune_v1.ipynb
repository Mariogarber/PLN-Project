{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8974bf8b",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-29 10:09:49--  https://raw.githubusercontent.com/Mariogarber/PLN-Project/main/requirements.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 147 [text/plain]\n",
      "Saving to: â€˜requirements.txtâ€™\n",
      "\n",
      "requirements.txt    100%[===================>]     147  --.-KB/s    in 0s      \n",
      "\n",
      "2025-11-29 10:09:49 (4.37 MB/s) - â€˜requirements.txtâ€™ saved [147/147]\n",
      "\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.57.2)\n",
      "Collecting dataset (from -r requirements.txt (line 5))\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (18.1.0)\n",
      "Collecting sacrebleu (from -r requirements.txt (line 7))\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge_score (from -r requirements.txt (line 8))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Collecting bert-score (from -r requirements.txt (line 10))\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.2.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.12.0)\n",
      "Collecting bitsandbytes (from -r requirements.txt (line 16))\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 4)) (4.67.1)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset->-r requirements.txt (line 5))\n",
      "  Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from dataset->-r requirements.txt (line 5)) (1.17.2)\n",
      "Collecting banal>=1.0.1 (from dataset->-r requirements.txt (line 5))\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting portalocker (from sacrebleu->-r requirements.txt (line 7))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r requirements.txt (line 7)) (0.9.0)\n",
      "Collecting colorama (from sacrebleu->-r requirements.txt (line 7))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score->-r requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 9)) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 12)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 12)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 12)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 12)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 12)) (3.2.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft->-r requirements.txt (line 14)) (5.9.5)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=0.6.2->dataset->-r requirements.txt (line 5)) (1.3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset->-r requirements.txt (line 5)) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2025.11.12)\n",
      "Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e21336a16d61baf9191bde6514c9245614b29aa79aa9d6704c59a2ecca221b8c\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: banal, sqlalchemy, portalocker, colorama, sacrebleu, rouge_score, dataset, bitsandbytes, bert-score\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.44\n",
      "    Uninstalling SQLAlchemy-2.0.44:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.44\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.19.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed banal-1.0.6 bert-score-0.3.13 bitsandbytes-0.48.2 colorama-0.4.6 dataset-1.6.2 portalocker-3.2.0 rouge_score-0.1.2 sacrebleu-2.5.1 sqlalchemy-1.4.54\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Mariogarber/PLN-Project/main/requirements.txt -O requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c554bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2ec49c915f4ecfbdf3364962cd37c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb399ff3f644410bbf9e6fe9af3fa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9167a6f3b04bca864268686d9797e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0008e3fbb7e24d46bb3a0fae05208478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958e826c2b864fbdaa0432e90bf2e620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f8c37df18e4fe99b6ddb9a049c9590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from bert_score import score as bertscore\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "\n",
    "TOKENIZER = T5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf871ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicNonToxicDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para tareas de detoxificaciÃ³n de texto.\n",
    "    Espera varios archivos .parquet con columnas:\n",
    "        - 'toxic_sentences': texto tÃ³xico original (string)\n",
    "        - 'neutral_sentences': versiÃ³n detoxificada del texto (string)\n",
    "    Cada fila es un par de oraciones (tÃ³xica -> neutral).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parquet_paths: List[str]):\n",
    "        # Cargar y mezclar todos los parquets\n",
    "        dfs = []\n",
    "        for p in parquet_paths:\n",
    "            df = pd.read_parquet(p)\n",
    "            lang = p.split('_')[-1].split('.parquet')[0]\n",
    "            df['language'] = lang\n",
    "            dfs.append(df)\n",
    "        self.data = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Limpiar datos: remover filas con valores nulos\n",
    "        self.data = self.data.dropna(subset=[\"toxic_sentence\", \"neutral_sentence\"])\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "\n",
    "        # Limpiar espacios en blanco\n",
    "        self.data[\"toxic_sentence\"] = self.data[\"toxic_sentence\"].str.strip()\n",
    "        self.data[\"neutral_sentence\"] = self.data[\"neutral_sentence\"].str.strip()\n",
    "\n",
    "        # Mezclar datos\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Asegurar tipos correctos\n",
    "        self.data[\"toxic_sentence\"] = self.data[\"toxic_sentence\"].astype(str)\n",
    "        self.data[\"neutral_sentence\"] = self.data[\"neutral_sentence\"].astype(str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"toxic_sentence\": self.data.iloc[idx][\"toxic_sentence\"],\n",
    "            \"neutral_sentence\": self.data.iloc[idx][\"neutral_sentence\"],\n",
    "            \"language\": self.data.iloc[idx][\"language\"]\n",
    "        }\n",
    "    \n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24872fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load multilingual detoxification dataset directly as HuggingFace Dataset with train/test split\"\"\"\n",
    "    lang = ['en', 'am', 'ar', 'de', 'es', 'hi', 'ru', 'uk', 'zh']\n",
    "    \n",
    "    url_template = \"https://raw.githubusercontent.com/Mariogarber/PLN-Project/main/dataset/toxic_nontoxic/multilingual_paradetox_{}.parquet\"\n",
    "    \n",
    "    datasets = []\n",
    "    for lang_code in lang:\n",
    "        url = url_template.format(lang_code)\n",
    "        try:\n",
    "            # Load directly as HuggingFace dataset\n",
    "            ds = load_dataset('parquet', data_files=url, split='train')\n",
    "            # Add language column\n",
    "            ds = ds.add_column(\"language\", [lang_code] * len(ds))\n",
    "            datasets.append(ds)\n",
    "            LOGGER.info(f\"Loaded {len(ds)} samples for language: {lang_code}\")\n",
    "        except Exception as e:\n",
    "            LOGGER.warning(f\"Failed to load data for {lang_code}: {e}\")\n",
    "    \n",
    "    # Concatenate all language datasets\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "    \n",
    "    # Shuffle the combined dataset\n",
    "    combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "    \n",
    "    # Split into train/test (80/20 split)\n",
    "    train_test_split = combined_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    # Create DatasetDict with train and test splits\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'test': train_test_split['test']\n",
    "    })\n",
    "    \n",
    "    LOGGER.info(f\"Total dataset size: {len(combined_dataset)} samples across {len(datasets)} languages\")\n",
    "    LOGGER.info(f\"Train split: {len(dataset_dict['train'])} samples\")\n",
    "    LOGGER.info(f\"Test split: {len(dataset_dict['test'])} samples\")\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "def load_forbidden_words():\n",
    "    \n",
    "    lang = ['en', 'am', 'ar', 'de', 'es', 'hi', 'ru', 'uk', 'zh']\n",
    "    \n",
    "    url_template = \"https://raw.githubusercontent.com/Mariogarber/PLN-Project/main/dataset/multilingual_toxic_spans/{}.parquet\"\n",
    "    \n",
    "    datasets = []\n",
    "    for lang_code in lang:\n",
    "        url = url_template.format(lang_code)\n",
    "        try:\n",
    "            # Load directly as HuggingFace dataset\n",
    "            ds = load_dataset('parquet', data_files=url, split='train')\n",
    "            # Add language column\n",
    "            ds = ds.add_column(\"language\", [lang_code] * len(ds))\n",
    "            datasets.append(ds)\n",
    "            LOGGER.info(f\"Loaded {len(ds)} samples for language: {lang_code}\")\n",
    "        except Exception as e:\n",
    "            LOGGER.warning(f\"Failed to load data for {lang_code}: {e}\")\n",
    "\n",
    "    # Concatenate all language datasets\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "    LOGGER.info(f\"Total forbidden words dataset size: {len(combined_dataset)} samples across {len(datasets)} languages\")\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3b9c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8b9f341ffb4a46bed1d60270b8e70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/51.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ae4d30ab8c4ed9acb9f6fd4c50dbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 991 samples for language: en\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10b6504999542a3beeef8678506887a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d708f9cd1834f44a21be67b1539c64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 995 samples for language: am\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a002cf573a544258534a22a76683ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/79.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5373a430cfb4964bacbb6f10d0359e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 990 samples for language: ar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11254f5af62c4afa9cda4098e70c4179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/96.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da550135339444ca79a0a98a1c7e2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 970 samples for language: de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ee606138cb466389640a829373d458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/61.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff38ed5a05b42ec8606ce1a7d147c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 987 samples for language: es\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd8bf7090394e0a857df3643b3efc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/73.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016741d5dcd24a9292d3a6d5a0480c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 992 samples for language: hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186696f5f74840ccb985a51a34175b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e11f203033e4a33b5be35e64fb24690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/86.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee17e854af744a6a8828ebbb534ba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 999 samples for language: ru\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdeda5489b14c62b3f536851225d0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/68.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4c8f256bae45909275c7843b0af815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 943 samples for language: uk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f039032aba476cb0e0676b183a4028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ac718e43be40df89a494bcd947c08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 921 samples for language: zh\n",
      "INFO:__main__:Total forbidden words dataset size: 8788 samples across 9 languages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6501e95adba3459985ae76d473f8cb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0203c101b6e94ef7a328e8f06857f3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: en\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d040d0630a444599978034bb7d12472d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/77.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64b6c0d746845199a5aac070c55ef1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: am\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decf4dde3a314b46a4d1774888a1d8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/53.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7aa3c67eeab4e5296e3f76030655d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: ar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e653a96beb42538fe37638e2850a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/62.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330c33dd43c744879595c566fa92d6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7043e02a5e413491b71ea07fc9efa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/44.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8eb1ca87584842bdc83e0fc54325b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: es\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1fa503b3e14a74a1f0acfbf59b0d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/48.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a0890bac074497bad00f65de7945aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087156124e4d4a88a29f5a5fa94d7550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/53.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e32f55405694d429b243b17064ca212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: ru\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092ae4930521418ab8b5b2349a99be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/48.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e50ee1108a44ec290b667c3a76aa365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: uk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f44fd9962ec47c1b92b367d9fa9ce0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/59.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d686ecccf3406c89305ec921141106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: zh\n",
      "INFO:__main__:Total dataset size: 3600 samples across 9 languages\n",
      "INFO:__main__:Train split: 2880 samples\n",
      "INFO:__main__:Test split: 720 samples\n"
     ]
    }
   ],
   "source": [
    "toxic_spans_dataset = load_forbidden_words()\n",
    "\n",
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_words = toxic_spans_dataset['Negative Connotations']\n",
    "\n",
    "# Convert to a flat list of unique words\n",
    "forbidden_words_list = []\n",
    "for words in forbidden_words:\n",
    "    if words:  # Check if not None or empty\n",
    "        # Split by common separators and add to list\n",
    "        if isinstance(words, str):\n",
    "            # Split by comma, semicolon, or other separators\n",
    "            word_parts = words.replace(';', ',').replace('|', ',').split(',')\n",
    "            for word in word_parts:\n",
    "                word = word.strip().lower()\n",
    "                if word and word not in forbidden_words_list:\n",
    "                    forbidden_words_list.append(word)\n",
    "\n",
    "forbidden_words_list = list(set(forbidden_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da6ad8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Custom loss function with forbidden words penalty created!\n",
      "ğŸ“ Forbidden words list contains 8869 words\n",
      "ğŸ¯ Example forbidden words: ['á‹²á‹³', 'Ù‚Ø¯ÙŠÙ…', 'à¤¬à¤¹à¤¨ à¤¬à¥‡à¤Ÿà¤¿à¤¯à¥‹', 'Ğ¶ĞµÑÑ‚Ğ¾ĞºĞ°Ñ', 'spoiled', 'golpista', 'áˆ›áˆáˆªá‹«', 'áˆ°á‹­áŒ£áŠ‘', 'Ø³Ø±Ø·Ø§Ù†', 'dÃ¼nnschiss']...\n",
      "âš–ï¸ Penalty weight: 2.0\n",
      "\n",
      "ğŸ”„ To use the custom trainer:\n",
      "custom_trainer = CustomLossTrainer(\n",
      "    model=model_lora,\n",
      "    args=training_args,\n",
      "    train_dataset=tokenized_dataset['train'],\n",
      "    eval_dataset=tokenized_dataset['eval'],\n",
      "    data_collator=data_collator,\n",
      "    tokenizer=TOKENIZER,\n",
      "    forbidden_words=FORBIDDEN_WORDS,\n",
      "    penalty_weight=PENALTY_WEIGHT\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ğŸš« CUSTOM LOSS FUNCTION WITH FORBIDDEN WORDS PENALTY\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "import re\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "class ForbiddenLossTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer with forbidden words penalty in loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, forbidden_tokens=None, penalty_weight=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Default forbidden words list (you can customize this)\n",
    "        if forbidden_tokens is None:\n",
    "            self.forbidden_words = [\n",
    "                \"hate\", \"stupid\", \"idiot\", \"moron\", \"dumb\", \"loser\", \n",
    "                \"worthless\", \"pathetic\", \"disgusting\", \"terrible\",\n",
    "                \"fuck\", \"shit\", \"damn\", \"bitch\", \"asshole\",\n",
    "                \"retard\",\n",
    "            ]\n",
    "        else:\n",
    "            self.forbidden_words = forbidden_words\n",
    "            \n",
    "        self.penalty_weight = penalty_weight\n",
    "        \n",
    "        self.forbidden_tokens = forbidden_tokens\n",
    "        \n",
    "        print(f\"ğŸš« Custom loss initialized with {len(self.forbidden_words)} forbidden words\")\n",
    "        print(f\"âš–ï¸ Penalty weight: {penalty_weight}\")\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom loss that penalizes the model for assigning probability\n",
    "        to forbidden tokens. The penalty IS differentiable.\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        original_loss = outputs.loss\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab]\n",
    "\n",
    "        # If not training, return original loss\n",
    "        if not self.model.training:\n",
    "            return (original_loss, outputs) if return_outputs else original_loss\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)  # [B, L, V]\n",
    "\n",
    "        device = logits.device\n",
    "        vocab_size = logits.size(-1)\n",
    "\n",
    "        # Creamos una mÃ¡scara [V] donde forbidden_tokens tienen 1\n",
    "        forbidden_mask = torch.zeros(vocab_size, device=device)\n",
    "        forbidden_mask[self.forbidden_tokens] = 1  # <-- aquÃ­ debe ser una lista de IDs\n",
    "\n",
    "        # expandimos a [B, L, V] mediante broadcast\n",
    "        forbidden_mask = forbidden_mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Penalizamos la probabilidad asignada a esos tokens\n",
    "        forbidden_prob = (probs * forbidden_mask).sum(dim=-1)  # [B, L]\n",
    "\n",
    "        # Ignorar posiciones donde label = -100 (padding en training)\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs[\"labels\"]\n",
    "            valid_mask = (labels != -100).float()  # [B, L]\n",
    "            forbidden_prob = forbidden_prob * valid_mask\n",
    "\n",
    "        # PenalizaciÃ³n media por batch\n",
    "        penalty = forbidden_prob.mean()\n",
    "\n",
    "        total_loss = (1 - self.penalty_weight) * original_loss + self.penalty_weight * penalty\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "    # def _calculate_forbidden_penalty(self, predictions, labels=None):\n",
    "    #     \"\"\"\n",
    "    #     Calculate penalty based on forbidden words in generated text\n",
    "    #     \"\"\"\n",
    "    #     batch_size = predictions.shape[0]\n",
    "    #     total_penalty = 0.0\n",
    "        \n",
    "    #     for i in range(batch_size):\n",
    "    #         # Decode the prediction to text\n",
    "    #         pred_ids = predictions[i]\n",
    "            \n",
    "    #         # Remove padding tokens and special tokens for cleaner text\n",
    "    #         # Filter out -100 (ignored tokens) and padding\n",
    "    #         valid_pred_ids = pred_ids[pred_ids >= 0]  # Remove -100 labels\n",
    "    #         try:                \n",
    "    #             # Count tokens that are in forbidden tokens\n",
    "    #             forbidden_count = sum(1 for token_id in valid_pred_ids if token_id.item() in self.forbidden_tokens)\n",
    "                \n",
    "    #             # Add penalty proportional to number of forbidden words found\n",
    "    #             if forbidden_count > 0:\n",
    "    #                 total_penalty += forbidden_count\n",
    "                    \n",
    "    #         except Exception as e:\n",
    "    #             # If decoding fails, skip this sample\n",
    "    #             continue\n",
    "        \n",
    "    #     # Convert to tensor on the same device as the model\n",
    "    #     device = predictions.device\n",
    "    #     penalty_tensor = torch.tensor(total_penalty, dtype=torch.float32, device=device)\n",
    "        \n",
    "    #     return penalty_tensor\n",
    "\n",
    "# Example usage configuration\n",
    "FORBIDDEN_WORDS = forbidden_words_list\n",
    "PENALTY_WEIGHT = 2.0  # How much to penalize forbidden words (adjust as needed)\n",
    "\n",
    "print(\"âœ… Custom loss function with forbidden words penalty created!\")\n",
    "print(f\"ğŸ“ Forbidden words list contains {len(FORBIDDEN_WORDS)} words\")\n",
    "print(f\"ğŸ¯ Example forbidden words: {FORBIDDEN_WORDS[:10]}...\")\n",
    "print(f\"âš–ï¸ Penalty weight: {PENALTY_WEIGHT}\")\n",
    "print(\"\\nğŸ”„ To use the custom trainer:\")\n",
    "print(\"custom_trainer = CustomLossTrainer(\")\n",
    "print(\"    model=model_lora,\")\n",
    "print(\"    args=training_args,\")\n",
    "print(\"    train_dataset=tokenized_dataset['train'],\")\n",
    "print(\"    eval_dataset=tokenized_dataset['eval'],\")\n",
    "print(\"    data_collator=data_collator,\")\n",
    "print(\"    tokenizer=TOKENIZER,\")\n",
    "print(\"    forbidden_words=FORBIDDEN_WORDS,\")\n",
    "print(\"    penalty_weight=PENALTY_WEIGHT\")\n",
    "print(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d866a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QLoRA model created with bfloat16 configuration\n",
      "ğŸ“Š Model type: <class 'peft.peft_model.PeftModelForSeq2SeqLM'>\n",
      "ğŸ”§ Compute dtype: torch.bfloat16\n",
      "ğŸ¯ Trainable parameters: 884,736 / 503,069,952 (0.18%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Create QLoRA configuration with bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=False,        # Disabled for stability\n",
    ")\n",
    "\n",
    "# Load and quantize the model\n",
    "model_quantized = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/mt5-base\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model_quantized = prepare_model_for_kbit_training(model_quantized)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank\n",
    "    lora_alpha=16,          # Alpha parameter\n",
    "    target_modules=[\"q\", \"v\", \"k\", \"o\"],  # Target attention modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to the quantized model\n",
    "model_lora = get_peft_model(model_quantized, lora_config)\n",
    "\n",
    "print(\"âœ… QLoRA model created with bfloat16 configuration\")\n",
    "print(f\"ğŸ“Š Model type: {type(model_lora)}\")\n",
    "print(f\"ğŸ”§ Compute dtype: {bnb_config.bnb_4bit_compute_dtype}\")\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "print(f\"ğŸ¯ Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82b19498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Tokenizing dataset...\n",
      "âœ… Tokenized dataset created with 2880 samples\n"
     ]
    }
   ],
   "source": [
    "# Custom callback to debug training issues\n",
    "from transformers import TrainerCallback, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 10 == 0:  # Log every 10 steps\n",
    "            logs = kwargs.get('logs', {})\n",
    "            loss = logs.get('train_loss', 'N/A')\n",
    "            \n",
    "            print(f\"Step {state.global_step}: Loss = {loss}\")\n",
    "            \n",
    "            # Check for problematic loss values\n",
    "            if isinstance(loss, (int, float)):\n",
    "                if loss == 0:\n",
    "                    print(\"âš ï¸  WARNING: Loss is exactly 0 - check your data preprocessing!\")\n",
    "                elif loss < 1e-6:\n",
    "                    print(\"âš ï¸  WARNING: Loss is extremely small - possible numerical issues!\")\n",
    "                elif np.isnan(loss):\n",
    "                    print(\"âš ï¸  ERROR: Loss is NaN - stopping training recommended!\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "\n",
    "# First, create the tokenized dataset\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the dataset for T5 training\"\"\"\n",
    "    # Create input text with task prefix\n",
    "    input_texts = [f\"detoxify: {text}\" for text in examples['toxic_sentence']]\n",
    "    target_texts = examples['neutral_sentence']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = TOKENIZER(\n",
    "        input_texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False  # We'll pad in the data collator\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with TOKENIZER.as_target_tokenizer():\n",
    "        labels = TOKENIZER(\n",
    "            target_texts,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to the train dataset\n",
    "print(\"ğŸ”„ Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenized dataset created with {len(tokenized_dataset)} samples\")\n",
    "\n",
    "forbidden_word_tokens = []\n",
    "for word in forbidden_words_list:\n",
    "    token_ids = TOKENIZER.encode(word, add_special_tokens=False)\n",
    "    forbidden_word_tokens.extend(token_ids)\n",
    "\n",
    "# Split train into train and eval\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'eval': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Add the debug callback to trainer\n",
    "debug_callback = DebugCallback()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mt5-detoxify-qlora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=200,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    TOKENIZER, \n",
    "    model=model_lora,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36be7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creating custom trainer with forbidden words penalty...\n",
      "ğŸš« Custom loss initialized with 8788 forbidden words\n",
      "âš–ï¸ Penalty weight: 0.15\n",
      "âœ… Custom trainer created successfully!\n",
      "ğŸš« Monitoring 8869 forbidden words\n",
      "âš–ï¸ Penalty weight: 2.0\n",
      "\n",
      "ğŸš€ Ready to train with custom loss! Run: custom_trainer.train()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-891937490.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ForbiddenLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ CREATE CUSTOM TRAINER WITH FORBIDDEN WORDS PENALTY\n",
    "\n",
    "# First, let's create a custom trainer using your existing model\n",
    "print(\"ğŸ”§ Creating custom trainer with forbidden words penalty...\")\n",
    "\n",
    "# Create custom trainer with forbidden words penalty\n",
    "custom_trainer = ForbiddenLossTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['eval'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=TOKENIZER,\n",
    "    forbidden_tokens=forbidden_word_tokens,\n",
    "    penalty_weight=0.15,\n",
    "    callbacks=[debug_callback]\n",
    ")\n",
    "\n",
    "print(\"âœ… Custom trainer created successfully!\")\n",
    "print(f\"ğŸš« Monitoring {len(FORBIDDEN_WORDS)} forbidden words\")\n",
    "print(f\"âš–ï¸ Penalty weight: {PENALTY_WEIGHT}\")\n",
    "print(\"\\nğŸš€ Ready to train with custom loss! Run: custom_trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing custom trainer compatibility with current Transformers version...\n",
      "ğŸš« Custom loss initialized with 8788 forbidden words\n",
      "âš–ï¸ Penalty weight: 0.15\n",
      "âœ… Custom loss computation successful! Loss: 5.8383\n",
      "âœ… All compatibility tests passed!\n",
      "ğŸš€ Custom trainer is ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3269202755.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ FIX VERIFIED: Test Custom Trainer Compatibility\n",
    "\n",
    "print(\"ğŸ§ª Testing custom trainer compatibility with current Transformers version...\")\n",
    "\n",
    "try:\n",
    "    # Test creating the custom trainer \n",
    "    test_trainer = ForbiddenLossTrainer(\n",
    "        model=model_lora,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['eval'],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=TOKENIZER,\n",
    "        forbidden_tokens=forbidden_word_tokens,\n",
    "        penalty_weight=0.15,\n",
    "        callbacks=[debug_callback]\n",
    "    )\n",
    "    \n",
    "    # Test compute_loss method with a small batch\n",
    "    test_samples = [tokenized_dataset['train'][i] for i in range(1)]\n",
    "    test_batch = data_collator(test_samples)\n",
    "    \n",
    "    # Move to model device\n",
    "    model_device = next(model_lora.parameters()).device\n",
    "    test_batch = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v \n",
    "                  for k, v in test_batch.items()}\n",
    "    \n",
    "    # Test the compute_loss method (this should now work without the TypeError)\n",
    "    with torch.no_grad():\n",
    "        loss = test_trainer.compute_loss(model_lora, test_batch)\n",
    "        print(f\"âœ… Custom loss computation successful! Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"âœ… All compatibility tests passed!\")\n",
    "    print(\"ğŸš€ Custom trainer is ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during testing: {e}\")\n",
    "    print(\"ğŸ”§ Please check the error and re-run the custom loss function cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c58d523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 1:17:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>10.682200</td>\n",
       "      <td>5.474257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.443400</td>\n",
       "      <td>2.623299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.996700</td>\n",
       "      <td>2.209522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.551800</td>\n",
       "      <td>2.086040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.420500</td>\n",
       "      <td>1.996282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.298300</td>\n",
       "      <td>1.917997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.248300</td>\n",
       "      <td>1.905115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Loss = N/A\n",
      "Step 20: Loss = N/A\n",
      "Step 30: Loss = N/A\n",
      "Step 40: Loss = N/A\n",
      "Step 50: Loss = N/A\n",
      "Step 60: Loss = N/A\n",
      "Step 70: Loss = N/A\n",
      "Step 80: Loss = N/A\n",
      "Step 90: Loss = N/A\n",
      "Step 100: Loss = N/A\n",
      "Step 110: Loss = N/A\n",
      "Step 120: Loss = N/A\n",
      "Step 130: Loss = N/A\n",
      "Step 140: Loss = N/A\n",
      "Step 150: Loss = N/A\n",
      "Step 160: Loss = N/A\n",
      "Step 170: Loss = N/A\n",
      "Step 180: Loss = N/A\n",
      "Step 190: Loss = N/A\n",
      "Step 200: Loss = N/A\n",
      "Step 210: Loss = N/A\n",
      "Step 220: Loss = N/A\n",
      "Step 230: Loss = N/A\n",
      "Step 240: Loss = N/A\n",
      "Step 250: Loss = N/A\n",
      "Step 260: Loss = N/A\n",
      "Step 270: Loss = N/A\n",
      "Step 280: Loss = N/A\n",
      "Step 290: Loss = N/A\n",
      "Step 300: Loss = N/A\n",
      "Step 310: Loss = N/A\n",
      "Step 320: Loss = N/A\n",
      "Step 330: Loss = N/A\n",
      "Step 340: Loss = N/A\n",
      "Step 350: Loss = N/A\n",
      "Step 360: Loss = N/A\n",
      "Step 370: Loss = N/A\n",
      "Step 380: Loss = N/A\n",
      "Step 390: Loss = N/A\n",
      "Step 400: Loss = N/A\n",
      "Step 410: Loss = N/A\n",
      "Step 420: Loss = N/A\n",
      "Step 430: Loss = N/A\n",
      "Step 440: Loss = N/A\n",
      "Step 450: Loss = N/A\n",
      "Step 460: Loss = N/A\n",
      "Step 470: Loss = N/A\n",
      "Step 480: Loss = N/A\n",
      "Step 490: Loss = N/A\n",
      "Step 500: Loss = N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510: Loss = N/A\n",
      "Step 520: Loss = N/A\n",
      "Step 530: Loss = N/A\n",
      "Step 540: Loss = N/A\n",
      "Step 550: Loss = N/A\n",
      "Step 560: Loss = N/A\n",
      "Step 570: Loss = N/A\n",
      "Step 580: Loss = N/A\n",
      "Step 590: Loss = N/A\n",
      "Step 600: Loss = N/A\n",
      "Step 610: Loss = N/A\n",
      "Step 620: Loss = N/A\n",
      "Step 630: Loss = N/A\n",
      "Step 640: Loss = N/A\n",
      "Step 650: Loss = N/A\n",
      "Step 660: Loss = N/A\n",
      "Step 670: Loss = N/A\n",
      "Step 680: Loss = N/A\n",
      "Step 690: Loss = N/A\n",
      "Step 700: Loss = N/A\n",
      "Step 710: Loss = N/A\n",
      "Step 720: Loss = N/A\n",
      "Step 730: Loss = N/A\n",
      "Step 740: Loss = N/A\n",
      "Step 750: Loss = N/A\n",
      "Step 760: Loss = N/A\n",
      "Step 770: Loss = N/A\n",
      "Step 780: Loss = N/A\n",
      "Step 790: Loss = N/A\n",
      "Step 800: Loss = N/A\n",
      "Step 810: Loss = N/A\n",
      "Step 820: Loss = N/A\n",
      "Step 830: Loss = N/A\n",
      "Step 840: Loss = N/A\n",
      "Step 850: Loss = N/A\n",
      "Step 860: Loss = N/A\n",
      "Step 870: Loss = N/A\n",
      "Step 880: Loss = N/A\n",
      "Step 890: Loss = N/A\n",
      "Step 900: Loss = N/A\n",
      "Step 910: Loss = N/A\n",
      "Step 920: Loss = N/A\n",
      "Step 930: Loss = N/A\n",
      "Step 940: Loss = N/A\n",
      "Step 950: Loss = N/A\n",
      "Step 960: Loss = N/A\n",
      "Step 970: Loss = N/A\n",
      "Step 980: Loss = N/A\n",
      "Step 990: Loss = N/A\n",
      "Step 1000: Loss = N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1010: Loss = N/A\n",
      "Step 1020: Loss = N/A\n",
      "Step 1030: Loss = N/A\n",
      "Step 1040: Loss = N/A\n",
      "Step 1050: Loss = N/A\n",
      "Step 1060: Loss = N/A\n",
      "Step 1070: Loss = N/A\n",
      "Step 1080: Loss = N/A\n",
      "Step 1090: Loss = N/A\n",
      "Step 1100: Loss = N/A\n",
      "Step 1110: Loss = N/A\n",
      "Step 1120: Loss = N/A\n",
      "Step 1130: Loss = N/A\n",
      "Step 1140: Loss = N/A\n",
      "Step 1150: Loss = N/A\n",
      "Step 1160: Loss = N/A\n",
      "Step 1170: Loss = N/A\n",
      "Step 1180: Loss = N/A\n",
      "Step 1190: Loss = N/A\n",
      "Step 1200: Loss = N/A\n",
      "Step 1210: Loss = N/A\n",
      "Step 1220: Loss = N/A\n",
      "Step 1230: Loss = N/A\n",
      "Step 1240: Loss = N/A\n",
      "Step 1250: Loss = N/A\n",
      "Step 1260: Loss = N/A\n",
      "Step 1270: Loss = N/A\n",
      "Step 1280: Loss = N/A\n",
      "Step 1290: Loss = N/A\n",
      "Step 1300: Loss = N/A\n",
      "Step 1310: Loss = N/A\n",
      "Step 1320: Loss = N/A\n",
      "Step 1330: Loss = N/A\n",
      "Step 1340: Loss = N/A\n",
      "Step 1350: Loss = N/A\n",
      "Step 1360: Loss = N/A\n",
      "Step 1370: Loss = N/A\n",
      "Step 1380: Loss = N/A\n",
      "Step 1390: Loss = N/A\n",
      "Step 1400: Loss = N/A\n",
      "Step 1410: Loss = N/A\n",
      "Step 1420: Loss = N/A\n",
      "Step 1430: Loss = N/A\n",
      "Step 1440: Loss = N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=4.041978756586711, metrics={'train_runtime': 4660.8229, 'train_samples_per_second': 4.943, 'train_steps_per_second': 0.309, 'total_flos': 2049937178775552.0, 'train_loss': 4.041978756586711, 'epoch': 10.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the inference in the test dataset\n",
    "# ğŸ”® INFERENCE ON TEST DATASET\n",
    "\n",
    "print(\"ğŸ”® Starting inference on test dataset...\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model_lora.eval()\n",
    "\n",
    "# Get test dataset\n",
    "test_dataset = dataset['test']\n",
    "train_dataset = dataset['train']\n",
    "# Filter test dataset to only include Spanish samples\n",
    "test_dataset_es = test_dataset.filter(lambda x: x['language'] == 'es')\n",
    "train_dataset_es = train_dataset.filter(lambda x: x['language'] == 'es')\n",
    "print(f\"ğŸ“Š Spanish test dataset size: {len(test_dataset_es)} samples\")\n",
    "print(f\"ğŸ“Š Original test dataset size: {len(test_dataset)} samples\")\n",
    "\n",
    "# Update test_dataset to use the filtered version\n",
    "test_dataset = test_dataset_es\n",
    "\n",
    "bad_words = [[TOKENIZER.convert_tokens_to_ids(f\"extra_id_{i}\")] for i in range(100)]\n",
    "\n",
    "# Function to generate detoxified text\n",
    "def detoxify_text(toxic_text, max_length=512):\n",
    "    \"\"\"Generate detoxified version of toxic text\"\"\"\n",
    "    # Add task prefix\n",
    "    input_text = f\"{toxic_text}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = TOKENIZER(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(model_device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model_lora.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=TOKENIZER.pad_token_id,\n",
    "            eos_token_id=TOKENIZER.eos_token_id,\n",
    "            bad_words_ids=bad_words\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Run inference on test samples\n",
    "results = []\n",
    "num_samples = max(10, len(test_dataset))\n",
    "\n",
    "print(f\"\\nğŸ§ª Testing on {num_samples} samples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = train_dataset_es[i]\n",
    "    toxic_sentence = sample['toxic_sentence']\n",
    "    expected_neutral = sample['neutral_sentence']\n",
    "    language = sample['language']\n",
    "    \n",
    "    # Generate detoxified version\n",
    "    try:\n",
    "        generated_neutral = detoxify_text(toxic_sentence)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'index': i,\n",
    "            'language': language,\n",
    "            'toxic_input': toxic_sentence,\n",
    "            'expected_output': expected_neutral,\n",
    "            'generated_output': generated_neutral\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nSample {i+1} ({language}):\")\n",
    "        print(f\"ğŸ”´ Toxic:     {toxic_sentence[:100]}{'...' if len(toxic_sentence) > 100 else ''}\")\n",
    "        print(f\"ğŸŸ¢ Expected:  {expected_neutral[:100]}{'...' if len(expected_neutral) > 100 else ''}\")\n",
    "        print(f\"ğŸ”µ Generated: {generated_neutral[:100]}{'...' if len(generated_neutral) > 100 else ''}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… Inference completed on {len(results)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02c17eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e08d6fa1bc842a8944df4946b1658c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0ff49697d54728b552d567b42bb9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea25bc498c444496ba57700eab3de1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757dfb70b3154ceaa0d4c61954b00e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e01a259decc4970a6b08370885b9721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ca7e4e468c404083b2c83efe7bf542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255fc8a837384dd48f5b000438e31849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc25d832d444b3794857e7d05834edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9366794fa183474fb7e3aca28d2b3200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6424dec0ab4c5ca258ecd8cce38694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66afe8aa880e44058d6b34e6f8f33016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DetoxificationEvaluator:\n",
    "    def __init__(self, bert_model='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with ROUGE, BLEU, and BERT similarity metrics.\n",
    "        \n",
    "        Args:\n",
    "            bert_model: The sentence transformer model to use for BERT similarity\n",
    "        \"\"\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.bert_model = SentenceTransformer(bert_model)\n",
    "        self.smoothing = SmoothingFunction()\n",
    "    \n",
    "    def compute_rouge(self, reference, prediction):\n",
    "        \"\"\"Compute ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, prediction)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    \n",
    "    def compute_bleu(self, reference, prediction):\n",
    "        \"\"\"Compute BLEU score with smoothing\"\"\"\n",
    "        ref_tokens = reference.split()\n",
    "        pred_tokens = prediction.split()\n",
    "        \n",
    "        # Use smoothing to handle cases with no n-gram overlap\n",
    "        bleu = sentence_bleu(\n",
    "            [ref_tokens], \n",
    "            pred_tokens, \n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        return bleu\n",
    "    \n",
    "    def compute_bert_similarity(self, text1, text2):\n",
    "        \"\"\"Compute BERT-based semantic similarity using sentence embeddings\"\"\"\n",
    "        embeddings = self.bert_model.encode([text1, text2])\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        return similarity\n",
    "    \n",
    "    def compute_input_output_similarity(self, input_text, output_text):\n",
    "        \"\"\"\n",
    "        Compute similarity ratio between input and output.\n",
    "        Uses both character-level and semantic similarity.\n",
    "        \"\"\"\n",
    "        # Character-level similarity (like difflib)\n",
    "        char_similarity = SequenceMatcher(None, input_text, output_text).ratio()\n",
    "        \n",
    "        # Semantic similarity using BERT\n",
    "        semantic_similarity = self.compute_bert_similarity(input_text, output_text)\n",
    "        \n",
    "        return {\n",
    "            'character_similarity': char_similarity,\n",
    "            'semantic_similarity': semantic_similarity,\n",
    "            'average_similarity': (char_similarity + semantic_similarity) / 2\n",
    "        }\n",
    "    \n",
    "    def evaluate_single(self, input_text, reference_text, prediction_text):\n",
    "        \"\"\"\n",
    "        Evaluate a single prediction against reference and input.\n",
    "        \n",
    "        Args:\n",
    "            input_text: The original toxic input\n",
    "            reference_text: The ground truth detoxified text\n",
    "            prediction_text: The model's predicted detoxified text\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all evaluation metrics\n",
    "        \"\"\"\n",
    "        rouge_scores = self.compute_rouge(reference_text, prediction_text)\n",
    "        bleu_score = self.compute_bleu(reference_text, prediction_text)\n",
    "        bert_similarity = self.compute_bert_similarity(reference_text, prediction_text)\n",
    "        input_output_sim = self.compute_input_output_similarity(input_text, prediction_text)\n",
    "        \n",
    "        return {\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'bleu': bleu_score,\n",
    "            'bert_similarity': bert_similarity,\n",
    "            'input_output_char_similarity': input_output_sim['character_similarity'],\n",
    "            'input_output_semantic_similarity': input_output_sim['semantic_similarity'],\n",
    "            'input_output_avg_similarity': input_output_sim['average_similarity']\n",
    "        }\n",
    "    \n",
    "    def evaluate_batch(self, inputs, references, predictions):\n",
    "        \"\"\"\n",
    "        Evaluate a batch of predictions.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of original toxic inputs\n",
    "            references: List of ground truth detoxified texts\n",
    "            predictions: List of model predictions\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with averaged metrics and individual scores\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for inp, ref, pred in zip(inputs, references, predictions):\n",
    "            result = self.evaluate_single(inp, ref, pred)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_metrics = {\n",
    "            metric: np.mean([r[metric] for r in results])\n",
    "            for metric in results[0].keys()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'average_metrics': avg_metrics,\n",
    "            'individual_scores': results\n",
    "        }\n",
    "    \n",
    "    def print_summary(self, evaluation_results):\n",
    "        \"\"\"Print a formatted summary of evaluation results\"\"\"\n",
    "        metrics = evaluation_results['average_metrics']\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"DETOXIFICATION EVALUATION SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nContent Preservation Metrics (vs Reference):\")\n",
    "        print(f\"  ROUGE-1:        {metrics['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2:        {metrics['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L:        {metrics['rougeL']:.4f}\")\n",
    "        print(f\"  BLEU:           {metrics['bleu']:.4f}\")\n",
    "        print(f\"  BERT Similarity: {metrics['bert_similarity']:.4f}\")\n",
    "        \n",
    "        print(\"\\nInput-Output Similarity (Content Retention):\")\n",
    "        print(f\"  Character-level: {metrics['input_output_char_similarity']:.4f}\")\n",
    "        print(f\"  Semantic:        {metrics['input_output_semantic_similarity']:.4f}\")\n",
    "        print(f\"  Average:         {metrics['input_output_avg_similarity']:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "evaluator = DetoxificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3a362e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    sample = test_dataset[i]\n",
    "    toxic_sentence = sample['toxic_sentence']\n",
    "    expected_neutral = sample['neutral_sentence']\n",
    "    language = sample['language']\n",
    "    \n",
    "    # Generate detoxified version\n",
    "    generated_neutral = detoxify_text(toxic_sentence)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'index': i,\n",
    "        'language': language,\n",
    "        'toxic_input': toxic_sentence,\n",
    "        'expected_output': expected_neutral,\n",
    "        'generated_output': generated_neutral\n",
    "    }\n",
    "    \n",
    "    result = evaluator.evaluate_single(\n",
    "        input_text=toxic_sentence,\n",
    "        reference_text=expected_neutral,\n",
    "        prediction_text=generated_neutral\n",
    "    )\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([result])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897cab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rouge1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rouge2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rougeL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bleu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bert_similarity",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "input_output_char_similarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "input_output_semantic_similarity",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "input_output_avg_similarity",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "6cd90796-6c05-458d-91e7-bbade03159fa",
       "rows": [
        [
         "0",
         "0.0",
         "0.0",
         "0.0",
         "0.025098621243978974",
         "0.775198",
         "0.8208955223880597",
         "0.8425691",
         "0.8317323"
        ],
        [
         "1",
         "0.0",
         "0.0",
         "0.0",
         "0.022416933501922302",
         "1.0",
         "0.7450980392156863",
         "0.9369675",
         "0.84103274"
        ],
        [
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0186749852743095",
         "0.6662729",
         "0.5153374233128835",
         "0.75781655",
         "0.636577"
        ],
        [
         "3",
         "0.0",
         "0.0",
         "0.0",
         "0.8633400213704505",
         "0.91781676",
         "0.7857142857142857",
         "0.86179674",
         "0.8237555"
        ],
        [
         "4",
         "0.8444444444444444",
         "0.8372093023255814",
         "0.8444444444444444",
         "0.7122051230572402",
         "0.94089895",
         "0.8363636363636363",
         "0.9276265",
         "0.8819951"
        ],
        [
         "5",
         "0.0",
         "0.0",
         "0.0",
         "0.2740311596835683",
         "0.8832638",
         "0.9056603773584906",
         "0.9492164",
         "0.9274384"
        ],
        [
         "6",
         "0.28571428571428575",
         "0.16666666666666666",
         "0.28571428571428575",
         "0.033031643180138064",
         "0.6369599",
         "0.8395061728395061",
         "0.82635045",
         "0.8329283"
        ],
        [
         "7",
         "0.0",
         "0.0",
         "0.0",
         "0.033031643180138064",
         "0.9952105",
         "0.9841269841269841",
         "1.0000001",
         "0.9920635"
        ],
        [
         "8",
         "0.0",
         "0.0",
         "0.0",
         "0.08872444253557525",
         "0.78952134",
         "0.6461538461538462",
         "0.8178466",
         "0.73200023"
        ],
        [
         "9",
         "0.0",
         "0.0",
         "0.0",
         "0.3301008309851503",
         "0.9533726",
         "0.8292682926829268",
         "0.9426424",
         "0.88595533"
        ],
        [
         "10",
         "0.0",
         "0.0",
         "0.0",
         "0.5133450480401704",
         "0.90563184",
         "0.8686868686868687",
         "0.9439642",
         "0.9063255"
        ],
        [
         "11",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.10153381",
         "0.1951219512195122",
         "0.033659063",
         "0.11439051"
        ],
        [
         "12",
         "0.0",
         "0.0",
         "0.0",
         "0.392814650900513",
         "0.98519886",
         "0.7787610619469026",
         "0.961947",
         "0.87035406"
        ],
        [
         "13",
         "0.8148148148148148",
         "0.8",
         "0.8148148148148148",
         "0.6132297420585351",
         "0.7763662",
         "0.9180327868852459",
         "0.9531391",
         "0.935586"
        ],
        [
         "14",
         "0.7826086956521738",
         "0.761904761904762",
         "0.7826086956521738",
         "0.6553925769960415",
         "0.7682193",
         "0.8201438848920863",
         "0.7037328",
         "0.76193833"
        ],
        [
         "15",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.88027453",
         "0.7225130890052356",
         "0.9568204",
         "0.8396667"
        ],
        [
         "16",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.8528043",
         "0.44776119402985076",
         "0.703842",
         "0.5758016"
        ],
        [
         "17",
         "0.0",
         "0.0",
         "0.0",
         "0.21771262023986562",
         "0.71969336",
         "0.9206349206349206",
         "0.9723572",
         "0.94649607"
        ],
        [
         "18",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0000002",
         "0.8823529411764706",
         "0.96654356",
         "0.92444825"
        ],
        [
         "19",
         "0.0",
         "0.0",
         "0.0",
         "0.023980296761827107",
         "0.3882714",
         "0.8333333333333334",
         "0.5166421",
         "0.6749877"
        ],
        [
         "20",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.9195414",
         "0.8717948717948718",
         "0.96541804",
         "0.91860646"
        ],
        [
         "21",
         "0.0",
         "0.0",
         "0.0",
         "0.5556957157837258",
         "0.8908467",
         "0.9257142857142857",
         "0.92102045",
         "0.9233674"
        ],
        [
         "22",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.98780924",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "23",
         "0.7547169811320754",
         "0.6274509803921569",
         "0.7547169811320754",
         "0.4541412498219455",
         "0.8623007",
         "0.9169675090252708",
         "0.9475982",
         "0.93228287"
        ],
        [
         "24",
         "0.0",
         "0.0",
         "0.0",
         "0.043472087194499145",
         "0.70446163",
         "0.918918918918919",
         "0.87820065",
         "0.8985598"
        ],
        [
         "25",
         "0.0",
         "0.0",
         "0.0",
         "0.6803749333171202",
         "0.86365163",
         "0.8222222222222222",
         "0.90314835",
         "0.8626853"
        ],
        [
         "26",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.7659333",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "27",
         "0.0",
         "0.0",
         "0.0",
         "0.025120879841450415",
         "0.98123467",
         "0.6333333333333333",
         "0.93959475",
         "0.78646404"
        ],
        [
         "28",
         "0.36363636363636365",
         "0.3",
         "0.36363636363636365",
         "0.06557378107648686",
         "0.49269003",
         "0.6451612903225806",
         "0.6881721",
         "0.6666667"
        ],
        [
         "29",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.6939149",
         "0.7619047619047619",
         "0.8719484",
         "0.8169266"
        ],
        [
         "30",
         "0.0",
         "0.0",
         "0.0",
         "0.5799150677091909",
         "0.95150757",
         "0.7919463087248322",
         "0.95349276",
         "0.8727195"
        ],
        [
         "31",
         "0.2",
         "0.07142857142857144",
         "0.2",
         "0.005619577389365218",
         "0.56632227",
         "0.4111111111111111",
         "0.68173623",
         "0.5464237"
        ],
        [
         "32",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.96785694",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "33",
         "0.0",
         "0.0",
         "0.0",
         "0.037951271263104894",
         "0.9126835",
         "0.3177570093457944",
         "0.66200423",
         "0.48988062"
        ],
        [
         "34",
         "0.6",
         "0.4444444444444444",
         "0.6",
         "0.24808415001701817",
         "0.588286",
         "0.8854961832061069",
         "0.860751",
         "0.8731236"
        ],
        [
         "35",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.8362378",
         "0.4489795918367347",
         "0.75450003",
         "0.6017398"
        ],
        [
         "36",
         "0.0",
         "0.0",
         "0.0",
         "0.08748468738276342",
         "0.905566",
         "0.6391752577319587",
         "0.8738214",
         "0.75649834"
        ],
        [
         "37",
         "0.0",
         "0.0",
         "0.0",
         "0.3155984539112945",
         "0.7904336",
         "0.8541666666666666",
         "0.8682088",
         "0.86118776"
        ],
        [
         "38",
         "0.6060606060606061",
         "0.5161290322580646",
         "0.6060606060606061",
         "0.26753788181976984",
         "0.76066804",
         "0.6270270270270271",
         "0.72809744",
         "0.67756224"
        ],
        [
         "39",
         "0.0",
         "0.0",
         "0.0",
         "0.03639945549178427",
         "0.7166009",
         "0.7047619047619048",
         "0.9429686",
         "0.8238653"
        ],
        [
         "40",
         "0.64",
         "0.608695652173913",
         "0.64",
         "0.46548383647045716",
         "0.79601854",
         "0.6612903225806451",
         "0.75780463",
         "0.7095475"
        ],
        [
         "41",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.7551517",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "42",
         "0.0",
         "0.0",
         "0.0",
         "0.5169731539571706",
         "0.8420742",
         "0.8623853211009175",
         "0.8573592",
         "0.8598722"
        ],
        [
         "43",
         "0.6923076923076923",
         "0.5833333333333334",
         "0.6923076923076923",
         "0.47987820666906633",
         "0.6320709",
         "0.8976377952755905",
         "0.83743787",
         "0.86753786"
        ],
        [
         "44",
         "0.14285714285714288",
         "0.0",
         "0.14285714285714288",
         "0.02777619034011792",
         "0.46079648",
         "0.8089887640449438",
         "0.7714343",
         "0.79021156"
        ],
        [
         "45",
         "0.8",
         "0.7692307692307693",
         "0.8",
         "0.5081327481546147",
         "0.4836983",
         "0.7191011235955056",
         "0.36977807",
         "0.5444396"
        ],
        [
         "46",
         "0.0",
         "0.0",
         "0.0",
         "0.4699152171992906",
         "0.83795583",
         "0.9253731343283582",
         "0.83785504",
         "0.8816141"
        ],
        [
         "47",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.29846144",
         "0.684931506849315",
         "0.55246025",
         "0.61869586"
        ],
        [
         "48",
         "0.0",
         "0.0",
         "0.0",
         "0.5749514614774233",
         "0.8761343",
         "0.6868686868686869",
         "0.83776176",
         "0.7623152"
        ],
        [
         "49",
         "0.0",
         "0.0",
         "0.0",
         "0.49296563639776614",
         "0.8392334",
         "0.6272189349112426",
         "0.83577013",
         "0.73149455"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 720
       }
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9216685f-51c3-443d-a9a1-0132086822ed\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bleu</th>\n",
       "      <th>bert_similarity</th>\n",
       "      <th>input_output_char_similarity</th>\n",
       "      <th>input_output_semantic_similarity</th>\n",
       "      <th>input_output_avg_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025099</td>\n",
       "      <td>0.775198</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.842569</td>\n",
       "      <td>0.831732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.936967</td>\n",
       "      <td>0.841033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.666273</td>\n",
       "      <td>0.515337</td>\n",
       "      <td>0.757817</td>\n",
       "      <td>0.636577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863340</td>\n",
       "      <td>0.917817</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.861797</td>\n",
       "      <td>0.823756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.712205</td>\n",
       "      <td>0.940899</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.927626</td>\n",
       "      <td>0.881995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305098</td>\n",
       "      <td>0.741868</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.919711</td>\n",
       "      <td>0.782932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279528</td>\n",
       "      <td>0.819495</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.857838</td>\n",
       "      <td>0.717808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042830</td>\n",
       "      <td>0.851192</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.843166</td>\n",
       "      <td>0.796583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.508133</td>\n",
       "      <td>0.533931</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.340088</td>\n",
       "      <td>0.447822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326390</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>0.694524</td>\n",
       "      <td>0.616228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 8 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9216685f-51c3-443d-a9a1-0132086822ed')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9216685f-51c3-443d-a9a1-0132086822ed button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9216685f-51c3-443d-a9a1-0132086822ed');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       rouge1    rouge2    rougeL      bleu  bert_similarity  \\\n",
       "0    0.000000  0.000000  0.000000  0.025099         0.775198   \n",
       "1    0.000000  0.000000  0.000000  0.022417         1.000000   \n",
       "2    0.000000  0.000000  0.000000  0.018675         0.666273   \n",
       "3    0.000000  0.000000  0.000000  0.863340         0.917817   \n",
       "4    0.844444  0.837209  0.844444  0.712205         0.940899   \n",
       "..        ...       ...       ...       ...              ...   \n",
       "715  0.000000  0.000000  0.000000  0.305098         0.741868   \n",
       "716  0.000000  0.000000  0.000000  0.279528         0.819495   \n",
       "717  0.000000  0.000000  0.000000  0.042830         0.851192   \n",
       "718  0.615385  0.545455  0.615385  0.508133         0.533931   \n",
       "719  0.000000  0.000000  0.000000  0.000000         0.326390   \n",
       "\n",
       "     input_output_char_similarity  input_output_semantic_similarity  \\\n",
       "0                        0.820896                          0.842569   \n",
       "1                        0.745098                          0.936967   \n",
       "2                        0.515337                          0.757817   \n",
       "3                        0.785714                          0.861797   \n",
       "4                        0.836364                          0.927626   \n",
       "..                            ...                               ...   \n",
       "715                      0.646154                          0.919711   \n",
       "716                      0.577778                          0.857838   \n",
       "717                      0.750000                          0.843166   \n",
       "718                      0.555556                          0.340088   \n",
       "719                      0.537931                          0.694524   \n",
       "\n",
       "     input_output_avg_similarity  \n",
       "0                       0.831732  \n",
       "1                       0.841033  \n",
       "2                       0.636577  \n",
       "3                       0.823756  \n",
       "4                       0.881995  \n",
       "..                           ...  \n",
       "715                     0.782932  \n",
       "716                     0.717808  \n",
       "717                     0.796583  \n",
       "718                     0.447822  \n",
       "719                     0.616228  \n",
       "\n",
       "[720 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ffd4d",
   "metadata": {},
   "source": [
    "### Focus Aproach on Spanish and English and adding the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import traceback\n",
    "\n",
    "# Clear any existing models to avoid conflicts\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset = load_data()\n",
    "\n",
    "# Filter dataset for Spanish and English only\n",
    "selected_languages = ['es', 'en']\n",
    "\n",
    "filtered_dataset_train = dataset['train'].filter(lambda example: example['language'] in selected_languages)\n",
    "filtered_dataset_test = dataset['test'].filter(lambda example: example['language'] in selected_languages)\n",
    "\n",
    "# Tokenize the filtered dataset\n",
    "tokenized_filtered_dataset = filtered_dataset_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_dataset_train.column_names\n",
    ")\n",
    "\n",
    "filtered_train_test_split = tokenized_filtered_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "tokenized_filtered_dataset_for_training = DatasetDict({\n",
    "    'train': filtered_train_test_split['train'],\n",
    "    'eval': filtered_train_test_split['test']\n",
    "})\n",
    "\n",
    "# Create a new model with 8-bit quantization (avoiding conflicts with previous models)\n",
    "model_es_en = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/mt5-base\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA configuration for the new model\n",
    "lora_config_new = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to the new model\n",
    "model_es_en_lora = get_peft_model(model_es_en, lora_config_new)\n",
    "\n",
    "print(\"âœ… New model created successfully!\")\n",
    "print(f\"ğŸ¯ Trainable parameters: {sum(p.numel() for p in model_es_en_lora.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Create new data collator for this model\n",
    "data_collator_es_en = DataCollatorForSeq2Seq(\n",
    "    TOKENIZER, \n",
    "    model=model_es_en_lora,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer_es_en = ForbiddenLossTrainer(\n",
    "    model=model_es_en_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_filtered_dataset_for_training['train'],\n",
    "    eval_dataset=tokenized_filtered_dataset_for_training['eval'],\n",
    "    data_collator=data_collator_es_en,\n",
    "    tokenizer=TOKENIZER,\n",
    "    forbidden_tokens=forbidden_word_tokens,\n",
    "    penalty_weight=0.15,\n",
    "    callbacks=[debug_callback]\n",
    ")\n",
    "\n",
    "print(\"âœ… Spanish-English trainer created successfully!\")\n",
    "\n",
    "def check_trainable_params(model):\n",
    "    count = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            count += 1\n",
    "            print(\"TRAINABLE:\", n)\n",
    "    print(\"Total trainable:\", count)\n",
    "\n",
    "check_trainable_params(model_es_en_lora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d488175",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_es_en.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50afea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Caching is incompatible with gradient checkpointing in T5Block. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1 (es):\n",
      "ğŸ”´ Toxic:     Que analogÃ­a rebuscada y de mierda\n",
      "ğŸŸ¢ Expected:  QuÃ© analogÃ­a confusa\n",
      "ğŸ”µ Generated: <extra_id_0>   <extra_id_10> <extra_id_23> cien <extra_id_55> <extra_id_39> <extra_id_55> <extra_id_...\n",
      "\n",
      "Sample 2 (en):\n",
      "ğŸ”´ Toxic:     the guy was fucking shot ! .\n",
      "ğŸŸ¢ Expected:  The guy was shot\n",
      "ğŸ”µ Generated: <extra_id_0>  <extra_id_39> <extra_id_20>    <extra_id_1>\n",
      "\n",
      "Sample 3 (es):\n",
      "ğŸ”´ Toxic:     DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona, intolerante fascista.\n",
      "ğŸŸ¢ Expected:  DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona.\n",
      "ğŸ”µ Generated: <extra_id_0>\n",
      "\n",
      "Sample 4 (en):\n",
      "ğŸ”´ Toxic:     shit looked like a music video with synced audio , and footage editing .\n",
      "ğŸŸ¢ Expected:  looked like a music video with synced audio , and footage editing .\n",
      "ğŸ”µ Generated: <extra_id_0>  <extra_id_30> <extra_id_27>? <extra_id_10>. <extra_id_20>  <extra_id_54>. <extra_id_39...\n",
      "\n",
      "Sample 5 (es):\n",
      "ğŸ”´ Toxic:     Ya decÃ­a yo que veÃ­a menos moros\n",
      "ğŸŸ¢ Expected:  Ya me habÃ­a dado cuenta que veÃ­a menos personas de origen magrebÃ­\n",
      "ğŸ”µ Generated: <extra_id_0> <extra_id_10>  <extra_id_1>. <extra_id_25>y.: , <extra_id_10> <extra_id_53> yo <extra_i...\n",
      "\n",
      "Sample 6 (en):\n",
      "ğŸ”´ Toxic:     that ignorant failure isnt qualified for any public office .\n",
      "ğŸŸ¢ Expected:  he is not qualified for any public office.\n",
      "ğŸ”µ Generated: <extra_id_0>   <extra_id_25>  for <extra_id_34> <extra_id_55>)  <extra_id_52> <extra_id_4> <extra_id...\n",
      "\n",
      "Sample 7 (es):\n",
      "ğŸ”´ Toxic:     a ti no catalÃ¡n cabron jajaja! Esta champions es vuestra y del bayer xd\n",
      "ğŸŸ¢ Expected:  a ti no catalÃ¡n jajaja! Esta champions es vuestra y del bayer xd\n",
      "ğŸ”µ Generated: <extra_id_0>   <extra_id_56>  <extra_id_8> <extra_id_54> en... <extra_id_39>   <extra_id_1>\n",
      "\n",
      "Sample 8 (es):\n",
      "ğŸ”´ Toxic:     Pues si, es una puta mierda, DEP por ellos.\n",
      "ğŸŸ¢ Expected:  Pues si, es horrible\n",
      "ğŸ”µ Generated: <extra_id_0> pos <extra_id_53> ?es <extra_id_55>   <extra_id_5>  <extra_id_55>  <extra_id_39> <extra...\n",
      "\n",
      "Sample 9 (en):\n",
      "ğŸ”´ Toxic:     You're not being helpful, either, tiny-dick.\n",
      "ğŸŸ¢ Expected:  You're not being helpful, either.\n",
      "ğŸ”µ Generated: <extra_id_0> <extra_id_55>  ... ... <extra_id_13> <extra_id_7>s <extra_id_40> is  <extra_id_51>\n",
      "\n",
      "Sample 10 (en):\n",
      "ğŸ”´ Toxic:     it sucks that youre an awful person .\n",
      "ğŸŸ¢ Expected:  It hurts that you have a bad character\n",
      "ğŸ”µ Generated: <extra_id_0> by...  ...\n"
     ]
    }
   ],
   "source": [
    "results_spanish_english = []\n",
    "# Run inference on test samples for spanish and english\n",
    "num_samples_se = min(10, len(filtered_dataset['eval']))  # Test on first 10 samples\n",
    "\n",
    "for i in range(num_samples_se):\n",
    "    sample = filtered_dataset['eval'][i]\n",
    "    toxic_sentence = sample['toxic_sentence']\n",
    "    expected_neutral = sample['neutral_sentence']\n",
    "    language = sample['language']\n",
    "    try:\n",
    "        generated_neutral = detoxify_text(toxic_sentence)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'index': i,\n",
    "            'language': language,\n",
    "            'toxic_input': toxic_sentence,\n",
    "            'expected_output': expected_neutral,\n",
    "            'generated_output': generated_neutral\n",
    "        }\n",
    "        results_spanish_english.append(result)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nSample {i+1} ({language}):\")\n",
    "        print(f\"ğŸ”´ Toxic:     {toxic_sentence[:100]}{'...' if len(toxic_sentence) > 100 else ''}\")\n",
    "        print(f\"ğŸŸ¢ Expected:  {expected_neutral[:100]}{'...' if len(expected_neutral) > 100 else ''}\")\n",
    "        print(f\"ğŸ”µ Generated: {generated_neutral[:100]}{'...' if len(generated_neutral) > 100 else ''}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing sample {i}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¬ ADVANCED CUSTOM LOSS OPTIONS & TESTING\n",
    "\n",
    "# =============================================\n",
    "# OPTION 1: DYNAMIC PENALTY SCALING\n",
    "# =============================================\n",
    "class AdaptiveLossTrainer(CustomLossTrainer):\n",
    "    \"\"\"\n",
    "    Advanced trainer with adaptive penalty that increases during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, penalty_start=0.5, penalty_end=3.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.penalty_start = penalty_start\n",
    "        self.penalty_end = penalty_end\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Calculate current penalty weight based on training progress\n",
    "        if hasattr(self.args, 'num_train_epochs') and hasattr(self.state, 'epoch'):\n",
    "            progress = self.state.epoch / self.args.num_train_epochs\n",
    "            current_penalty = self.penalty_start + (self.penalty_end - self.penalty_start) * progress\n",
    "            \n",
    "            # Temporarily update penalty weight\n",
    "            original_penalty = self.penalty_weight\n",
    "            self.penalty_weight = current_penalty\n",
    "            \n",
    "            # Calculate loss with dynamic penalty\n",
    "            result = super().compute_loss(model, inputs, return_outputs, **kwargs)\n",
    "            \n",
    "            # Restore original penalty weight\n",
    "            self.penalty_weight = original_penalty\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return super().compute_loss(model, inputs, return_outputs, **kwargs)\n",
    "\n",
    "# =============================================\n",
    "# OPTION 2: CONTEXT-AWARE PENALTY\n",
    "# =============================================\n",
    "class ContextAwareLossTrainer(CustomLossTrainer):\n",
    "    \"\"\"\n",
    "    Trainer that applies different penalties based on context\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context_penalties=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Define different penalty weights for different contexts\n",
    "        self.context_penalties = context_penalties or {\n",
    "            'high_toxicity': 5.0,    # Higher penalty for very toxic inputs\n",
    "            'medium_toxicity': 2.0,  # Medium penalty for moderately toxic inputs  \n",
    "            'low_toxicity': 1.0,     # Lower penalty for mildly toxic inputs\n",
    "            'neutral': 0.5           # Minimal penalty for neutral inputs\n",
    "        }\n",
    "    \n",
    "    def _get_toxicity_level(self, input_text):\n",
    "        \"\"\"Determine toxicity level of input text\"\"\"\n",
    "        input_lower = input_text.lower()\n",
    "        \n",
    "        # Count forbidden words in input\n",
    "        forbidden_count = sum(1 for pattern in self.forbidden_patterns \n",
    "                            if pattern.search(input_lower))\n",
    "        \n",
    "        if forbidden_count >= 3:\n",
    "            return 'high_toxicity'\n",
    "        elif forbidden_count >= 2:\n",
    "            return 'medium_toxicity'\n",
    "        elif forbidden_count >= 1:\n",
    "            return 'low_toxicity'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def _calculate_forbidden_penalty(self, predictions, labels=None):\n",
    "        \"\"\"Calculate penalty with context awareness\"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        total_penalty = 0.0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get input context (you might need to modify this based on your data structure)\n",
    "            # For now, we'll use a simplified approach\n",
    "            \n",
    "            pred_ids = predictions[i]\n",
    "            valid_pred_ids = pred_ids[pred_ids >= 0]\n",
    "            \n",
    "            try:\n",
    "                generated_text = self.tokenizer.decode(valid_pred_ids, skip_special_tokens=True).lower()\n",
    "                \n",
    "                # Determine context-specific penalty\n",
    "                toxicity_level = 'medium_toxicity'  # Default - you can enhance this\n",
    "                context_penalty_weight = self.context_penalties.get(toxicity_level, 1.0)\n",
    "                \n",
    "                # Count forbidden words\n",
    "                forbidden_count = 0\n",
    "                for pattern in self.forbidden_patterns:\n",
    "                    matches = pattern.findall(generated_text)\n",
    "                    forbidden_count += len(matches)\n",
    "                \n",
    "                if forbidden_count > 0:\n",
    "                    total_penalty += forbidden_count * context_penalty_weight\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        device = predictions.device\n",
    "        penalty_tensor = torch.tensor(total_penalty, dtype=torch.float32, device=device)\n",
    "        return penalty_tensor\n",
    "\n",
    "# =============================================\n",
    "# TESTING UTILITIES\n",
    "# =============================================\n",
    "def test_custom_loss_function():\n",
    "    \"\"\"Test the custom loss function with sample data\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª Testing custom loss function...\")\n",
    "    \n",
    "    # Create a small test batch\n",
    "    test_samples = [tokenized_dataset['train'][i] for i in range(2)]\n",
    "    test_batch = data_collator(test_samples)\n",
    "    \n",
    "    # Move to model device\n",
    "    model_device = next(model_lora.parameters()).device\n",
    "    test_batch = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v \n",
    "                  for k, v in test_batch.items()}\n",
    "    \n",
    "    # Test with regular trainer\n",
    "    print(\"\\nğŸ“Š Regular trainer loss:\")\n",
    "    regular_loss = trainer.compute_loss(model_lora, test_batch)\n",
    "    print(f\"Regular loss: {regular_loss.item():.4f}\")\n",
    "    \n",
    "    # Test with custom trainer  \n",
    "    print(\"\\nğŸš« Custom trainer loss:\")\n",
    "    custom_loss = custom_trainer.compute_loss(model_lora, test_batch)\n",
    "    print(f\"Custom loss: {custom_loss.item():.4f}\")\n",
    "    print(f\"Penalty added: {(custom_loss - regular_loss).item():.4f}\")\n",
    "    \n",
    "    return regular_loss, custom_loss\n",
    "\n",
    "def analyze_forbidden_words_in_dataset():\n",
    "    \"\"\"Analyze how many forbidden words are in the training dataset\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š Analyzing forbidden words in dataset...\")\n",
    "    \n",
    "    forbidden_in_toxic = 0\n",
    "    forbidden_in_neutral = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Check first 100 samples for analysis\n",
    "    for i in range(min(100, len(dataset['train']))):\n",
    "        sample = dataset['train'][i]\n",
    "        toxic_text = sample['toxic_sentence'].lower()\n",
    "        neutral_text = sample['neutral_sentence'].lower()\n",
    "        \n",
    "        # Count forbidden words in toxic sentences\n",
    "        toxic_count = sum(1 for word in FORBIDDEN_WORDS if word in toxic_text)\n",
    "        forbidden_in_toxic += toxic_count\n",
    "        \n",
    "        # Count forbidden words in neutral sentences  \n",
    "        neutral_count = sum(1 for word in FORBIDDEN_WORDS if word in neutral_text)\n",
    "        forbidden_in_neutral += neutral_count\n",
    "        \n",
    "        total_samples += 1\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Analysis Results (first {total_samples} samples):\")\n",
    "    print(f\"ğŸ”´ Forbidden words in toxic sentences: {forbidden_in_toxic}\")\n",
    "    print(f\"ğŸŸ¢ Forbidden words in neutral sentences: {forbidden_in_neutral}\")\n",
    "    print(f\"ğŸ“‰ Reduction ratio: {(forbidden_in_toxic - forbidden_in_neutral) / max(forbidden_in_toxic, 1) * 100:.1f}%\")\n",
    "    \n",
    "    if forbidden_in_neutral > 0:\n",
    "        print(f\"âš ï¸  Warning: {forbidden_in_neutral} forbidden words found in neutral sentences!\")\n",
    "        print(\"ğŸ’¡ Consider reviewing your forbidden words list or dataset quality\")\n",
    "    \n",
    "    return forbidden_in_toxic, forbidden_in_neutral\n",
    "\n",
    "print(\"âœ… Advanced custom loss options created!\")\n",
    "print(\"\\nğŸ¯ Available trainer types:\")\n",
    "print(\"1. CustomLossTrainer - Basic forbidden words penalty\")\n",
    "print(\"2. AdaptiveLossTrainer - Dynamic penalty that increases during training\")  \n",
    "print(\"3. ContextAwareLossTrainer - Different penalties based on input toxicity\")\n",
    "print(\"\\nğŸ§ª Testing utilities:\")\n",
    "print(\"- test_custom_loss_function() - Test loss calculation\")\n",
    "print(\"- analyze_forbidden_words_in_dataset() - Analyze dataset content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3261892",
   "metadata": {},
   "source": [
    "# ğŸ¯ Custom Loss Function Guide\n",
    "\n",
    "## ğŸ”§ **How It Works**\n",
    "\n",
    "The custom loss function works by:\n",
    "\n",
    "1. **Computing original loss** - Standard cross-entropy loss from the model\n",
    "2. **Decoding predictions** - Convert model logits to text during training\n",
    "3. **Detecting forbidden words** - Scan generated text for banned words using regex\n",
    "4. **Adding penalty** - Add `penalty_weight Ã— forbidden_word_count` to the loss\n",
    "\n",
    "## ğŸ“Š **Mathematical Formula**\n",
    "\n",
    "```\n",
    "Total Loss = Original Loss + (penalty_weight Ã— Î£ forbidden_words_found)\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Original loss: `2.45`\n",
    "- Forbidden words found: `2` (e.g., \"stupid\", \"hate\")\n",
    "- Penalty weight: `2.0`\n",
    "- **Total loss: `2.45 + (2.0 Ã— 2) = 6.45`**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›ï¸ **Configuration Options**\n",
    "\n",
    "### **Basic Configuration**\n",
    "```python\n",
    "# Simple setup\n",
    "custom_trainer = CustomLossTrainer(\n",
    "    forbidden_words=[\"hate\", \"stupid\", \"toxic\", \"bad\"],\n",
    "    penalty_weight=1.5,  # Moderate penalty\n",
    "    # ... other trainer arguments\n",
    ")\n",
    "```\n",
    "\n",
    "### **Aggressive Penalty (Stronger Deterrent)**\n",
    "```python\n",
    "AGGRESSIVE_WORDS = [\"hate\", \"kill\", \"die\", \"stupid\", \"idiot\", \"worthless\"]\n",
    "aggressive_trainer = CustomLossTrainer(\n",
    "    forbidden_words=AGGRESSIVE_WORDS,\n",
    "    penalty_weight=5.0,  # High penalty\n",
    ")\n",
    "```\n",
    "\n",
    "### **Adaptive Penalty (Increases Over Time)**\n",
    "```python\n",
    "adaptive_trainer = AdaptiveLossTrainer(\n",
    "    penalty_start=0.5,   # Start with low penalty\n",
    "    penalty_end=3.0,     # End with high penalty\n",
    "    forbidden_words=FORBIDDEN_WORDS\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ **Penalty Weight Guidelines**\n",
    "\n",
    "| Penalty Weight | Effect | Use Case |\n",
    "|----------------|--------|----------|\n",
    "| `0.1 - 0.5` | Gentle nudge | Fine-tuning already good models |\n",
    "| `1.0 - 2.0` | Moderate penalty | **Recommended starting point** |\n",
    "| `2.5 - 5.0` | Strong deterrent | High-toxicity datasets |\n",
    "| `5.0+` | Aggressive blocking | Zero-tolerance scenarios |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª **Testing & Validation**\n",
    "\n",
    "### **Step 1: Test the Function**\n",
    "```python\n",
    "# Test if custom loss is working\n",
    "regular_loss, custom_loss = test_custom_loss_function()\n",
    "```\n",
    "\n",
    "### **Step 2: Analyze Dataset**\n",
    "```python\n",
    "# Check forbidden words in your data\n",
    "toxic_count, neutral_count = analyze_forbidden_words_in_dataset()\n",
    "```\n",
    "\n",
    "### **Step 3: Monitor During Training**\n",
    "The trainer will log every 50 steps:\n",
    "```\n",
    "Step 100: Original Loss = 2.45, Forbidden Penalty = 1.20, Total Loss = 3.65\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Best Practices**\n",
    "\n",
    "### **1. Start Conservative**\n",
    "- Begin with `penalty_weight=1.0`\n",
    "- Monitor training stability\n",
    "- Increase gradually if needed\n",
    "\n",
    "### **2. Balance is Key**\n",
    "- Too high penalty â†’ Model may avoid generating anything\n",
    "- Too low penalty â†’ Ineffective deterrent\n",
    "- **Sweet spot: 1.0-2.0 for most cases**\n",
    "\n",
    "### **3. Customize Forbidden Words**\n",
    "```python\n",
    "# Domain-specific forbidden words\n",
    "MEDICAL_FORBIDDEN = [\"kill\", \"die\", \"harm\", \"dangerous\", \"toxic\"]\n",
    "SOCIAL_FORBIDDEN = [\"hate\", \"stupid\", \"worthless\", \"loser\", \"pathetic\"]\n",
    "\n",
    "# Combine lists\n",
    "CUSTOM_FORBIDDEN = MEDICAL_FORBIDDEN + SOCIAL_FORBIDDEN\n",
    "```\n",
    "\n",
    "### **4. Monitor Training**\n",
    "- Watch for loss spikes (penalty too high)\n",
    "- Check generated samples during training\n",
    "- Validate on test set regularly\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Quick Start Recipe**\n",
    "\n",
    "```python\n",
    "# 1. Create custom trainer\n",
    "custom_trainer = CustomLossTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['eval'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=TOKENIZER,\n",
    "    forbidden_words=FORBIDDEN_WORDS,\n",
    "    penalty_weight=2.0,  # Start here\n",
    "    callbacks=[debug_callback]\n",
    ")\n",
    "\n",
    "# 2. Test it works\n",
    "test_custom_loss_function()\n",
    "\n",
    "# 3. Start training\n",
    "custom_trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e49aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 400 samples for language: en\n",
      "INFO:__main__:Loaded 400 samples for language: am\n",
      "INFO:__main__:Loaded 400 samples for language: am\n",
      "INFO:__main__:Loaded 400 samples for language: ar\n",
      "INFO:__main__:Loaded 400 samples for language: ar\n",
      "INFO:__main__:Loaded 400 samples for language: de\n",
      "INFO:__main__:Loaded 400 samples for language: de\n",
      "INFO:__main__:Loaded 400 samples for language: es\n",
      "INFO:__main__:Loaded 400 samples for language: es\n",
      "INFO:__main__:Loaded 400 samples for language: hi\n",
      "INFO:__main__:Loaded 400 samples for language: hi\n",
      "INFO:__main__:Loaded 400 samples for language: ru\n",
      "INFO:__main__:Loaded 400 samples for language: uk\n",
      "INFO:__main__:Loaded 400 samples for language: ru\n",
      "INFO:__main__:Loaded 400 samples for language: uk\n",
      "INFO:__main__:Loaded 400 samples for language: zh\n",
      "INFO:__main__:Total dataset size: 3600 samples across 9 languages\n",
      "INFO:__main__:Loaded 400 samples for language: zh\n",
      "INFO:__main__:Total dataset size: 3600 samples across 9 languages\n",
      "INFO:__main__:Train split: 2880 samples\n",
      "INFO:__main__:Test split: 720 samples\n",
      "INFO:__main__:Train split: 2880 samples\n",
      "INFO:__main__:Test split: 720 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "Dataset size: 2\n",
      "Dataset features: {'toxic_sentence': Value('string'), 'neutral_sentence': Value('string'), 'language': Value('string')}\n",
      "Sample: {'toxic_sentence': 'Sehe ich Kathrin GÃ¶ring Eckart, Merkel, Gysi, Petry, Wagenknecht und all die anderen WendehÃ¤lse die uns heute regieren. Frage ich mich. War der Fall der Mauer ein Fehler.', 'neutral_sentence': 'Sehe ich Kathrin GÃ¶ring Eckart, Merkel, Gysi, Petry, Wagenknecht und all die anderen wechselnhaften  Politiker die uns heute regieren. Frage ich mich. War der Fall der Mauer ein Fehler.', 'language': 'de'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data()\n",
    "\n",
    "# Now dataset is already a HuggingFace Dataset object\n",
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset features: {dataset['train'].features}\")\n",
    "print(f\"Sample: {dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb26863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QLoRA model created with bfloat16 configuration\n",
      "ğŸ“Š Model type: <class 'peft.peft_model.PeftModelForSeq2SeqLM'>\n",
      "ğŸ”§ Compute dtype: torch.bfloat16\n",
      "ğŸ¯ Trainable parameters: 884,736 / 503,069,952 (0.18%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Create QLoRA configuration with bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=False,        # Disabled for stability\n",
    ")\n",
    "\n",
    "# Load and quantize the model\n",
    "model_quantized = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/mt5-base\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model_quantized = prepare_model_for_kbit_training(model_quantized)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank\n",
    "    lora_alpha=16,          # Alpha parameter\n",
    "    target_modules=[\"q\", \"v\"],  # Target attention modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to the quantized model\n",
    "model_lora = get_peft_model(model_quantized, lora_config)\n",
    "\n",
    "print(\"âœ… QLoRA model created with bfloat16 configuration\")\n",
    "print(f\"ğŸ“Š Model type: {type(model_lora)}\")\n",
    "print(f\"ğŸ”§ Compute dtype: {bnb_config.bnb_4bit_compute_dtype}\")\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "print(f\"ğŸ¯ Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efd14c",
   "metadata": {},
   "source": [
    "## Default Qlora Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d916f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93fb6e5444848f180ff138cff370648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenized dataset created with 2880 samples\n",
      "Trainer recreated with debug callback. Ready to train with detailed monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Custom callback to debug training issues\n",
    "from transformers import TrainerCallback, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 10 == 0:  # Log every 10 steps\n",
    "            logs = kwargs.get('logs', {})\n",
    "            loss = logs.get('train_loss', 'N/A')\n",
    "            \n",
    "            print(f\"Step {state.global_step}: Loss = {loss}\")\n",
    "            \n",
    "            # Check for problematic loss values\n",
    "            if isinstance(loss, (int, float)):\n",
    "                if loss == 0:\n",
    "                    print(\"âš ï¸  WARNING: Loss is exactly 0 - check your data preprocessing!\")\n",
    "                elif loss < 1e-6:\n",
    "                    print(\"âš ï¸  WARNING: Loss is extremely small - possible numerical issues!\")\n",
    "                elif np.isnan(loss):\n",
    "                    print(\"âš ï¸  ERROR: Loss is NaN - stopping training recommended!\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "\n",
    "# First, create the tokenized dataset\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the dataset for T5 training\"\"\"\n",
    "    # Create input text with task prefix\n",
    "    input_texts = [text for text in examples['toxic_sentence']]\n",
    "    target_texts = examples['neutral_sentence']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = TOKENIZER(\n",
    "        input_texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False  # We'll pad in the data collator\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with TOKENIZER.as_target_tokenizer():\n",
    "        labels = TOKENIZER(\n",
    "            target_texts,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to the train dataset\n",
    "print(\"ğŸ”„ Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenized dataset created with {len(tokenized_dataset)} samples\")\n",
    "\n",
    "# Split train into train and eval\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'eval': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Add the debug callback to trainer\n",
    "debug_callback = DebugCallback()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mt5-detoxify-qlora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=200,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    TOKENIZER, \n",
    "    model=model_lora,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Recreate trainer with debug callback\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['eval'],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[debug_callback]\n",
    ")\n",
    "\n",
    "print(\"Trainer recreated with debug callback. Ready to train with detailed monitoring.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a656b25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUICK DATA CHECK ===\n",
      "Dataset size: 2880\n",
      "Dataset features: {'toxic_sentence': Value('string'), 'neutral_sentence': Value('string'), 'language': Value('string')}\n",
      "\n",
      "Sample 0:\n",
      "Toxic  : 'Sehe ich Kathrin GÃ¶ring Eckart, Merkel, Gysi, Petry, Wagenknecht und all die and...'\n",
      "Neutral: 'Sehe ich Kathrin GÃ¶ring Eckart, Merkel, Gysi, Petry, Wagenknecht und all die and...'\n",
      "Same? False\n",
      "\n",
      "Sample 1:\n",
      "Toxic  : 'THIS SCHOOL DISTRICT SUCKS i know cause i go there'\n",
      "Neutral: 'This school district is unsatifactory i know cause i go there'\n",
      "Same? False\n",
      "\n",
      "Sample 2:\n",
      "Toxic  : 'áˆ°á‹­áŒ£áŠ• áŠ¥á‰£áŠ­áˆ…áŠ• á‹­áˆ…áŠ•áŠ• áŠƒáŒ¢áŠ á‰°áŠ› á‹­á‰…áˆ­ á‰ áˆˆá‹ á£ áŠ¥áˆ± á‹¨áˆšáŠ“áŒˆáˆ¨á‹áŠ• áŠ á‹«á‹á‰…áˆ'\n",
      "Neutral: 'áŠ¥á‰£áŠ­áˆ…áŠ• á‹­áˆ…áŠ•áŠ• áˆáˆµáŠªáŠ• á‹­á‰…áˆ­ á‰ áˆˆá‹ á£ á‰£áˆˆáˆ›á‹ˆá‰… áŠá‹ á‹¨á‰°áŠ“áŒˆáˆ¨á‹'\n",
      "Same? False\n",
      "\n",
      "=== TOKENIZED CHECK ===\n",
      "\n",
      "Tokenized Sample 0:\n",
      "Input : 'áŠ áŠ•á‰° á‹¶áˆ› á‹¨áˆšá‹«áˆ³á‹áŠá‹ áˆŒáˆ‹ á‹¨á‰µáŒáˆ«á‹­áŠ“ á‹¨áˆ€áŒˆáˆ«á‰½áŠ• á‰µá‹áˆá‹µ áˆˆáŠ“áŠ•á‰° á‹á‹­áŠá‰± á‹áˆ» á‰¥áˆ áˆ˜áˆá‰±áŠ“ áŠ áŠ«áˆ‰áŠ• áˆ›áŒ£á‰± áŠá‹ á‹¨áˆšá‹«áˆ³á‹áŠá‹á¢'\n",
      "Label : 'á‹¨áˆšá‹«áˆ³á‹áŠá‹ áˆŒáˆ‹ á‹¨á‰µáŒáˆ«á‹­áŠ“ á‹¨áˆ€áŒˆáˆ«á‰½áŠ• á‰µá‹áˆá‹µ áˆˆáŒ¥á‰‚á‰¶á‰¸ á‰°á‰¥áˆ áˆ˜áˆá‰±áŠ“ áŠ áŠ«áˆ‰áŠ• áˆ›áŒ£á‰± áŠá‹ á‹¨áˆšá‹«áˆ³á‹áŠá‹á¢'\n",
      "Same? False\n",
      "\n",
      "Tokenized Sample 1:\n",
      "Input : 'Ğ¢Ğ¸Ğ½ĞºĞ° Ñ…Ğ¾Ğ´Ğ¸ Ğ½Ğ° Jazz fest Ñ– Ğ²Ğ¸Ğ½Ğ¾ Ğ¿Ñ–ÑĞ»ÑĞ–Ñ–Ğ½ĞºÑƒ ÑĞ²Ğ¾Ñ Ğ²ĞµĞ´Ğ¸ Ğ±Ğ»ÑÑ‚ÑŒ.'\n",
      "Label : 'Ğ¢Ğ¸Ğ½ĞºĞ° Ñ…Ğ¾Ğ´Ğ¸ Ğ½Ğ° Jazz fest Ñ– Ğ²Ğ¸Ğ½Ğ¾ Ğ¿Ñ–ÑĞ»ÑĞ–Ñ–Ğ½ĞºÑƒ ÑĞ²Ğ¾Ñ Ğ²ĞµĞ´Ğ¸.'\n",
      "Same? False\n",
      "=== END QUICK CHECK ===\n"
     ]
    }
   ],
   "source": [
    "# QUICK CHECK: Let's see what's actually in your data\n",
    "print(\"=== QUICK DATA CHECK ===\")\n",
    "\n",
    "# Check the dataset used for tokenization\n",
    "print(f\"Dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Dataset features: {dataset['train'].features}\")\n",
    "\n",
    "# Check a few raw samples\n",
    "for i in range(3):\n",
    "    sample = dataset['train'][i]\n",
    "    toxic = sample['toxic_sentence']\n",
    "    neutral = sample['neutral_sentence']\n",
    "    \n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Toxic  : '{toxic[:80]}{'...' if len(toxic) > 80 else ''}'\")\n",
    "    print(f\"Neutral: '{neutral[:80]}{'...' if len(neutral) > 80 else ''}'\")\n",
    "    print(f\"Same? {toxic == neutral}\")\n",
    "    \n",
    "    if toxic == neutral:\n",
    "        print(\"ğŸš¨ PROBLEM: Toxic and neutral sentences are identical!\")\n",
    "    \n",
    "# Check tokenized samples\n",
    "print(f\"\\n=== TOKENIZED CHECK ===\")\n",
    "tokenized_train_dataset = tokenized_dataset['train']\n",
    "for i in range(2):\n",
    "    tokenized_sample = tokenized_train_dataset[i]\n",
    "    \n",
    "    # Decode back to text to see what was actually tokenized\n",
    "    input_text = TOKENIZER.decode(tokenized_sample['input_ids'], skip_special_tokens=True)\n",
    "    label_text = TOKENIZER.decode(tokenized_sample['labels'], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nTokenized Sample {i}:\")\n",
    "    print(f\"Input : '{input_text}'\")\n",
    "    print(f\"Label : '{label_text}'\")\n",
    "    print(f\"Same? {input_text == label_text}\")\n",
    "\n",
    "print(\"=== END QUICK CHECK ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5031d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATACOLLATOR DEBUG ===\n",
      "Before DataCollator:\n",
      "\n",
      "Sample 0:\n",
      "  Input IDs (first 10): [19293, 5359, 259, 12549, 3866, 32241, 3562, 3858, 51663, 259]\n",
      "  Labels (first 10): [32241, 3562, 3858, 51663, 259, 7234, 2423, 259, 24637, 41782]\n",
      "\n",
      "Sample 1:\n",
      "  Input IDs (first 10): [1454, 68588, 259, 67303, 310, 34792, 12065, 259, 389, 75376]\n",
      "  Labels (first 10): [1454, 68588, 259, 67303, 310, 34792, 12065, 259, 389, 75376]\n",
      "\n",
      "After DataCollator:\n",
      "Batch keys: KeysView({'input_ids': tensor([[19293,  5359,   259, 12549,  3866, 32241,  3562,  3858, 51663,   259,\n",
      "          7234,  2423,   259, 24637, 41782, 87090,  1237, 29795, 51301, 19593,\n",
      "           259,   986,  1984,  1305,  2308,  7958,  1791, 90748,   259, 66631,\n",
      "          2226, 73354,   259,  1984, 11258,  4494,  6304,  2338,  5293,  5644,\n",
      "          1791,  1876,  2401, 80344,  5218,  4580,  5644,  4476, 32241,  3562,\n",
      "          3858, 51663,  3369,     1],\n",
      "        [ 1454, 68588,   259, 67303,   310, 34792, 12065,   259,   389, 75376,\n",
      "         15322,  3283,  6758,  3431,   989,  1498,   748, 26862,   279,  1713,\n",
      "         22856,   260,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'labels': tensor([[32241,  3562,  3858, 51663,   259,  7234,  2423,   259, 24637, 41782,\n",
      "         87090,  1237, 29795, 51301, 19593,   259,   986,  1984,  1305,  2308,\n",
      "          7958,  1925, 17679,  7009, 59458,  5956,  2204,  6304,  2338,  5293,\n",
      "          5644,  1791,  1876,  2401, 80344,  5218,  4580,  5644,  4476, 32241,\n",
      "          3562,  3858, 51663,  3369,     1],\n",
      "        [ 1454, 68588,   259, 67303,   310, 34792, 12065,   259,   389, 75376,\n",
      "         15322,  3283,  6758,  3431,   989,  1498,   748, 26862,   279,   260,\n",
      "             1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[    0, 32241,  3562,  3858, 51663,   259,  7234,  2423,   259, 24637,\n",
      "         41782, 87090,  1237, 29795, 51301, 19593,   259,   986,  1984,  1305,\n",
      "          2308,  7958,  1925, 17679,  7009, 59458,  5956,  2204,  6304,  2338,\n",
      "          5293,  5644,  1791,  1876,  2401, 80344,  5218,  4580,  5644,  4476,\n",
      "         32241,  3562,  3858, 51663,  3369],\n",
      "        [    0,  1454, 68588,   259, 67303,   310, 34792, 12065,   259,   389,\n",
      "         75376, 15322,  3283,  6758,  3431,   989,  1498,   748, 26862,   279,\n",
      "           260,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]])})\n",
      "Input shape: torch.Size([2, 54])\n",
      "Labels shape: torch.Size([2, 45])\n",
      "\n",
      "Detailed check of first sample:\n",
      "Input IDs (first 15): [19293, 5359, 259, 12549, 3866, 32241, 3562, 3858, 51663, 259, 7234, 2423, 259, 24637, 41782]\n",
      "Labels (first 15): [32241, 3562, 3858, 51663, 259, 7234, 2423, 259, 24637, 41782, 87090, 1237, 29795, 51301, 19593]\n",
      "Valid labels: 45/45 (100.0%)\n",
      "\n",
      "Comparison at position 0-10:\n",
      "Input segment: [19293, 5359, 259, 12549, 3866, 32241, 3562, 3858, 51663, 259]\n",
      "Label segment: [32241, 3562, 3858, 51663, 259, 7234, 2423, 259, 24637, 41782]\n",
      "Input text: 'áŠ áŠ•á‰° á‹¶áˆ› á‹¨áˆšá‹«áˆ³á‹áŠá‹'\n",
      "Label text: 'á‹¨áˆšá‹«áˆ³á‹áŠá‹ áˆŒáˆ‹ á‹¨á‰µáŒáˆ«'\n",
      "Segments identical? False\n",
      "=== END DATACOLLATOR DEBUG ===\n"
     ]
    }
   ],
   "source": [
    "# TARGETED DEBUG: Check what DataCollator is actually doing\n",
    "print(\"=== DATACOLLATOR DEBUG ===\")\n",
    "\n",
    "# Get a few samples\n",
    "samples = [tokenized_train_dataset[i] for i in range(2)]\n",
    "\n",
    "print(\"Before DataCollator:\")\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Input IDs (first 10): {sample['input_ids'][:10]}\")\n",
    "    print(f\"  Labels (first 10): {sample['labels'][:10]}\")\n",
    "\n",
    "# Apply DataCollator\n",
    "batch = data_collator(samples)\n",
    "\n",
    "print(f\"\\nAfter DataCollator:\")\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "\n",
    "# Check first sample in detail\n",
    "input_ids_0 = batch['input_ids'][0]\n",
    "labels_0 = batch['labels'][0]\n",
    "\n",
    "print(f\"\\nDetailed check of first sample:\")\n",
    "print(f\"Input IDs (first 15): {input_ids_0[:15].tolist()}\")\n",
    "print(f\"Labels (first 15): {labels_0[:15].tolist()}\")\n",
    "\n",
    "# Count valid vs ignored labels\n",
    "valid_labels = (labels_0 != -100).sum().item()\n",
    "total_labels = len(labels_0)\n",
    "print(f\"Valid labels: {valid_labels}/{total_labels} ({valid_labels/total_labels*100:.1f}%)\")\n",
    "\n",
    "# Check if there's any real difference\n",
    "if valid_labels > 0:\n",
    "    # Find where valid labels start and compare\n",
    "    valid_mask = labels_0 != -100\n",
    "    valid_positions = torch.where(valid_mask)[0]\n",
    "    \n",
    "    if len(valid_positions) > 0:\n",
    "        start_pos = valid_positions[0].item()\n",
    "        end_pos = min(start_pos + 10, len(labels_0))\n",
    "        \n",
    "        input_segment = input_ids_0[start_pos:end_pos]\n",
    "        label_segment = labels_0[start_pos:end_pos]\n",
    "        \n",
    "        print(f\"\\nComparison at position {start_pos}-{end_pos}:\")\n",
    "        print(f\"Input segment: {input_segment.tolist()}\")\n",
    "        print(f\"Label segment: {label_segment.tolist()}\")\n",
    "        \n",
    "        # Decode these segments\n",
    "        try:\n",
    "            input_text = TOKENIZER.decode(input_segment, skip_special_tokens=True)\n",
    "            label_text = TOKENIZER.decode(label_segment, skip_special_tokens=True) \n",
    "            print(f\"Input text: '{input_text}'\")\n",
    "            print(f\"Label text: '{label_text}'\")\n",
    "            \n",
    "            are_identical = torch.equal(input_segment, label_segment)\n",
    "            print(f\"Segments identical? {are_identical}\")\n",
    "            \n",
    "            if are_identical:\n",
    "                print(\"ğŸš¨ FOUND THE PROBLEM: DataCollator is making input and labels identical!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Decode error: {e}\")\n",
    "\n",
    "print(\"=== END DATACOLLATOR DEBUG ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d3aa75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL & TRAINING DEBUG ===\n",
      "LoRA Configuration:\n",
      "Model type: <class 'peft.peft_model.PeftModelForSeq2SeqLM'>\n",
      "Trainable parameters:\n",
      "  Trainable: 884,736\n",
      "  Total: 503,069,952\n",
      "  Percentage: 0.18%\n",
      "\n",
      "=== MANUAL FORWARD PASS TEST ===\n",
      "Test batch shapes:\n",
      "  Input IDs: torch.Size([2, 54])\n",
      "  Labels: torch.Size([2, 45])\n",
      "Model device: cuda:0\n",
      "Moved batch to device: cuda:0\n",
      "\n",
      "Model outputs:\n",
      "  Loss: 3.701512098312378\n",
      "  Loss item: 3.701512098312378\n",
      "  Logits shape: torch.Size([2, 45, 250112])\n",
      "âœ… Loss is normal: 3.701512098312378\n",
      "=== END MODEL DEBUG ===\n"
     ]
    }
   ],
   "source": [
    "# FINAL DEBUG: Check model and training setup\n",
    "print(\"=== MODEL & TRAINING DEBUG ===\")\n",
    "\n",
    "# 1. Check if LoRA is actually applied\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"Model type: {type(model_lora)}\")\n",
    "print(f\"Trainable parameters:\")\n",
    "trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Percentage: {trainable_params/total_params*100:.2f}%\")\n",
    "\n",
    "# 2. Test forward pass manually\n",
    "print(f\"\\n=== MANUAL FORWARD PASS TEST ===\")\n",
    "model_lora.eval()\n",
    "\n",
    "# Get a small batch\n",
    "test_batch = data_collator([tokenized_train_dataset[0], tokenized_train_dataset[1]])\n",
    "\n",
    "print(f\"Test batch shapes:\")\n",
    "print(f\"  Input IDs: {test_batch['input_ids'].shape}\")\n",
    "print(f\"  Labels: {test_batch['labels'].shape}\")\n",
    "\n",
    "# Get model device\n",
    "model_device = next(model_lora.parameters()).device\n",
    "print(f\"Model device: {model_device}\")\n",
    "\n",
    "# Move batch to same device as model\n",
    "test_batch = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n",
    "print(f\"Moved batch to device: {model_device}\")\n",
    "\n",
    "# Manual forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model_lora(\n",
    "        input_ids=test_batch['input_ids'],\n",
    "        attention_mask=test_batch['attention_mask'],\n",
    "        labels=test_batch['labels']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel outputs:\")\n",
    "    print(f\"  Loss: {outputs.loss}\")\n",
    "    print(f\"  Loss item: {outputs.loss.item()}\")\n",
    "    print(f\"  Logits shape: {outputs.logits.shape}\")\n",
    "    \n",
    "    # Check if loss is actually zero\n",
    "    if outputs.loss.item() == 0.0:\n",
    "        print(\"ğŸš¨ CONFIRMED: Model is returning exactly 0 loss!\")\n",
    "        \n",
    "        # Check logits and labels in detail\n",
    "        logits = outputs.logits\n",
    "        labels = test_batch['labels']\n",
    "        \n",
    "        # Manual loss calculation\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        manual_loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        print(f\"  Manual loss calculation: {manual_loss.item()}\")\n",
    "        \n",
    "        # Check if all labels are being ignored\n",
    "        valid_labels = (labels != -100).sum()\n",
    "        print(f\"  Valid labels in batch: {valid_labels}\")\n",
    "        \n",
    "        if valid_labels == 0:\n",
    "            print(\"ğŸš¨ PROBLEM: All labels are being ignored (-100)!\")\n",
    "        else:\n",
    "            print(\"ğŸ¤” MYSTERY: Labels are valid but loss is still 0...\")\n",
    "            \n",
    "            # Check if model weights are frozen\n",
    "            print(f\"\\nChecking if model weights are trainable:\")\n",
    "            for name, param in model_lora.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(f\"  {name}: requires_grad=True, shape={param.shape}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"ğŸš¨ PROBLEM: No trainable parameters found!\")\n",
    "    else:\n",
    "        print(f\"âœ… Loss is normal: {outputs.loss.item()}\")\n",
    "\n",
    "model_lora.train()  # Back to training mode\n",
    "print(\"=== END MODEL DEBUG ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7d3b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NaN LOSS INVESTIGATION ===\n",
      "Input analysis:\n",
      "  Input IDs range: 1 - 90748\n",
      "  Vocab size: 250100\n",
      "  Model vocab size: 250112\n",
      "\n",
      "Label analysis:\n",
      "  Valid labels range: 1 - 87090\n",
      "\n",
      "Model embedding analysis:\n",
      "  Sample embeddings shape: torch.Size([3, 768])\n",
      "  Sample embeddings contain NaN: False\n",
      "  Sample embeddings contain Inf: False\n",
      "\n",
      "Logits analysis:\n",
      "  Logits shape: torch.Size([1, 45, 250112])\n",
      "  Logits contain NaN: False\n",
      "  Logits contain Inf: False\n",
      "  Logits range: -66.000 - 5.062\n",
      "\n",
      "Manual loss calculation:\n",
      "  Shift logits shape: torch.Size([45, 250112])\n",
      "  Shift labels shape: torch.Size([45])\n",
      "  Valid token losses: 45\n",
      "  Token losses contain NaN: False\n",
      "  Token losses contain Inf: False\n",
      "  Token losses range: 0.098 - 26.500\n",
      "  Mean loss: 3.417591094970703\n",
      "=== END NaN INVESTIGATION ===\n"
     ]
    }
   ],
   "source": [
    "# EXTENDED DEBUG: Investigate NaN loss issue\n",
    "print(\"=== NaN LOSS INVESTIGATION ===\")\n",
    "\n",
    "# Get a small batch for detailed analysis\n",
    "test_batch = data_collator([tokenized_train_dataset[0]])\n",
    "model_device = next(model_lora.parameters()).device\n",
    "test_batch = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n",
    "\n",
    "model_lora.eval()\n",
    "with torch.no_grad():\n",
    "    # Check input ranges\n",
    "    input_ids = test_batch['input_ids']\n",
    "    labels = test_batch['labels']\n",
    "    \n",
    "    print(f\"Input analysis:\")\n",
    "    print(f\"  Input IDs range: {input_ids.min().item()} - {input_ids.max().item()}\")\n",
    "    print(f\"  Vocab size: {TOKENIZER.vocab_size}\")\n",
    "    print(f\"  Model vocab size: {model_lora.config.vocab_size}\")\n",
    "    \n",
    "    # Check if any input IDs are out of vocab range\n",
    "    max_id = input_ids.max().item()\n",
    "    vocab_size = model_lora.config.vocab_size\n",
    "    if max_id >= vocab_size:\n",
    "        print(f\"ğŸš¨ PROBLEM: Input ID {max_id} >= vocab size {vocab_size}\")\n",
    "    \n",
    "    print(f\"\\nLabel analysis:\")\n",
    "    valid_labels = labels[labels != -100]\n",
    "    if len(valid_labels) > 0:\n",
    "        print(f\"  Valid labels range: {valid_labels.min().item()} - {valid_labels.max().item()}\")\n",
    "        max_label = valid_labels.max().item()\n",
    "        if max_label >= vocab_size:\n",
    "            print(f\"ğŸš¨ PROBLEM: Label {max_label} >= vocab size {vocab_size}\")\n",
    "    else:\n",
    "        print(\"  No valid labels found!\")\n",
    "    \n",
    "    # Check model embedding layer\n",
    "    print(f\"\\nModel embedding analysis:\")\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings for a small range of tokens\n",
    "        try:\n",
    "            sample_embeddings = model_lora.base_model.model.shared(torch.tensor([0, 1, 2]).to(model_device))\n",
    "            print(f\"  Sample embeddings shape: {sample_embeddings.shape}\")\n",
    "            print(f\"  Sample embeddings contain NaN: {torch.isnan(sample_embeddings).any().item()}\")\n",
    "            print(f\"  Sample embeddings contain Inf: {torch.isinf(sample_embeddings).any().item()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error accessing embeddings: {e}\")\n",
    "    \n",
    "    # Forward pass with gradient checking\n",
    "    try:\n",
    "        outputs = model_lora(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=test_batch['attention_mask'],\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        print(f\"\\nLogits analysis:\")\n",
    "        print(f\"  Logits shape: {logits.shape}\")\n",
    "        print(f\"  Logits contain NaN: {torch.isnan(logits).any().item()}\")\n",
    "        print(f\"  Logits contain Inf: {torch.isinf(logits).any().item()}\")\n",
    "        print(f\"  Logits range: {logits.min().item():.3f} - {logits.max().item():.3f}\")\n",
    "        \n",
    "        # Manual cross-entropy calculation with detailed checks\n",
    "        print(f\"\\nManual loss calculation:\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        shift_logits = logits.view(-1, logits.size(-1))\n",
    "        shift_labels = labels.view(-1)\n",
    "        \n",
    "        print(f\"  Shift logits shape: {shift_logits.shape}\")\n",
    "        print(f\"  Shift labels shape: {shift_labels.shape}\")\n",
    "        \n",
    "        # Calculate loss per token\n",
    "        token_losses = loss_fct(shift_logits, shift_labels)\n",
    "        valid_losses = token_losses[shift_labels != -100]\n",
    "        \n",
    "        print(f\"  Valid token losses: {len(valid_losses)}\")\n",
    "        if len(valid_losses) > 0:\n",
    "            print(f\"  Token losses contain NaN: {torch.isnan(valid_losses).any().item()}\")\n",
    "            print(f\"  Token losses contain Inf: {torch.isinf(valid_losses).any().item()}\")\n",
    "            print(f\"  Token losses range: {valid_losses.min().item():.3f} - {valid_losses.max().item():.3f}\")\n",
    "            \n",
    "            mean_loss = valid_losses.mean()\n",
    "            print(f\"  Mean loss: {mean_loss.item()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Forward pass error: {e}\")\n",
    "\n",
    "model_lora.train()\n",
    "print(\"=== END NaN INVESTIGATION ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01bb6142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 1:14:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.174100</td>\n",
       "      <td>2.758302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.690300</td>\n",
       "      <td>2.345390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.131000</td>\n",
       "      <td>2.206075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.905000</td>\n",
       "      <td>2.126004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.824300</td>\n",
       "      <td>2.068775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.741000</td>\n",
       "      <td>2.021767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.693800</td>\n",
       "      <td>2.008225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Loss = N/A\n",
      "Step 20: Loss = N/A\n",
      "Step 20: Loss = N/A\n",
      "Step 30: Loss = N/A\n",
      "Step 30: Loss = N/A\n",
      "Step 40: Loss = N/A\n",
      "Step 40: Loss = N/A\n",
      "Step 50: Loss = N/A\n",
      "Step 50: Loss = N/A\n",
      "Step 60: Loss = N/A\n",
      "Step 60: Loss = N/A\n",
      "Step 70: Loss = N/A\n",
      "Step 70: Loss = N/A\n",
      "Step 80: Loss = N/A\n",
      "Step 80: Loss = N/A\n",
      "Step 90: Loss = N/A\n",
      "Step 90: Loss = N/A\n",
      "Step 100: Loss = N/A\n",
      "Step 100: Loss = N/A\n",
      "Step 110: Loss = N/A\n",
      "Step 110: Loss = N/A\n",
      "Step 120: Loss = N/A\n",
      "Step 120: Loss = N/A\n",
      "Step 130: Loss = N/A\n",
      "Step 130: Loss = N/A\n",
      "Step 140: Loss = N/A\n",
      "Step 140: Loss = N/A\n",
      "Step 150: Loss = N/A\n",
      "Step 150: Loss = N/A\n",
      "Step 160: Loss = N/A\n",
      "Step 160: Loss = N/A\n",
      "Step 170: Loss = N/A\n",
      "Step 170: Loss = N/A\n",
      "Step 180: Loss = N/A\n",
      "Step 180: Loss = N/A\n",
      "Step 190: Loss = N/A\n",
      "Step 190: Loss = N/A\n",
      "Step 200: Loss = N/A\n",
      "Step 200: Loss = N/A\n",
      "Step 210: Loss = N/A\n",
      "Step 210: Loss = N/A\n",
      "Step 220: Loss = N/A\n",
      "Step 220: Loss = N/A\n",
      "Step 230: Loss = N/A\n",
      "Step 230: Loss = N/A\n",
      "Step 240: Loss = N/A\n",
      "Step 240: Loss = N/A\n",
      "Step 250: Loss = N/A\n",
      "Step 250: Loss = N/A\n",
      "Step 260: Loss = N/A\n",
      "Step 260: Loss = N/A\n",
      "Step 270: Loss = N/A\n",
      "Step 270: Loss = N/A\n",
      "Step 280: Loss = N/A\n",
      "Step 280: Loss = N/A\n",
      "Step 290: Loss = N/A\n",
      "Step 290: Loss = N/A\n",
      "Step 300: Loss = N/A\n",
      "Step 300: Loss = N/A\n",
      "Step 310: Loss = N/A\n",
      "Step 310: Loss = N/A\n",
      "Step 320: Loss = N/A\n",
      "Step 320: Loss = N/A\n",
      "Step 330: Loss = N/A\n",
      "Step 330: Loss = N/A\n",
      "Step 340: Loss = N/A\n",
      "Step 340: Loss = N/A\n",
      "Step 350: Loss = N/A\n",
      "Step 350: Loss = N/A\n",
      "Step 360: Loss = N/A\n",
      "Step 360: Loss = N/A\n",
      "Step 370: Loss = N/A\n",
      "Step 370: Loss = N/A\n",
      "Step 380: Loss = N/A\n",
      "Step 380: Loss = N/A\n",
      "Step 390: Loss = N/A\n",
      "Step 390: Loss = N/A\n",
      "Step 400: Loss = N/A\n",
      "Step 400: Loss = N/A\n",
      "Step 410: Loss = N/A\n",
      "Step 410: Loss = N/A\n",
      "Step 420: Loss = N/A\n",
      "Step 420: Loss = N/A\n",
      "Step 430: Loss = N/A\n",
      "Step 430: Loss = N/A\n",
      "Step 440: Loss = N/A\n",
      "Step 440: Loss = N/A\n",
      "Step 450: Loss = N/A\n",
      "Step 450: Loss = N/A\n",
      "Step 460: Loss = N/A\n",
      "Step 460: Loss = N/A\n",
      "Step 470: Loss = N/A\n",
      "Step 470: Loss = N/A\n",
      "Step 480: Loss = N/A\n",
      "Step 480: Loss = N/A\n",
      "Step 490: Loss = N/A\n",
      "Step 490: Loss = N/A\n",
      "Step 500: Loss = N/A\n",
      "Step 500: Loss = N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510: Loss = N/A\n",
      "Step 520: Loss = N/A\n",
      "Step 520: Loss = N/A\n",
      "Step 530: Loss = N/A\n",
      "Step 530: Loss = N/A\n",
      "Step 540: Loss = N/A\n",
      "Step 540: Loss = N/A\n",
      "Step 550: Loss = N/A\n",
      "Step 550: Loss = N/A\n",
      "Step 560: Loss = N/A\n",
      "Step 560: Loss = N/A\n",
      "Step 570: Loss = N/A\n",
      "Step 570: Loss = N/A\n",
      "Step 580: Loss = N/A\n",
      "Step 580: Loss = N/A\n",
      "Step 590: Loss = N/A\n",
      "Step 590: Loss = N/A\n",
      "Step 600: Loss = N/A\n",
      "Step 600: Loss = N/A\n",
      "Step 610: Loss = N/A\n",
      "Step 610: Loss = N/A\n",
      "Step 620: Loss = N/A\n",
      "Step 620: Loss = N/A\n",
      "Step 630: Loss = N/A\n",
      "Step 630: Loss = N/A\n",
      "Step 640: Loss = N/A\n",
      "Step 640: Loss = N/A\n",
      "Step 650: Loss = N/A\n",
      "Step 650: Loss = N/A\n",
      "Step 660: Loss = N/A\n",
      "Step 660: Loss = N/A\n",
      "Step 670: Loss = N/A\n",
      "Step 670: Loss = N/A\n",
      "Step 680: Loss = N/A\n",
      "Step 680: Loss = N/A\n",
      "Step 690: Loss = N/A\n",
      "Step 690: Loss = N/A\n",
      "Step 700: Loss = N/A\n",
      "Step 700: Loss = N/A\n",
      "Step 710: Loss = N/A\n",
      "Step 710: Loss = N/A\n",
      "Step 720: Loss = N/A\n",
      "Step 720: Loss = N/A\n",
      "Step 730: Loss = N/A\n",
      "Step 730: Loss = N/A\n",
      "Step 740: Loss = N/A\n",
      "Step 740: Loss = N/A\n",
      "Step 750: Loss = N/A\n",
      "Step 750: Loss = N/A\n",
      "Step 760: Loss = N/A\n",
      "Step 760: Loss = N/A\n",
      "Step 770: Loss = N/A\n",
      "Step 770: Loss = N/A\n",
      "Step 780: Loss = N/A\n",
      "Step 780: Loss = N/A\n",
      "Step 790: Loss = N/A\n",
      "Step 790: Loss = N/A\n",
      "Step 800: Loss = N/A\n",
      "Step 800: Loss = N/A\n",
      "Step 810: Loss = N/A\n",
      "Step 810: Loss = N/A\n",
      "Step 820: Loss = N/A\n",
      "Step 820: Loss = N/A\n",
      "Step 830: Loss = N/A\n",
      "Step 830: Loss = N/A\n",
      "Step 840: Loss = N/A\n",
      "Step 840: Loss = N/A\n",
      "Step 850: Loss = N/A\n",
      "Step 850: Loss = N/A\n",
      "Step 860: Loss = N/A\n",
      "Step 860: Loss = N/A\n",
      "Step 870: Loss = N/A\n",
      "Step 870: Loss = N/A\n",
      "Step 880: Loss = N/A\n",
      "Step 880: Loss = N/A\n",
      "Step 890: Loss = N/A\n",
      "Step 890: Loss = N/A\n",
      "Step 900: Loss = N/A\n",
      "Step 900: Loss = N/A\n",
      "Step 910: Loss = N/A\n",
      "Step 910: Loss = N/A\n",
      "Step 920: Loss = N/A\n",
      "Step 920: Loss = N/A\n",
      "Step 930: Loss = N/A\n",
      "Step 930: Loss = N/A\n",
      "Step 940: Loss = N/A\n",
      "Step 940: Loss = N/A\n",
      "Step 950: Loss = N/A\n",
      "Step 950: Loss = N/A\n",
      "Step 960: Loss = N/A\n",
      "Step 960: Loss = N/A\n",
      "Step 970: Loss = N/A\n",
      "Step 970: Loss = N/A\n",
      "Step 980: Loss = N/A\n",
      "Step 980: Loss = N/A\n",
      "Step 990: Loss = N/A\n",
      "Step 990: Loss = N/A\n",
      "Step 1000: Loss = N/A\n",
      "Step 1000: Loss = N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1010: Loss = N/A\n",
      "Step 1020: Loss = N/A\n",
      "Step 1020: Loss = N/A\n",
      "Step 1030: Loss = N/A\n",
      "Step 1030: Loss = N/A\n",
      "Step 1040: Loss = N/A\n",
      "Step 1040: Loss = N/A\n",
      "Step 1050: Loss = N/A\n",
      "Step 1050: Loss = N/A\n",
      "Step 1060: Loss = N/A\n",
      "Step 1060: Loss = N/A\n",
      "Step 1070: Loss = N/A\n",
      "Step 1070: Loss = N/A\n",
      "Step 1080: Loss = N/A\n",
      "Step 1080: Loss = N/A\n",
      "Step 1090: Loss = N/A\n",
      "Step 1090: Loss = N/A\n",
      "Step 1100: Loss = N/A\n",
      "Step 1100: Loss = N/A\n",
      "Step 1110: Loss = N/A\n",
      "Step 1110: Loss = N/A\n",
      "Step 1120: Loss = N/A\n",
      "Step 1120: Loss = N/A\n",
      "Step 1130: Loss = N/A\n",
      "Step 1130: Loss = N/A\n",
      "Step 1140: Loss = N/A\n",
      "Step 1140: Loss = N/A\n",
      "Step 1150: Loss = N/A\n",
      "Step 1150: Loss = N/A\n",
      "Step 1160: Loss = N/A\n",
      "Step 1160: Loss = N/A\n",
      "Step 1170: Loss = N/A\n",
      "Step 1170: Loss = N/A\n",
      "Step 1180: Loss = N/A\n",
      "Step 1180: Loss = N/A\n",
      "Step 1190: Loss = N/A\n",
      "Step 1190: Loss = N/A\n",
      "Step 1200: Loss = N/A\n",
      "Step 1200: Loss = N/A\n",
      "Step 1210: Loss = N/A\n",
      "Step 1210: Loss = N/A\n",
      "Step 1220: Loss = N/A\n",
      "Step 1220: Loss = N/A\n",
      "Step 1230: Loss = N/A\n",
      "Step 1230: Loss = N/A\n",
      "Step 1240: Loss = N/A\n",
      "Step 1240: Loss = N/A\n",
      "Step 1250: Loss = N/A\n",
      "Step 1250: Loss = N/A\n",
      "Step 1260: Loss = N/A\n",
      "Step 1260: Loss = N/A\n",
      "Step 1270: Loss = N/A\n",
      "Step 1270: Loss = N/A\n",
      "Step 1280: Loss = N/A\n",
      "Step 1280: Loss = N/A\n",
      "Step 1290: Loss = N/A\n",
      "Step 1290: Loss = N/A\n",
      "Step 1300: Loss = N/A\n",
      "Step 1300: Loss = N/A\n",
      "Step 1310: Loss = N/A\n",
      "Step 1310: Loss = N/A\n",
      "Step 1320: Loss = N/A\n",
      "Step 1320: Loss = N/A\n",
      "Step 1330: Loss = N/A\n",
      "Step 1330: Loss = N/A\n",
      "Step 1340: Loss = N/A\n",
      "Step 1340: Loss = N/A\n",
      "Step 1350: Loss = N/A\n",
      "Step 1350: Loss = N/A\n",
      "Step 1360: Loss = N/A\n",
      "Step 1360: Loss = N/A\n",
      "Step 1370: Loss = N/A\n",
      "Step 1370: Loss = N/A\n",
      "Step 1380: Loss = N/A\n",
      "Step 1380: Loss = N/A\n",
      "Step 1390: Loss = N/A\n",
      "Step 1390: Loss = N/A\n",
      "Step 1400: Loss = N/A\n",
      "Step 1400: Loss = N/A\n",
      "Step 1410: Loss = N/A\n",
      "Step 1410: Loss = N/A\n",
      "Step 1420: Loss = N/A\n",
      "Step 1420: Loss = N/A\n",
      "Step 1430: Loss = N/A\n",
      "Step 1430: Loss = N/A\n",
      "Step 1440: Loss = N/A\n",
      "Step 1440: Loss = N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=3.4316887113783094, metrics={'train_runtime': 4488.4579, 'train_samples_per_second': 5.133, 'train_steps_per_second': 0.321, 'total_flos': 1833619651448832.0, 'train_loss': 3.4316887113783094, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27a02d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® Starting inference on test dataset...\n",
      "ğŸ“Š Spanish test dataset size: 70 samples\n",
      "ğŸ“Š Original test dataset size: 720 samples\n",
      "\n",
      "ğŸ§ª Testing on 10 samples:\n",
      "================================================================================\n",
      "\n",
      "Sample 1 (es):\n",
      "ğŸ”´ Toxic:     Que analogÃ­a rebuscada y de mierda\n",
      "ğŸŸ¢ Expected:  QuÃ© analogÃ­a confusa\n",
      "ğŸ”µ Generated: <extra_id_0> Que analogÃ­a rebuscada y de mierda\n",
      "\n",
      "Sample 1 (es):\n",
      "ğŸ”´ Toxic:     Que analogÃ­a rebuscada y de mierda\n",
      "ğŸŸ¢ Expected:  QuÃ© analogÃ­a confusa\n",
      "ğŸ”µ Generated: <extra_id_0> Que analogÃ­a rebuscada y de mierda\n",
      "\n",
      "Sample 2 (es):\n",
      "ğŸ”´ Toxic:     DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona, intolerante fascista.\n",
      "ğŸŸ¢ Expected:  DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona.\n",
      "ğŸ”µ Generated: <extra_id_0> deberÃ­a darte vergÃ¼enza ofender las creencias de otras personas, intolerante fascista.\n",
      "\n",
      "Sample 2 (es):\n",
      "ğŸ”´ Toxic:     DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona, intolerante fascista.\n",
      "ğŸŸ¢ Expected:  DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona.\n",
      "ğŸ”µ Generated: <extra_id_0> deberÃ­a darte vergÃ¼enza ofender las creencias de otras personas, intolerante fascista.\n",
      "\n",
      "Sample 3 (es):\n",
      "ğŸ”´ Toxic:     Ya decÃ­a yo que veÃ­a menos moros\n",
      "ğŸŸ¢ Expected:  Ya me habÃ­a dado cuenta que veÃ­a menos personas de origen magrebÃ­\n",
      "ğŸ”µ Generated: áº£nÃ­a yo que veÃ­a menos moros\n",
      "\n",
      "Sample 3 (es):\n",
      "ğŸ”´ Toxic:     Ya decÃ­a yo que veÃ­a menos moros\n",
      "ğŸŸ¢ Expected:  Ya me habÃ­a dado cuenta que veÃ­a menos personas de origen magrebÃ­\n",
      "ğŸ”µ Generated: áº£nÃ­a yo que veÃ­a menos moros\n",
      "\n",
      "Sample 4 (es):\n",
      "ğŸ”´ Toxic:     a ti no catalÃ¡n cabron jajaja! Esta champions es vuestra y del bayer xd\n",
      "ğŸŸ¢ Expected:  a ti no catalÃ¡n jajaja! Esta champions es vuestra y del bayer xd\n",
      "ğŸ”µ Generated: <extra_id_0> esta champions es vuestra y del bayer xd\n",
      "\n",
      "Sample 4 (es):\n",
      "ğŸ”´ Toxic:     a ti no catalÃ¡n cabron jajaja! Esta champions es vuestra y del bayer xd\n",
      "ğŸŸ¢ Expected:  a ti no catalÃ¡n jajaja! Esta champions es vuestra y del bayer xd\n",
      "ğŸ”µ Generated: <extra_id_0> esta champions es vuestra y del bayer xd\n",
      "\n",
      "Sample 5 (es):\n",
      "ğŸ”´ Toxic:     Pues si, es una puta mierda, DEP por ellos.\n",
      "ğŸŸ¢ Expected:  Pues si, es horrible\n",
      "ğŸ”µ Generated: <extra_id_0> si, es una puta mierda, DEP por ellos\n",
      "\n",
      "Sample 5 (es):\n",
      "ğŸ”´ Toxic:     Pues si, es una puta mierda, DEP por ellos.\n",
      "ğŸŸ¢ Expected:  Pues si, es horrible\n",
      "ğŸ”µ Generated: <extra_id_0> si, es una puta mierda, DEP por ellos\n",
      "\n",
      "Sample 6 (es):\n",
      "ğŸ”´ Toxic:     Luego se queja Neymar... valiente subnormal.\n",
      "ğŸŸ¢ Expected:  Entonces Neymar se queja... valiente y extraÃ±o.\n",
      "ğŸ”µ Generated: <extra_id_0>ego se queja Neymar valiente subnormal.\n",
      "\n",
      "Sample 6 (es):\n",
      "ğŸ”´ Toxic:     Luego se queja Neymar... valiente subnormal.\n",
      "ğŸŸ¢ Expected:  Entonces Neymar se queja... valiente y extraÃ±o.\n",
      "ğŸ”µ Generated: <extra_id_0>ego se queja Neymar valiente subnormal.\n",
      "\n",
      "Sample 7 (es):\n",
      "ğŸ”´ Toxic:     This. Que puto asco de respuestas, sois repulsivos\n",
      "ğŸŸ¢ Expected:  This. Que mala respuesta\n",
      "ğŸ”µ Generated: ',' Que puto asco de respuestas, sois repulsivos\n",
      "\n",
      "Sample 7 (es):\n",
      "ğŸ”´ Toxic:     This. Que puto asco de respuestas, sois repulsivos\n",
      "ğŸŸ¢ Expected:  This. Que mala respuesta\n",
      "ğŸ”µ Generated: ',' Que puto asco de respuestas, sois repulsivos\n",
      "\n",
      "Sample 8 (es):\n",
      "ğŸ”´ Toxic:     Ahora me leo la secta que seguro que amplian informaciÃ³n\n",
      "ğŸŸ¢ Expected:  Voy a leer el artÃ­culo para obtener mÃ¡s informaciÃ³n.\n",
      "ğŸ”µ Generated: <extra_id_0> me leo la secta que seguro que amplian informaciÃ³n\n",
      "\n",
      "Sample 8 (es):\n",
      "ğŸ”´ Toxic:     Ahora me leo la secta que seguro que amplian informaciÃ³n\n",
      "ğŸŸ¢ Expected:  Voy a leer el artÃ­culo para obtener mÃ¡s informaciÃ³n.\n",
      "ğŸ”µ Generated: <extra_id_0> me leo la secta que seguro que amplian informaciÃ³n\n",
      "\n",
      "Sample 9 (es):\n",
      "ğŸ”´ Toxic:     Quien putas estÃ¡ RT esa mierda ?\n",
      "ğŸŸ¢ Expected:  Quien estÃ¡ RT esto?\n",
      "ğŸ”µ Generated: <extra_id_0> Quien putas estÃ¡ RT esa mierda ?\n",
      "\n",
      "Sample 9 (es):\n",
      "ğŸ”´ Toxic:     Quien putas estÃ¡ RT esa mierda ?\n",
      "ğŸŸ¢ Expected:  Quien estÃ¡ RT esto?\n",
      "ğŸ”µ Generated: <extra_id_0> Quien putas estÃ¡ RT esa mierda ?\n",
      "\n",
      "Sample 10 (es):\n",
      "ğŸ”´ Toxic:     ğŸš¨ğŸš¨ Los okupas causan graves disturbios en Santiago y el alcalde 'podemita' les respalda â¡ï¸  SON GENT...\n",
      "ğŸŸ¢ Expected:  Los okupas causan graves disturbios en santiago y el alcalde podemita les respalda\n",
      "ğŸ”µ Generated: ğŸš¨ğŸš¨ Los okupas causan graves disturbios en Santiago y el alcalde 'podemita' les respalda â¡ï¸ SON GENTU...\n",
      "================================================================================\n",
      "âœ… Inference completed on 10 samples\n",
      "\n",
      "Sample 10 (es):\n",
      "ğŸ”´ Toxic:     ğŸš¨ğŸš¨ Los okupas causan graves disturbios en Santiago y el alcalde 'podemita' les respalda â¡ï¸  SON GENT...\n",
      "ğŸŸ¢ Expected:  Los okupas causan graves disturbios en santiago y el alcalde podemita les respalda\n",
      "ğŸ”µ Generated: ğŸš¨ğŸš¨ Los okupas causan graves disturbios en Santiago y el alcalde 'podemita' les respalda â¡ï¸ SON GENTU...\n",
      "================================================================================\n",
      "âœ… Inference completed on 10 samples\n"
     ]
    }
   ],
   "source": [
    "# apply the inference in the test dataset\n",
    "# ğŸ”® INFERENCE ON TEST DATASET\n",
    "\n",
    "print(\"ğŸ”® Starting inference on test dataset...\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model_lora.eval()\n",
    "\n",
    "# Get test dataset\n",
    "test_dataset = dataset['test']\n",
    "# Filter test dataset to only include Spanish samples\n",
    "test_dataset_es = test_dataset.filter(lambda x: x['language'] == 'es')\n",
    "print(f\"ğŸ“Š Spanish test dataset size: {len(test_dataset_es)} samples\")\n",
    "print(f\"ğŸ“Š Original test dataset size: {len(test_dataset)} samples\")\n",
    "\n",
    "# Update test_dataset to use the filtered version\n",
    "test_dataset = test_dataset_es\n",
    "\n",
    "# Function to generate detoxified text\n",
    "def detoxify_text(toxic_text, max_length=512):\n",
    "    \"\"\"Generate detoxified version of toxic text\"\"\"\n",
    "    # Add task prefix\n",
    "    input_text = f\"{toxic_text}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = TOKENIZER(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(model_device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model_lora.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=TOKENIZER.pad_token_id,\n",
    "            eos_token_id=TOKENIZER.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Run inference on test samples\n",
    "results = []\n",
    "num_samples = min(10, len(test_dataset))  # Test on first 10 samples\n",
    "\n",
    "print(f\"\\nğŸ§ª Testing on {num_samples} samples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = test_dataset[i]\n",
    "    toxic_sentence = sample['toxic_sentence']\n",
    "    expected_neutral = sample['neutral_sentence']\n",
    "    language = sample['language']\n",
    "    \n",
    "    # Generate detoxified version\n",
    "    try:\n",
    "        generated_neutral = detoxify_text(toxic_sentence)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'index': i,\n",
    "            'language': language,\n",
    "            'toxic_input': toxic_sentence,\n",
    "            'expected_output': expected_neutral,\n",
    "            'generated_output': generated_neutral\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nSample {i+1} ({language}):\")\n",
    "        print(f\"ğŸ”´ Toxic:     {toxic_sentence[:100]}{'...' if len(toxic_sentence) > 100 else ''}\")\n",
    "        print(f\"ğŸŸ¢ Expected:  {expected_neutral[:100]}{'...' if len(expected_neutral) > 100 else ''}\")\n",
    "        print(f\"ğŸ”µ Generated: {generated_neutral[:100]}{'...' if len(generated_neutral) > 100 else ''}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… Inference completed on {len(results)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17b8b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® Starting full dataset inference...\n",
      "ğŸ“Š Full test dataset size: 720 samples\n",
      "\n",
      "ğŸ§ª Processing 720 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  14%|â–ˆâ–        | 100/720 [02:20<15:09,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 100/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  28%|â–ˆâ–ˆâ–Š       | 200/720 [04:48<14:02,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 200/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 300/720 [07:48<11:55,  1.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 300/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 400/720 [09:46<06:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 400/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 500/720 [13:51<06:02,  1.65s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 500/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 600/720 [16:42<03:20,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 600/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 700/720 [19:39<00:26,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 700/720 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating detoxified text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 720/720 [20:05<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Inference completed!\n",
      "ğŸ“Š Results summary:\n",
      "  Total samples processed: 720\n",
      "  Languages in dataset: {'am': 89, 'uk': 88, 'en': 86, 'zh': 83, 'ar': 79, 'ru': 76, 'de': 76, 'hi': 73, 'es': 70}\n",
      "  Successful generations: 720\n",
      "  Errors: 0\n",
      "  ğŸ“ Results saved to: mt5_detoxification_results_20251124_194544.csv\n",
      "\n",
      "ğŸ¯ Sample results:\n",
      "====================================================================================================\n",
      "\n",
      "Sample 1 (ru):\n",
      "ğŸ”´ Toxic:     Ğ½Ñƒ Ğ¸ Ğ´ÑƒÑ€Ğ°Ğº ! Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´ĞµĞ½Ğ½Ğ¸Ñ‚ÑŒ Ğº Ğ±ĞµĞ»Ğ¾Ñ€ÑƒÑĞ¸Ğ¸ - Ğ»ÑƒÑ‡Ñ‰ĞµĞ± Ğ±Ñ‹Ğ»Ğ¾ !\n",
      "ğŸŸ¢ Expected:  ĞÑƒĞ¶Ğ½Ğ¾ Ğ Ğ¾ÑÑĞ¸Ñ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğº Ğ‘ĞµĞ»Ğ¾Ñ€ÑƒÑĞ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ñ‹ Ğ±Ñ‹Ğ»Ğ¾\n",
      "ğŸ”µ Generated: Ğ½Ñƒ Ğ¸ Ğ´ÑƒÑ€Ğ°Ğº ! Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´ĞµĞ½Ğ½Ğ¸Ñ‚ÑŒ Ğº Ğ±ĞµĞ»Ğ¾Ñ€ÑƒÑĞ¸Ğ¸ - Ğ»ÑƒÑ‡Ñ‰ĞµĞ± Ğ±Ñ‹Ğ»Ğ¾ !\n",
      "\n",
      "Sample 2 (am):\n",
      "ğŸ”´ Toxic:     áŠ áŠ•á‰° á‹°áŠ•á‰†áˆ® á‹¨áˆ¸áˆ­áˆ™áŒ£ áˆáŒ… áŠ áˆ…á‹« á‰¦á‹¶ áŒ­áŠ•á‰…áˆ‹á‰µ á‹˜áˆ¨áŠ› á‹¨áˆ´á‰µ áˆáŒ… áŠ á‰…áˆáˆ…áŠ• áŠ¥á‹á‰… áŠ¥áŠ•á‹³ áŠ áŠ•á‰° áŠ á‹­áŠá‰± á‰°áˆ‹áˆ‹áŠª áŠá‹ áŠ áŒˆáˆ«á‰½áŠ• áŠ¥...\n",
      "ğŸŸ¢ Expected:  áŠ á‰…áˆáˆ…áŠ• á‰¥á‰³á‹á‰… á‹°áˆ…áŠ“ áŠá‹ á¤ áŠ¥áŠ“áŠ•á‰° áŠ“á‰½áˆ á‰£áŠ•á‹µ á‰ áŠ©áˆ áˆ€áŒˆáˆªá‰± áŠ¥áŠ•á‹³á‰µáˆ¨áŒ‹áŒ‹ á‹«á‹°áˆ¨áŒ‹á‰½áŠ‹á‰µ\n",
      "ğŸ”µ Generated: á‹°áŠ•á‰†áˆ® á‹¨áˆ´á‰µ áˆáŒ… áŠ á‰…áˆáˆ…áŠ• áŠ¥á‹á‰… áŠ¥áŠ•á‹³ áŠ áŠ•á‰° áŠ á‹­áŠá‰± á‰°áˆ‹áˆ‹áŠª áŠá‹ áŠ áŒˆáˆ«á‰½áŠ• áŠ¥áŠ•á‹³á‰µáˆ¨áŒ‹áŒ‹ á‹«á‹°áˆ¨áŒ‹á‰µ á‰†áˆ»áˆ»\n",
      "\n",
      "Sample 3 (ru):\n",
      "ğŸ”´ Toxic:     Ğ²ĞµĞ·Ğ´Ğµ Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ¸ Ğ±Ğ»ÑĞ´ÑŒ Ğ° Ğ´ĞµĞ´ÑƒÑˆĞºĞ¸ Ğ²ĞµĞ´ÑŒ Ğ¾Ğ½Ğ° Ğ¿Ğ¸Ğ·Ğ´ÑƒĞµÑ‚ Ğº Ğ²Ğ½ÑƒĞºĞ°Ğ¼ Ğ° Ğ´ĞµĞ´ ĞµĞ±Ğ°ÑˆĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸...\n",
      "ğŸŸ¢ Expected:  Ğ²ĞµĞ·Ğ´Ğµ Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ¸ Ğ¸ Ğ´ĞµĞ´ÑƒÑˆĞºĞ¸: Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ° ĞµĞ·Ğ´Ğ¸Ñ‚ Ğº Ğ²Ğ½ÑƒĞºĞ°Ğ¼, Ğ° Ğ´ĞµĞ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚, Ğ½Ğ¾ Ğ¾Ğ½Ğ° Ğ²ÑĞµĞ³Ğ´Ğ° Ñ…...\n",
      "ğŸ”µ Generated: Ğ° Ğ´ĞµĞ´ ĞµĞ±Ğ°ÑˆĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ½Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ Ğ° Ğ´ĞµĞ´ Ñ…ÑƒĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ\n",
      "\n",
      "Sample 4 (uk):\n",
      "ğŸ”´ Toxic:     Ğ±Ğ»ÑÑ…Ğ°, ÑĞº Ğ¶Ğµ Ğ¼ĞµĞ½Ñ– Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾, Ñ‰Ğ¾ Ğ²ÑĞµ Ñ‚Ğ°Ğº ;(\n",
      "ğŸŸ¢ Expected:  ÑĞº Ğ¶Ğµ Ğ¼ĞµĞ½Ñ– Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾, Ñ‰Ğ¾ Ğ²ÑĞµ Ñ‚Ğ°Ğº ;(\n",
      "ğŸ”µ Generated: <extra_id_0> ÑĞº Ğ¶Ğµ Ğ¼ĞµĞ½Ñ– Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾, Ñ‰Ğ¾ Ğ²ÑĞµ Ñ‚Ğ°Ğº ;(\n",
      "\n",
      "Sample 5 (de):\n",
      "ğŸ”´ Toxic:     LaÃŸt doch verdammt noch mal das NS Regime aus dem Spiel. Es war damals viel bess...\n",
      "ğŸŸ¢ Expected:  LaÃŸt doch mal das NS Regime aus dem Spiel. Es war damals viel besser fÃ¼r das Vol...\n",
      "ğŸ”µ Generated: <extra_id_0> viel besser fÃ¼r das Volk als ihr euch vorstelle\n",
      "\n",
      "ğŸ“ˆ Quick evaluation metrics:\n",
      "\n",
      "ğŸ“Š Per-language statistics:\n",
      "  am: 89/89 (100.0% success)\n",
      "  ar: 79/79 (100.0% success)\n",
      "  de: 76/76 (100.0% success)\n",
      "  en: 86/86 (100.0% success)\n",
      "  es: 70/70 (100.0% success)\n",
      "  hi: 73/73 (100.0% success)\n",
      "  ru: 76/76 (100.0% success)\n",
      "  uk: 88/88 (100.0% success)\n",
      "  zh: 83/83 (100.0% success)\n",
      "\n",
      "ğŸ’¾ CSV file 'mt5_detoxification_results_20251124_194544.csv' contains all results and can be opened in Excel or imported for further analysis.\n",
      "sample_id,language,toxic_input,expected_output,generated_output\n",
      "0,ru,Ğ½Ñƒ Ğ¸ Ğ´ÑƒÑ€Ğ°Ğº ! Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´ĞµĞ½Ğ½Ğ¸Ñ‚ÑŒ Ğº Ğ±ĞµĞ»Ğ¾Ñ€ÑƒÑĞ¸Ğ¸ - Ğ»ÑƒÑ‡Ñ‰ĞµĞ± Ğ±Ñ‹Ğ»Ğ¾ !,ĞÑƒĞ¶Ğ½Ğ¾ Ğ Ğ¾ÑÑĞ¸Ñ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğº Ğ‘ĞµĞ»Ğ¾Ñ€ÑƒÑĞ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ñ‹ Ğ±Ñ‹Ğ»Ğ¾,Ğ½Ñƒ Ğ¸ Ğ´ÑƒÑ€Ğ°Ğº ! Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ñ€Ğ¸ÑĞ¾ĞµĞ´ĞµĞ½Ğ½Ğ¸Ñ‚ÑŒ Ğº Ğ±ĞµĞ»Ğ¾Ñ€ÑƒÑĞ¸Ğ¸ - Ğ»ÑƒÑ‡Ñ‰ĞµĞ± Ğ±Ñ‹Ğ»Ğ¾ !\n",
      "1,am,áŠ áŠ•á‰° á‹°áŠ•á‰†áˆ® á‹¨áˆ¸áˆ­áˆ™áŒ£ áˆáŒ… áŠ áˆ…á‹« á‰¦á‹¶ áŒ­áŠ•á‰…áˆ‹á‰µ á‹˜áˆ¨áŠ› á‹¨áˆ´á‰µ áˆáŒ… áŠ á‰…áˆáˆ…áŠ• áŠ¥á‹á‰… áŠ¥áŠ•á‹³ áŠ áŠ•á‰° áŠ á‹­áŠá‰± á‰°áˆ‹áˆ‹áŠª áŠá‹ áŠ áŒˆáˆ«á‰½áŠ• áŠ¥áŠ•á‹³á‰µáˆ¨áŒ‹áŒ‹ á‹«á‹°áˆ¨áŒ‹á‰µ á‰†áˆ»áˆ»,áŠ á‰…áˆáˆ…áŠ• á‰¥á‰³á‹á‰… á‹°áˆ…áŠ“ áŠá‹ á¤ áŠ¥áŠ“áŠ•á‰° áŠ“á‰½áˆ á‰£áŠ•á‹µ á‰ áŠ©áˆ áˆ€áŒˆáˆªá‰± áŠ¥áŠ•á‹³á‰µáˆ¨áŒ‹áŒ‹ á‹«á‹°áˆ¨áŒ‹á‰½áŠ‹á‰µ,á‹°áŠ•á‰†áˆ® á‹¨áˆ´á‰µ áˆáŒ… áŠ á‰…áˆáˆ…áŠ• áŠ¥á‹á‰… áŠ¥áŠ•á‹³ áŠ áŠ•á‰° áŠ á‹­áŠá‰± á‰°áˆ‹áˆ‹áŠª áŠá‹ áŠ áŒˆáˆ«á‰½áŠ• áŠ¥áŠ•á‹³á‰µáˆ¨áŒ‹áŒ‹ á‹«á‹°áˆ¨áŒ‹á‰µ á‰†áˆ»áˆ»\n",
      "2,ru,Ğ²ĞµĞ·Ğ´Ğµ Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ¸ Ğ±Ğ»ÑĞ´ÑŒ Ğ° Ğ´ĞµĞ´ÑƒÑˆĞºĞ¸ Ğ²ĞµĞ´ÑŒ Ğ¾Ğ½Ğ° Ğ¿Ğ¸Ğ·Ğ´ÑƒĞµÑ‚ Ğº Ğ²Ğ½ÑƒĞºĞ°Ğ¼ Ğ° Ğ´ĞµĞ´ ĞµĞ±Ğ°ÑˆĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ½Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ Ğ° Ğ´ĞµĞ´ Ñ…ÑƒĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ,\"Ğ²ĞµĞ·Ğ´Ğµ Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ¸ Ğ¸ Ğ´ĞµĞ´ÑƒÑˆĞºĞ¸: Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ° ĞµĞ·Ğ´Ğ¸Ñ‚ Ğº Ğ²Ğ½ÑƒĞºĞ°Ğ¼, Ğ° Ğ´ĞµĞ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚, Ğ½Ğ¾ Ğ¾Ğ½Ğ° Ğ²ÑĞµĞ³Ğ´Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ, Ğ° Ğ´ĞµĞ´ÑƒÑˆĞºĞ° Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğ¹. ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ!\",Ğ° Ğ´ĞµĞ´ ĞµĞ±Ğ°ÑˆĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ½Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ Ğ° Ğ´ĞµĞ´ Ñ…ÑƒĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ\n",
      "3,uk,\"Ğ±Ğ»ÑÑ…Ğ°, ÑĞº Ğ¶Ğµ Ğ¼ĞµĞ½Ñ– Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾, Ñ‰Ğ¾ Ğ²ÑĞµ Ñ‚Ğ°Ğº ;(\",\"ÑĞº Ğ¶Ğµ Ğ¼ĞµĞ½Ñ– Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾, Ñ‰Ğ¾ Ğ²ÑĞµ Ñ‚Ğ°Ğº ;(\",\"<extra_id_0> ÑĞº Ğ¶Ğµ Ğ¼ĞµĞ½Ñ– Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾, Ñ‰Ğ¾ Ğ²ÑĞµ Ñ‚Ğ°Ğº ;(\"\n",
      "4,de,LaÃŸt doch verdammt noch mal das NS Regime aus dem Spiel. Es war damals viel besser fÃ¼r das Volk als ihr euch vorstelle,LaÃŸt doch mal das NS Regime aus dem Spiel. Es war damals viel besser fÃ¼r das Volk als ihr euch vorstelle,<extra_id_0> viel besser fÃ¼r das Volk als ihr euch vorstelle\n",
      "5,hi,\"à¤­à¥‡à¤¨à¤šà¥‹à¤¦, à¤¹à¤® à¤•à¥à¤¯à¤¾ à¤®à¤¾à¤ à¤šà¥à¤¦à¤¾à¤¨à¥‡ à¤‡à¤¤à¤¨à¥€ à¤®à¥‡à¤¹à¤¨à¤¤ à¤•à¤°à¤•à¥‡ à¤Ÿà¥‡à¤•à¥à¤¸ à¤­à¤°à¤¤à¥‡ à¤¹à¥ˆ..\",à¤¹à¤® à¤•à¥à¤¯à¤¾ à¤¹à¤¦ à¤¸à¥‡ à¤œà¥à¤¯à¤¾à¤¦à¤¾ à¤ªà¤°à¥‡à¤¶à¤¾à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‡à¤¤à¤¨à¥€ à¤®à¥‡à¤¹à¤¨à¤¤ à¤•à¤°à¤•à¥‡ à¤Ÿà¥‡à¤•à¥à¤¸ à¤­à¤°à¤¤à¥‡ à¤¹à¥ˆ..,à¤¹à¤® à¤•à¥à¤¯à¤¾ à¤®à¤¾à¤ à¤šà¥à¤¦à¤¾à¤¨à¥‡ à¤‡à¤¤à¤¨à¥€ à¤®à¥‡à¤¹à¤¨à¤¤ à¤•à¤°à¤•à¥‡ à¤Ÿà¥‡à¤•à¥à¤¸ à¤­à¤°à¤¤à¥‡ à¤¹à¥ˆ..\n",
      "6,es,Que analogÃ­a rebuscada y de mierda,QuÃ© analogÃ­a confusa,<extra_id_0> Que analogÃ­a rebuscada y de mierda\n",
      "7,am,á‰ áŠ¢á‰µá‹®áŒµá‹«áŠ• áŠ á‰£á‰£áˆ á‰ áŒ£áˆ á‹°á‹°á‰¥ áŠ¥áŠ“ á‹áˆ¸á‰³áˆ áŠáˆ…á¢,á‰ áŠ¢á‰µá‹®áŒµá‹«áŠ• áŠ á‰£á‰£áˆ áˆ˜áˆáŠ«áˆ áˆ°á‹ áŠ á‹­á‹°áˆˆáŠ¸áˆá¡á¡,<extra_id_0> á‰ áŠ¢á‰µá‹®áŒµá‹«áŠ• áŠ á‰£á‰£áˆ á‰ áŒ£áˆ á‹°á‹°á‰¥ áŠ¥áŠ“ á‹áˆ¸á‰³áˆ áŠáˆ…á¢\n",
      "8,am,áˆ…á‹á‰¡áŠ• áŠ á‹­á‹ˆáŠ­áˆáˆ á‰°á‰¥áˆ á‰°áŠáˆµá‰·áˆá¢ áŠ á‹­á‰° áŠ á‹²áˆ± áˆˆáŒˆáˆ° á‹µáˆáŒ»á‰¸á‹ áŒ á‹áˆ³,áˆ…á‹á‰¡áŠ• áŠ á‹­á‹ˆáŠ­áˆáˆ á‰°á‰¥áˆ á‰°áŠáˆµá‰·áˆá¢ áŠ á‰¶ áŠ á‹²áˆ± áˆˆáŒˆáˆ° áˆáŠá‹ á‹µáˆáŒ»á‰¸á‹ áŒ á‹,áˆ…á‹á‰¡áŠ• áŠ á‹­á‹ˆáŠ­áˆáˆ á‰°á‰¥áˆ á‰°áŠáˆµá‰·áˆ\n",
      "9,hi,à¤˜à¤° à¤¸à¥‡ à¤¬à¤¾à¤¹à¤° à¤†à¤•à¤° à¤‰à¤¨ à¤¸à¤¾à¤‚à¤ªà¥‹ à¤•à¥‹ à¤¸à¤¬à¤• à¤¨à¤¹à¥€ à¤¸à¤¿à¤–à¤¾ à¤¸à¤•à¤¤à¥‡?,à¤˜à¤° à¤¸à¥‡ à¤¬à¤¾à¤¹à¤° à¤†à¤•à¤° à¤‰à¤¨ à¤•à¥‹ à¤¸à¤¬à¤• à¤¨à¤¹à¥€ à¤¸à¤¿à¤–à¤¾ à¤¸à¤•à¤¤à¥‡?,<extra_id_0> à¤˜à¤° à¤¸à¥‡ à¤¬à¤¾à¤¹à¤° à¤†à¤•à¤° à¤‰à¤¨ à¤¸à¤¾à¤‚à¤ªà¥‹ à¤•à¥‹ à¤¸à¤¬à¤• à¤¸à¤¿à¤–à¤¾ à¤¸à¤•à¤¤à¥‡?\n",
      "10,uk,Ğ† Ñ‰Ğ¾Ğ± Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾ Ñ‰Ğµ Ğ»Ñ–Ğ¿ÑˆĞµ: Ğ½Ğ°Ñ…ÑƒĞ¹ Ğ¿Ğ¾Ñ€Ğ²Ğ°Ğ½Ñ– Ğ´Ğ¶Ñ–Ğ½ÑĞ¸,Ğ† Ñ‰Ğ¾Ğ± Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾ Ñ‰Ğµ Ğ»Ñ–Ğ¿ÑˆĞµ: Ğ¿Ğ¾Ñ€Ğ²Ğ°Ğ½Ñ– Ğ´Ğ¶Ñ–Ğ½ÑĞ¸,<extra_id_0> Ñ‰Ğ¾Ğ± Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾ Ñ‰Ğµ Ğ»Ñ–Ğ¿ÑˆĞµ: Ğ½Ğ°Ñ…ÑƒĞ¹ Ğ¿Ğ¾Ñ€Ğ²Ğ°Ğ½Ñ– Ğ´Ğ¶Ñ–Ğ½ÑĞ¸\n",
      "11,en,the guy was fucking shot ! .,The guy was shot,<extra_id_0> the guy was fucking shot !\n",
      "12,am,á‹°áŠ•á‰†áˆ® á‰ áŠ­á‰µ á‹¨á‰ áŠ¨á‰µáˆ… áŒˆáˆ¨á‹µ á‹¨áˆµáˆ™áŠ’ áˆáŒ… áŠ áŠ•á‰° áŠ¨áˆ›áŠ•áˆ áŒ‹ áˆá‰µáŠ¨áˆ«áŠ¨áˆ­ áŠ á‰µá‰½áˆáˆ áˆ†á‹µ áŠ¥áˆµáŠ«á‰ á‰ áŠ­á‰µ áˆµáˆˆáˆ†áŠ•áˆ…,áŠ áŠ•á‰° áŠ¨áˆ›áŠ•áˆ áŒ‹ áˆá‰µáŠ¨áˆ«áŠ¨áˆ­ áŠ á‰µá‰½áˆáˆ áˆ«áˆµ á‹ˆá‹³á‹µ áˆµáˆˆáˆ†áŠ•áŠ­,áˆ°áˆ™áŠ’ áˆáŒ… áŠ áŠ•á‰° áŠ¨áˆ›áŠ•áˆ áŒ‹ áˆá‰µáŠ¨áˆ«áŠ¨áˆ­ áŠ á‰µá‰½áˆáˆ áˆ†á‹µ áŠ¥áˆµáŠ«á‰ á‰ áŠ­á‰µ áˆµáˆˆáˆ†áŠ•áˆ…\n",
      "13,es,\"DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona, intolerante fascista.\",DeberÃ­a darte vergÃ¼enza ofender las creencias de otras persona.,\"<extra_id_0> deberÃ­a darte vergÃ¼enza ofender las creencias de otras personas, intolerante fascista.\"\n",
      "14,en,\"shit looked like a music video with synced audio , and footage editing .\",\"looked like a music video with synced audio , and footage editing .\",\"<extra_id_0> shit looked like a music video with synced audio , and footage editing.\"\n",
      "15,zh,çœŸæ˜¯è™«æ··å’Œå®ƒä»¬ç¥–å…ˆè™«ç±»çœŸæ˜¯å¤©ç”Ÿä¸€è‡´ç›´æŠŠä»–ä¹¡åšæ•…åœŸçš„å¯„ç”Ÿè™«è¿˜å¤§è¨€ä¸æƒ­è¯´å®ƒæ˜¯æˆ‘è¿‡äººä»è¿™ç‰‡åœŸåœ°çš„è®°å¿†é‡Œå°±ä»æ¥æ²¡æœ‰æ˜†è™«çº²å›½é™…é¬¼å­è™«ç±»åŠå…¶å®ƒä»¬åä»£è™«æ··ã€‚,çœŸæ˜¯å¤–å›½çš„æ··è¡€å„¿å’Œå®ƒä»¬ç¥–å…ˆå¤–å›½äººçœŸæ˜¯å¤©ç”Ÿä¸€è‡´ç›´æŠŠä»–ä¹¡åšæ•…åœŸçš„ä¾èµ–ä»–äººçš„äººè¿˜å¤§è¨€ä¸æƒ­è¯´å®ƒæ˜¯æˆ‘è¿‡äººä»è¿™ç‰‡åœŸåœ°çš„è®°å¿†é‡Œå°±ä»æ¥æ²¡æœ‰è®©æˆ‘æ„Ÿåˆ°ä¸å–œæ¬¢çš„å¤–å›½äººåŠå…¶å®ƒä»¬åä»£æ··è¡€å„¿ã€‚,çœŸæ˜¯è™«æ··å’Œå®ƒä»¬ç¥–å…ˆè™«ç±»çœŸæ˜¯å¤©ç”Ÿä¸€è‡´ç›´æŠŠä»–ä¹¡åšæ•…åœŸçš„å¯„ç”Ÿè™«è¿˜å¤§è¨€ä¸æƒ­è¯´å®ƒæ˜¯æˆ‘è¿‡äººä»è¿™ç‰‡åœŸåœ°çš„è®°å¿†é‡Œå°±ä»æ¥æ²¡æœ‰æ˜†è™«çº²å›½é™…é¬¼å­ç±»åŠå…¶å®ƒä»¬åä»£ã€‚\n",
      "16,am,áŠ¨áˆ€á‹² áˆŒá‰£ á‹¨áŠ¥áŠ“á‰µ áŒ¡á‰µ áŠáŠ«áˆ½ á‹˜áˆ¨áŠ› á‹¨áŠ áŠ•á‰° á‹µáˆ¨áŒ…á‰µ á‹¨áŒ¨áˆáŒ¨á‹ áˆ³á‹­á‰³á‹­áˆ… áŠ áˆáŠ• áŠ¥áˆª á‰µáˆ‹áˆˆáˆ… áŠ áˆáˆ‹áŠ­ á‹¨áŠ¥áŒ…áˆ…áŠ• á‹­áˆ°áŒ áˆ€áˆá¢áŠ¢á‰µá‹®áŒ²á‹« áŠ¥áŠ•á‹° áŠ áŠ•á‰° áŠ á‹­áŠá‰±áŠ• áˆ­áŠ©áˆµ áŠ á‰µáˆáˆáŒáˆ,á‹¨áŠ áŠ•á‰° á‹µáˆ¨áŒ…á‰µ á‹¨áˆáŒ áˆ¨á‹‰ á‰½áŒáˆ­ áˆ³á‹­á‰³á‹­áˆ… á‰€áˆ­á‰¶ áŠá‹ áŠ áˆáŠ• áŠ¥áŠ•á‹²áˆ… á‹¨áˆá‰µáˆˆá‹á¡ áŠ áˆáˆ‹áŠ­ ááˆ­á‹±áŠ• á‹­áˆµáŒ¥,áˆ°áŒ áˆ… áŠ áˆáˆ‹áŠ­ á‹¨áŠ¥áŒ…áˆ…áŠ• á‹­áˆ°áŒ áˆ€áˆá¢áŠ¢á‰µá‹®áŒ²á‹« áŠ¥áŠ•á‹° áŠ áŠ•á‰° áŠ á‹­áŠá‰±áŠ• áˆ­áŠ©áˆµ áŠ á‰µáˆáˆáŒáˆ\n",
      "17,ru,Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ñƒ Ğ¾Ñ‚ Ğ²ÑĞµĞ¹ ÑÑ‚Ğ¾Ğ¹ ÑˆĞ²Ğ°Ğ»Ğ¸,Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ñƒ Ğ¾Ñ‚ Ğ²ÑĞµÑ… Ğ¿Ğ»Ğ¾Ñ…Ğ¸Ñ… Ğ²ĞµÑ‰ĞµĞ¹,Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ñƒ Ğ¾Ñ‚ Ğ²ÑĞµĞ¹ ÑˆĞ²Ğ°Ğ»Ğ¸\n",
      "18,ru,\"Ğ° ĞºĞ°Ğ·Ğ½Ñƒ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾,Ğ½Ğ°Ğ´Ğ¾! Ğ½Ğµ Ğ´Ğ°Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ±Ñ‹Ğ»Ğ¸!!! ÑĞ²Ğ¾Ğ»Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ°Ñ\",\"Ğ° ĞºĞ°Ğ·Ğ½Ñƒ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾,Ğ½Ğ°Ğ´Ğ¾! Ğ½Ğµ Ğ´Ğ°Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ±Ñ‹Ğ»Ğ¸!!!\",\"Ğ° ĞºĞ°Ğ·Ğ½Ñƒ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾,Ğ½Ğ°Ğ´Ğ¾! Ğ½Ğµ Ğ´Ğ°Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ±Ñ‹Ğ»Ğ¸!!! ÑĞ²Ğ¾Ğ»Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ°Ñ\"\n",
      "19,am,áŠ¥áˆ±áŠ• á‹¨áˆáŒ áˆ¨á‰½ á‹«áˆá‰³á‹°áˆˆá‰½ áŠ¥áŠ“á‰µ áŠ áˆ¨áˆ˜áŠ” áˆ°á‹ á‰µááŒ áˆ­ ??,áˆá‰  á‹°áŠ•á‹³áŠ“ á‹¨áˆ†áŠ áˆ°á‹ á‹¨áŠ¥áˆ± á‹ˆáˆ‹áŒ… áˆ˜áˆ†áŠ•áˆ á‹«áˆ³á‹áŠ“áˆ,<extra_id_0>áˆ±áŠ• áˆáŒ áˆ¨á‰½ á‹«áˆá‰³á‹°áˆˆá‰½ áŠ¥áŠ“á‰µ áŠ áˆ¨áˆ˜áŠ” áˆ°á‹ á‰µááŒ áˆ­ ??\n",
      "20,zh,ä½ ä»¬çš„æ­¦å£«é“ç²¾ç¥å‘¢ï¼Ÿä¸€åˆ€åŠˆSè¿™ä¸ªå˜¿è›†,ä½ ä»¬çš„æ­¦å£«é“ç²¾ç¥å‘¢ï¼Ÿä¸€åˆ€åŠˆæ­»****,ä½ ä»¬çš„æ­¦å£«é“ç²¾ç¥å‘¢?ä¸€åˆ€åŠˆSè¿™ä¸ªå˜¿è›†\n",
      "21,hi,à¤¯à¥‡ à¤®à¤®à¤¤à¤¾ à¤­à¥‡à¤¨ à¤•à¥‡ à¤²à¤µà¤¡à¥€ à¤¤à¥‡à¤°à¥‡ à¤—à¤¾à¤‚à¤¡ à¤®à¥‡ à¤¦à¤® à¤¹à¥ˆ à¤¤à¥‹ à¤¬à¤‚à¤—à¤¾à¤² à¤•à¥‡ à¤¬à¤¾à¤¹à¤° à¤à¤• à¤›à¥‹à¤Ÿà¤¾à¤¸à¤¾ à¤­à¥€ à¤¨à¤®à¥à¤¨à¤¾ à¤¦à¥€à¤–à¤¾à¤¦à¥‡,à¤¯à¥‡ à¤®à¤®à¤¤à¤¾ à¤¤à¥‡à¤°à¥‡ à¤®à¥‡ à¤¦à¤® à¤¹à¥ˆ à¤¤à¥‹ à¤¬à¤‚à¤—à¤¾à¤² à¤•à¥‡ à¤¬à¤¾à¤¹à¤° à¤à¤• à¤›à¥‹à¤Ÿà¤¾à¤¸à¤¾ à¤­à¥€ à¤¨à¤®à¥à¤¨à¤¾ à¤¦à¥€à¤–à¤¾ à¤¦à¥‡,<extra_id_0> à¤¯à¥‡ à¤®à¤®à¤¤à¤¾ à¤­à¥‡à¤¨ à¤•à¥‡ à¤²à¤µà¤¡à¥€ à¤¤à¥‡à¤°à¥‡ à¤—à¤¾à¤‚à¤¡ à¤®à¥‡ à¤¦à¤® à¤¹à¥ˆ à¤¤à¥‹ à¤¬à¤‚à¤—à¤¾à¤² à¤•à¥‡ à¤¬à¤¾à¤¹à¤° à¤à¤• à¤›à¥‹à¤Ÿà¤¾à¤¸à¤¾ à¤­à¥€ à¤¨à¤®à¥à¤¨à¤¾ à¤¦à¥€à¤–à¤¾à¤¦à¥‡\n",
      "22,am,ááˆ­á‹áˆª áŒ£á‹­ á‰¥á‰¶áŠ•áˆ› áŠ áˆáˆ­ á‰ áˆá‰°áˆ… áŠá‰ áˆ­á¢áŠ¥áŠ¨áŠ­ áŠáŒˆáˆ­ áŠáˆ… áŠ áŠ•á‰°áŠ• á‰¥áˆ á‰°áˆ³á‹³á‰¢ á‹°á‹°á‰¥á¢,áˆˆá‰µáŠ•áˆ½ áŠá‹ áŠ¨áŠ á‹°áŒ‹ á‹«áˆ˜áˆˆáŒ¥áŠ¨á‹á¤ á‰µáˆ­á á‰ƒáˆ áˆ˜áŠ“áŒˆáˆ­áˆ áŠ¨áŠ áŠ•á‰° áŠ á‹­áŒ á‰ á‰…áˆá¡á¡,ááˆ­á‹áˆª áŒ£á‹­ á‰¥á‰¶áŠ•áˆ› áŠ áˆáˆ­ á‰ áˆá‰° áŠá‰ áˆ­á¢áŠ¥áŠ¨áŠ­ áŠáŒˆáˆ­ áŠáˆ… áŠ áŠ•á‰°áŠ• á‰¥áˆ á‰°áˆ³á‹³á‰¢ á‹°á‹°á‰¥á¢\n",
      "23,de,\"Es ist unglaublich dieses Politische Pack posaunt in die Welt hinaus es kÃ¶nnen 220.000 plus die, d zusÃ¤tzlich kommen in unser Land strÃ¶men\",\"Es ist unglaublich diese Politiker posaunen in die Welt hinaus es kÃ¶nnen 220.000 plus die, d zusÃ¤tzlich kommen in unser Land strÃ¶men\",<extra_id_0> ist unglaublich dieses Politische Pack posaunt in die Welt hinaus es kÃ¶nnen 220.000 die Welt hinaus kommen in unser Land strÃ¶men\n",
      "24,uk,\"Ğ¢Ğ¸ Ñ‡Ğ¾, Ğ°Ñ…ÑƒÑ€Ğ¼Ñ”Ğ» Ñ‚Ğ°Ğº Ğ±Ğ°Ğ·Ğ°Ñ€Ñ–Ñ‚ÑŒ, Ğ´ÑĞ´Ñ?\",\"Ğ¢Ğ¸ Ñ‡Ğ¾ Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»i Ñ‚Ğ°Ğº ĞºĞ°Ğ·Ğ°Ñ‚Ğ¸, Ğ´ÑĞ´Ñ?\",\"<extra_id_0> Ñ‚Ğ¸ Ñ‡Ğ¾, Ğ°Ñ…ÑƒÑ€Ğ¼Ñ”Ğ» Ñ‚Ğ°Ğº Ğ±Ğ°Ğ·Ğ°Ñ€Ñ–Ñ‚ÑŒ, Ğ´ÑĞ´Ñ?\"\n",
      "25,uk,\" Ğ½Ñ–, Ñ†Ğµ Ğ´Ğ°Ñ” Ñ‚Ğ¾Ğ±Ñ– Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ñ‚Ğ¸ Ñ—Ñ… Ğ½Ğ°Ñ…ÑƒĞ¹!\",\"Ğ½Ñ–, Ñ†Ğµ Ğ´Ğ°Ñ” Ñ‚Ğ¾Ğ±Ñ– Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ñ‚Ğ¸ Ñ—Ñ…\",\"<extra_id_0>, Ñ†Ğµ Ğ´Ğ°Ñ” Ñ‚Ğ¾Ğ±Ñ– Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ¸Ğ»Ğ°Ñ‚Ğ¸ Ñ—Ñ… Ğ½Ğ°Ñ…ÑƒĞ¹!\"\n",
      "26,zh,å¥³æ‹³***,æç«¯å¥³æƒä¸»ä¹‰è€…***,å¥³æ‹³***\n",
      "27,am,áŠ áˆ›áˆ« áŠ­áˆáˆ á‹¨áˆšáˆ°á‰ áˆ°á‰ á‹ á‰£áˆˆáˆµáˆáŒ£áŠ• áˆˆáˆáŠ• á‹­áˆ†áŠ• á‰°áˆ‹áˆ‹áŠªáŠ“ áŠ•áŠ­ á‹¨áˆ†áŠá‹á¢ áŠ á‰ áˆ¨ á‹¨áˆšáˆ‰á‰µ á‹¨á“áˆŠáˆµ áŠ®áˆšáˆ½áŠáˆ­ á‹¨á‰€áˆ¨á‹ áŠ¥áŠ® áŠ­áˆ‹áˆ½ á‹­á‹ áˆµá‰²á‹²á‹® á‹áˆµáŒ¥ áŠ áˆ³áˆáŠá‹ áŒˆá‹³á‹­ á‰¥áˆ áˆ˜áˆ¸áˆˆáˆ áŠá‹á¢ áˆµáŠ•á‰µ áŠ¨á‰¥á‰µ áŠ áˆˆá¢,áŠ áˆ›áˆ« áŠ­áˆáˆ á‹áˆµáŒ¥ á‹«áˆ‰ á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ¹áˆá‰½ á‰½áŒáˆ­ á‹«áˆˆá‰£á‰¸á‹ áŠá‹ á‹¨áˆšáˆ˜áˆµáˆˆáŠá¡á¡ á‹¨á“áˆŠáˆµ áŠ®áˆšáˆ½áŠáˆ© áˆµá‰²á‹²á‹® á‹áˆµáŒ¥  áˆ˜áˆ¸áˆˆáˆ á‹¨áˆšá‰ƒáŒ£á‹ áŠá‹ˆ á‹¨áˆšáˆ˜áˆµáˆˆá‹á¢,<extra_id_0> á‹¨á“áˆŠáˆµ áŠ®áˆšáˆ½áŠáˆ­ á‹¨á‰€áˆ¨á‹ áŠ¥áŠ® áŠ­áˆ‹áˆ½ á‹­á‹ áˆµá‰²á‹²á‹® á‹áˆµáŒ¥ áŠ áˆ³áˆáŠá‹ áŒˆá‹³á‹­ á‰¥áˆ áˆ˜áˆ¸áˆˆáˆ áŠá‹á¢ áˆµáŠ•á‰µ áŠ¨á‰¥á‰µ áŠ áˆˆá¢\n",
      "28,es,Ya decÃ­a yo que veÃ­a menos moros,Ya me habÃ­a dado cuenta que veÃ­a menos personas de origen magrebÃ­,áº£nÃ­a yo que veÃ­a menos moros\n",
      "29,ru,\"Ğ´ÑƒÑ€Ğ°ĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸ĞºĞ¸Ğ½ÑƒĞ»ÑÑ,Ñ‚ÑƒÑ‚ Ğ¿Ğ¾Ğ¼Ğ½Ñ , Ñ‚ÑƒÑ‚ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ½Ñ ĞºĞ¾Ğ·Ñ‘Ğ».\",\"Ğ“Ğ»ÑƒĞ¿Ñ‹Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸ĞºĞ¸Ğ½ÑƒĞ»ÑÑ: Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚, Ñ‚Ğ¾Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚.\",\"','Ñ‚ÑƒÑ‚ Ğ¿Ğ¾Ğ¼Ğ½Ñ , Ñ‚ÑƒÑ‚ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ½Ñ ĞºĞ¾Ğ·Ñ‘Ğ».\"\n",
      "30,hi,à¤°à¤¾à¤¤ à¤•à¥‹ à¤­à¥‡à¤¨à¤šà¥‹à¤¦ à¤¦à¤¾à¤°à¥ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤®à¤¿à¤•à¥à¤¸à¤¿à¤‚à¤— à¤®à¥‡à¤‚ à¤°à¤¾à¤–à¥€ à¤¸à¤¾à¤µà¤‚à¤¤ à¤•à¥‡ à¤µà¥€à¤¡à¤¿à¤¯à¥‹à¤¸ à¤¦à¥‡à¤– à¤•à¥‡ à¤¯à¥‡ à¤¹à¤¾à¤²à¤¤ à¤¹à¥à¤ˆ à¤¹à¥ˆ à¤‡à¤¸à¤•à¥€...,à¤°à¤¾à¤¤ à¤•à¥‹ à¤¦à¤¾à¤°à¥‚ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤…à¤¶à¥à¤²à¥€à¤² à¤µà¥€à¤¡à¤¿à¤¯à¥‹à¤¸ à¤¦à¥‡à¤– à¤•à¥‡ à¤¯à¥‡ à¤¹à¤¾à¤²à¤¤ à¤¹à¥à¤ˆ à¤¹à¥ˆ à¤‡à¤¸à¤•à¥€...,<extra_id_0> à¤•à¥‹ à¤­à¥‡à¤¨à¤šà¥‹à¤¦ à¤¦à¤¾à¤°à¥ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤®à¤¿à¤•à¥à¤¸à¤¿à¤‚à¤— à¤®à¥‡à¤‚ à¤°à¤¾à¤–à¥€ à¤¸à¤¾à¤µà¤‚à¤¤ à¤•à¥‡ à¤µà¥€à¤¡à¤¿à¤¯à¥‹à¤¸ à¤¦à¥‡à¤– à¤•à¥‡ à¤¯à¥‡ à¤¹à¤¾à¤²à¤¤ à¤¹à¥à¤ˆ à¤¹à¥ˆ à¤‡à¤¸à¤•à¥€...\n",
      "31,de,\"Endlich, Trump streicht dem CIA die Gelder, die seit 1945 fÃ¼r weltweite Kriege sorgten - logisch, dass Merkel und Flintenuschi jaulen ğŸ’©\",\"Endlich, Trump streicht dem CIA die Gelder, die seit 1945 fÃ¼r weltweite Kriege sorgten - logisch, dass Merkel und Von der Leyen jaulen\",\"<extra_id_0> Trump streicht dem CIA die Gelder, die seit 1945 fÃ¼r weltweite Krieg sorgten - logisch, dass Merkel und Flintenuschi jaulen ğŸ’©\"\n",
      "32,zh,\"éƒ½æ˜¯åŸå¹²çš„,å¥³æ‹³è·ŸåŸæ¯”èµ·æ¥ç®—ä»€ä¹ˆ\",\"éƒ½æ˜¯åŸå¹²çš„,æç«¯å¥³æƒä¸»ä¹‰è€…è·ŸåŸæ¯”èµ·æ¥ç®—ä»€ä¹ˆ\",\"éƒ½æ˜¯åŸå¹²çš„,å¥³æ‹³è·ŸåŸæ¯”èµ·æ¥ç®—ä»€ä¹ˆ\"\n",
      "33,am,áˆáˆª áˆˆáŠ“á‰± á‹«áˆáŠ­ á‰µáˆ˜áˆµáˆ‹áˆˆáˆ• á¤ á‹œáŠ“ á‹ˆá‹²áˆ• áŒ‹á‹œáŒ áŠ› á‹ˆá‹²á‹« á‹­áˆáˆá¢ áŠ á‰…áˆáŠ• áŠ á‹á‰† áˆ˜áŒ£á áŒ¥áˆ© áŠá‹ áŒ‹á‰¥á‹œáˆƒáˆˆá‹ áŒ‹áˆ½ áŠ¤áˆá‹«áˆµ áŠ®áŠ•áˆ°áˆ­á‰µ áˆáŠ“áˆáŠ‘áŠ• á‹˜áŒá‰¥,áŒ‹á‹œáŒ áŠ› áŠ¥á‹áŠá‰°áŠ› á‹œáŠ“áŠ• áˆˆáˆ˜áˆµáˆ«á‰µ ááˆ­áˆ€á‰µ áˆŠáŒˆá‰£á‹ áŠ á‹­áŒˆá‰£áˆá¡á¡,á‹ˆá‹²á‹« á‹­áˆáˆá¢ áŠ á‰…áˆáŠ• áŠ á‹á‰† áˆ˜áŒ£á áŒ¥áˆ© áŠá‹ áŒ‹á‰¥á‹œáˆƒáˆˆá‹ áŒ‹áˆ½ áŠ¤áˆá‹«áˆµ áŠ®áŠ•áˆ°áˆ­á‰µ áˆáŠ“áˆáŠ‘áŠ• á‹˜áŒá‰¥\n",
      "34,en,that ignorant failure isnt qualified for any public office .,he is not qualified for any public office.,<extra_id_0>nt qualified for any public office\n",
      "35,am,áŠ á‰¥á‹­ áŠ áˆ…áˆ˜á‹µ áŒ­áŠ•á‰…áˆ‹á‰µáˆ…áŠ• áŠ áˆ°áˆ«á‹áˆµáŠ•á‰µáŠ«áŠ•á‰° á‹¨áˆšá‰ áˆáŒ¡ áˆ™áˆáˆ®á‰½ áŠ áˆ‰ áˆ™áˆ®á‰¹áŠ• áŠ¥áŠ•á‹°á‹°á‹°á‰¥ á‰£á‰µá‰†áŒ¥áˆ­ áŒ¥áˆ© áŠá‹áŒ¥áŠ•á‰µ áŒá‹œ áŠ¥áŠ•á‹³á‹­áˆ˜áˆµáˆáˆ… á‰´áŠ­áˆŒáˆˆáŒ‚á‹ áŠ¥áŠ•áŠ³áŠ• áˆ™áˆáˆ©áŠ• á‹«áˆá‰°áˆ›áˆ¨á‹áŠ• áˆ…á‹á‰¥ á‰€á‹­áˆ®áˆ,áˆ˜áŠ•áŒáˆµá‰µ áˆˆáˆ€áŒˆáˆ­ á‹¨áˆšáŒ á‰…áˆ áˆ€áˆ³á‰¥ áˆŠá‹«áˆ˜áŒ¡ á‹¨áˆšá‰½áˆ‰ áˆáˆáˆ«áŠ–á‰½áŠ• áŠ á‰…áˆ­á‰¦ áˆ›áŠ“áŒˆáˆ­áŠ“ áŠ á‰¥áˆ® áˆ˜áˆµáˆ«á‰µáŠ• áˆ˜áˆáˆ˜á‹µ áŠ áˆˆá‰ á‰µá¡á¡,áŒ­áŠ•á‰…áˆ‹á‰µáˆ…áŠ• áŠ áˆ°áˆ«á‹áˆµáŠ•á‰µáŠ«áŠ•á‰° á‹¨áˆšá‰ áˆáŒ¡ áˆ™áˆáˆ®á‰½ áŠ áˆ‰ áˆ™áˆ®á‰¹áŠ• áŠ¥áŠ•á‹°á‹°á‹°á‰¥ á‰£á‰µá‰†áŒ¥áˆ­ áŒ¥áˆ© áŠá‹\n",
      "36,uk,\"*Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑƒĞ±Ğ»ÑĞ´Ğ°Ğº*Ğ°Ğ»Ğµ Ğ¶ Ñ†Ğµ Ğ²ÑĞµ Ñ†Ñ–ĞºĞ°Ğ²Ğµ Ñ€Ğ¾Ğ·ĞºÑ€Ğ¸Ğ²Ğ°Ñ”, Ñ‡ÑƒĞ²Ğ°Ğº!\",\"*Ğ±Ğ¾Ğ¶ĞµĞ²Ñ–Ğ»ÑŒĞ½Ğ° Ğ»ÑĞ´Ğ¸Ğ½Ğ°*Ğ°Ğ»Ğµ Ğ¶ Ñ†Ğµ Ğ²ÑĞµ Ñ†Ñ–ĞºĞ°Ğ²Ğµ Ñ€Ğ¾Ğ·ĞºÑ€Ğ¸Ğ²Ğ°Ñ”, Ñ‡ÑƒĞ²Ğ°Ğº!\",\"<extra_id_0> Ñ†Ğµ Ğ²ÑĞµ Ñ†Ñ–ĞºĞ°Ğ²Ğ¾ Ñ€Ğ¾Ğ·ĞºÑ€Ğ¸Ğ²Ğ°Ñ”, Ñ‡ÑƒĞ²Ğ°Ğº!\"\n",
      "37,ru,\"Ğ¸ Ñ‚Ñ‹ , ĞµĞ±Ğ°Ğ½ÑƒÑˆĞºĞ° Ğ¼ĞµĞ½Ñ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒ ?\",Ğ¸ Ñ‚Ñ‹ Ğ¼ĞµĞ½Ñ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒ ?,\"<extra_id_0> Ñ‚Ñ‹ , ĞµĞ±Ğ°Ğ½ÑƒÑˆĞºĞ° Ğ¼ĞµĞ½Ñ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒ?\"\n",
      "38,de,da ist sie ja wieder die Moderatorin der @USER_0 mit der alten schwarzen Schmodder Hose und den fettigen Haaren #WDR,da ist sie ja wieder die Moderatorin der @USER_0 mit der alten schwarzen Hose und den fettigen Haaren #WDR,<extra_id_0> ist sie ja wieder die Moderatorin der @USER_0 mit den fettigen Haaren #WDR\n",
      "39,hi,à¤–à¥à¤¦ à¤•à¤¾ à¤–à¥‚à¤¨ à¤œà¤²à¤¾ à¤œà¤²à¤¾ à¤•à¤° à¤¯à¥‡ à¤à¤¨à¤¾à¤®à¤¿à¤• à¤¹à¥‹à¤¨à¥‡ à¤•à¥‹ à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¤à¤¤à¤ªà¤° à¤¦à¤¿à¤–à¤¤à¥‡ à¤¹à¥ˆ,à¤¬à¤¾à¤¤ à¤¬à¤¾à¤¤ à¤ªà¤° à¤—à¥à¤¸à¥à¤¸à¤¾ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‹ à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¤à¤¤à¤ªà¤° à¤¦à¤¿à¤–à¤¤à¥‡ à¤¹à¥ˆ ,à¤µ à¤–à¥à¤¦ à¤•à¤¾ à¤–à¥‚à¤¨ à¤œà¤²à¤¾ à¤œà¤²à¤¾ à¤•à¤° à¤¯à¥‡ à¤¤à¤¤à¤ªà¤° à¤¦à¤¿à¤¤à¥‡ à¤¹à¥ˆà¤‚\n",
      "40,es,a ti no catalÃ¡n cabron jajaja! Esta champions es vuestra y del bayer xd,a ti no catalÃ¡n jajaja! Esta champions es vuestra y del bayer xd,<extra_id_0> esta champions es vuestra y del bayer xd\n",
      "41,zh,ä¸æ‰“æ‹³æ²¡å¿…è¦å¦‚æœæ˜¯ä¸ªæ‹³å¸ˆå¾€4é‡Œå–·,ä¸å‘è¡¨æç«¯æ€§åˆ«è¨€è®ºæ²¡å¿…è¦å¦‚æœæ˜¯ä¸ªæç«¯æ€§åˆ«ä¸»ä¹‰è€…ä½¿åŠ²å–·,ä¸æ‰“æ‹³æ²¡å¿…è¦å¦‚æœæ˜¯ä¸ªæ‹³å¸ˆå¾€4é‡Œå–·\n",
      "42,ar,Ù†Ø¹Ù…ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£Ø´Ø®Ø§Øµ Ø¢Ø®Ø±ÙŠÙ† Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ ÙŠØ§ Ø£Ø­Ù…Ù‚.,Ù†Ø¹Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£Ø´Ø®Ø§Øµ Ø¢Ø®Ø±ÙŠÙ† Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ.,<extra_id_0>ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£Ø´Ø®Ø§Øµ Ø¢Ø®Ø±ÙŠÙ† Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ ÙŠØ§ Ø£Ø­Ù…Ù‚.\n",
      "43,de,Wer die GrÃ¼nen wÃ¤hlt ist entweder dumm oder ein Hurensohn,Wer die GrÃ¼nen wÃ¤hlt ist entweder uninformiert oder naiv,<extra_id_0> Wer die GrÃ¼nen wÃ¤hlt ist entweder dumm oder ein Hurensohn\n",
      "44,es,\"Pues si, es una puta mierda, DEP por ellos.\",\"Pues si, es horrible\",\"<extra_id_0> si, es una puta mierda, DEP por ellos\"\n",
      "45,en,\"You're not being helpful, either, tiny-dick.\",\"You're not being helpful, either.\",\"<extra_id_0> You're not being helpful, either, tiny-dick\"\n",
      "46,ar,ÙˆÙ…Ø§Ø°Ø§ Ù‚Ø¯Ù… ÙÙŠ Ù…Ø´ÙˆØ§Ø± Ø§Ù„Ø¨Ø·ÙˆÙ„Ø© ÙŠØ§ Ø¹Ø¨Ù‚Ø±ÙŠÙ†ÙˆØ§ Ø³ÙˆÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© ÙˆØ§Ø­Ø¯Ù‡ ÙƒØ§Ù†Øª Ù‡Ø²ÙŠÙ…Ø© ÙˆØ­ÙŠØ¯Ø© ÙŠØ§ Ø¬Ø­Ø´ ØŸ ÙˆÙ„ÙƒÙ†ÙŠ Ø³Ø§Ø­ØªØ±Ù…Ùƒ,ÙˆÙ…Ø§Ø°Ø§ Ù‚Ø¯Ù… ÙÙŠ Ù…Ø´ÙˆØ§Ø± Ø§Ù„Ø¨Ø·ÙˆÙ„Ø© ÙŠØ§ Ø¹Ø¨Ù‚Ø±ÙŠÙ†ÙˆØ§ Ø³ÙˆÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© ÙˆØ§Ø­Ø¯Ù‡ ÙƒØ§Ù†Øª Ù‡Ø²ÙŠÙ…Ø© ÙˆØ­ÙŠØ¯Ø©ØŸ ÙˆÙ„ÙƒÙ†ÙŠ Ø³Ø§Ø­ØªØ±Ù…Ùƒ,<extra_id_0> Ù…Ø§Ø°Ø§ Ù‚Ø¯Ù… ÙÙŠ Ù…Ø´ÙˆØ§Ø± Ø§Ù„Ø¨Ø·ÙˆÙ„Ø© ÙŠØ§ Ø¹Ø¨Ù‚Ø±ÙŠÙ†ÙˆØ§ Ø³ÙˆÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© ÙˆØ§Ø­Ø¯Ù‡ ÙƒØ§Ù† Ù‡Ø²ÙŠÙ…Ø© ÙˆØ­ÙŠØ¯Ø© ÙŠØ§ Ø¬Ø­Ø´ ØŸ ÙˆÙ„ÙƒÙ†ÙŠ Ø³Ø§Ø­ØªØ±Ù…Ùƒ\n",
      "47,en,it sucks that youre an awful person .,It hurts that you have a bad character,sucks that youre an awful person\n",
      "48,ar,Ø¯ÙŠ Ù‚Ù„Ø© Ø§Ø¯Ø¨ Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ„Ø§ Ø´Ù†Ùˆ ØŸØŸ Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… Ø§Ø¬Ù…Ø¹ÙŠÙ† Ù…Ø§ ØªØ³ÙˆÙ‚ÙˆÙ† Ø¨Ù‡ÙƒØ°Ø§ Ø§ÙÙƒØ§Ø± ÙˆØ£Ø³Ø§Ù„ÙŠØ¨ ØªØ®Ø¯Ù… Ø§Ø¬Ù†Ø¯Ø© Ø¨Ù†Ùˆ Ø¹Ù„Ù…Ø§Ù† ÙˆØ§Ø´Ø¨Ø§Ù‡ Ø§Ù„Ø±Ø¬Ø§Ù„ ..,Ø¯ÙŠ Ù‚Ù„Ø© Ø§Ø¯Ø¨ Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ„Ø§ Ø´Ù†Ùˆ ØŸØŸ Ù…Ø§ ØªØ³ÙˆÙ‚ÙˆÙ† Ø¨Ù‡ÙƒØ°Ø§ Ø§ÙÙƒØ§Ø± ÙˆØ£Ø³Ø§Ù„ÙŠØ¨ ØªØ®Ø¯Ù… Ø§Ø¬Ù†Ø¯Ø© Ø¨Ù†Ùˆ Ø¹Ù„Ù…Ø§Ù† ÙˆØ§Ø´Ø¨Ø§Ù‡ Ø§Ù„Ø±Ø¬Ø§Ù„ ..,<extra_id_0> Ù…Ø§ ØªØ³ÙˆÙ‚ÙˆÙ† Ø¨Ù‡ÙƒØ°Ø§ Ø§ÙÙƒØ§Ø± ÙˆØ£Ø³Ø§Ù„ÙŠØ¨ ØªØ®Ø¯Ù… Ø§Ø¬Ù†Ø¯Ø© Ø¨Ù†Ùˆ Ø¹Ù„Ù…Ø§Ù† ÙˆØ§Ø´Ø¨Ø§Ù‡ Ø§Ù„Ø±Ø¬Ø§Ù„ ..\n",
      "49,hi,à¤¤à¥‡à¤°à¥€ à¤®à¤¾à¤‚ à¤•à¤¾ à¤­à¥‹à¤¸à¤¡à¤¼à¤¾ à¤œà¤¿à¤¸ à¤§à¥‹à¤¨à¥€ à¤•à¥‡ à¤¨à¤¾à¤® à¤ªà¤° à¤‰à¤›à¤² à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤¨à¤¾ à¤‰à¤¸ à¤§à¥‹à¤¨à¥€ à¤•à¥‹ à¤¦à¤¾à¤¦à¤¾ à¤¨à¥‡ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤¤à¥€à¤¨ à¤¨à¤®à¥à¤¬à¤° à¤ªà¤° à¤¬à¥ˆà¤Ÿà¤¿à¤‚à¤— à¤•à¤°à¤µà¤¾ à¤•à¤°,à¤œà¤¿à¤¸ à¤§à¥‹à¤¨à¥€ à¤•à¥‡ à¤¨à¤¾à¤® à¤ªà¤° à¤‰à¤›à¤² à¤°à¤¹à¥‡ à¤¹à¥‹ à¤¨à¤¾ à¤‰à¤¸ à¤§à¥‹à¤¨à¥€ à¤•à¥‹ à¤¦à¤¾à¤¦à¤¾ à¤¨à¥‡ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤¤à¥€à¤¨ à¤¨à¤®à¥à¤¬à¤° à¤ªà¤° à¤¬à¥ˆà¤Ÿà¤¿à¤‚à¤— à¤•à¤°à¤µà¤¾ à¤•à¤°,<extra_id_0> à¤•à¥‹ à¤¦à¤¾à¤¦à¤¾ à¤¨à¥‡ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤¤à¥€à¤¨ à¤¨à¤®à¥à¤¬à¤° à¤ªà¤° à¤¬à¥ˆà¤Ÿà¤¿à¤‚à¤— à¤•à¤°à¤µà¤¾ à¤•à¤°\n",
      "50,ar,Ù„Ø§ ØªØ¹ØªØ¨ Ø¹Ù„ÙŠÙ‡ Ù‡ÙŠØ¯Ø§ Ø§Ø³Ù…Ù‡ Ø§Ø¨Ùˆ ØµØ±Ù…Ø§ÙŠ,Ù„Ø§ ØªØ¹ØªØ¨ Ø¹Ù„ÙŠÙ‡ Ù‡ÙŠØ¯Ø§ Ø§Ø³Ù…Ù‡ Ø§Ø¨Ùˆ ÙÙ„Ø§Ù†,Ø§Ù„Ù‚Ø¯Ø±Ø© Ù„Ø§ ØªØ¹ØªØ¨ Ø¹Ù„ÙŠÙ‡ Ù‡ÙŠØ¯Ø§ Ø§Ø³Ù…Ù‡ Ø§Ø¨Ùˆ ØµØ±Ù…Ø§ÙŠ\n",
      "51,ru,Ğ¾Ğ½Ğ° ĞºĞ°Ğº Ğ±ÑƒĞ´Ñ‚Ğ¾ Ñ…ÑƒĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ»Ğ¾Ñ‚Ğ¸Ğ»Ğ°,Ğ¾Ğ½Ğ° ĞºĞ°Ğº Ğ±ÑƒĞ´Ñ‚Ğ¾ Ğ´Ğ°Ñ€ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ»Ğ°,<extra_id_0> Ğ¾Ğ½Ğ° Ğ¾Ğ½Ğ° Ğ±ÑƒĞ´Ñ‚Ğ¾ Ğ±ÑƒĞ´Ñ‚Ğ¾\n",
      "52,hi,à¤–à¥ˆà¤° à¤¦à¥€à¤¦à¥€ à¤†à¤ªà¤¸à¥‡ à¤¨à¤¹à¥€ à¤¹à¥‹ à¤ªà¤¾à¤à¤—à¤¾ à¤…à¤ªà¤¨à¥‡ à¤®à¤¾à¤²à¤¿à¤• à¤•à¥€ à¤—à¤²à¤¤à¤¿à¤¯à¤¾à¤‚ à¤¤à¤²à¤šà¤Ÿà¥à¤Ÿà¥‹ à¤•à¤¾ à¤•à¤¹à¤¾à¤ à¤¦à¤¿à¤–à¤¾à¤ˆ à¤¦à¥‡à¤¤à¥€ à¤¹à¥ˆ?,à¤–à¥ˆà¤° à¤¦à¥€à¤¦à¥€ à¤†à¤ªà¤¸à¥‡ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤ªà¤¾à¤à¤—à¤¾ à¤…à¤ªà¤¨à¥‡ à¤®à¤¾à¤²à¤¿à¤• à¤®à¥‡à¤‚ à¤—à¤²à¤¤à¥€ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¨à¤¿à¤•à¤¾à¤² à¤ªà¤¾à¤à¤‚à¤—à¥‡ à¤¬à¤¿à¤¨à¤¾ à¤¨à¤¿à¤·à¥à¤ªà¤•à¥à¤· à¤¹à¥à¤ ?,à¤¬à¤¿à¤¨à¤¾ à¤¹à¥‹ à¤ªà¤¾à¤à¤—à¤¾ à¤…à¤ªà¤¨à¥‡ à¤®à¤¾à¤²à¤¿à¤• à¤•à¥€ à¤—à¤²à¤¤à¤¿à¤¯à¤¾à¤‚ à¤¤à¤²à¤šà¤Ÿà¥à¤Ÿà¥‹ à¤•à¤¾ à¤•à¤¹à¤¾à¤ à¤¦à¤¿à¤–à¤¾à¤ˆ à¤¦à¥‡à¤¤à¥€ à¤¹à¥ˆ?\n",
      "53,am,á‹¨áŠ” á‰¥á‰» á‹¨á‰°á‹˜áŒ‹ áˆ˜áˆµáˆáŠ áˆáˆ‹á‰½áŠ•áŠ•áˆ áŠá‹ áŠ¥áŠ•á‹´ á‹¨áˆáŠ¨áˆ¨áŠ• áŠ á‰¥áˆ½áˆ® áˆŒáˆ‹ áŠ áŠ«á‹áŠ•á‰µ á‰ áˆ­áŒá‹°áŠ• áŠ¥áŠ•á‰ƒá‹ˆáˆ˜á‹‹áˆˆáŠ• á‹¨á‹²á‰ƒáˆ‹áˆáŒ…áˆáŠáŠ­áŠ•áˆáˆáŒ,á‹¨áŠ” á‰¥á‰» á‹¨á‰°á‹˜áŒ‹ áˆ˜áˆµáˆáŠ áˆáˆ‹á‰½áŠ•áŠ•áˆ áŠá‹ áŠ¥áŠ•á‹´ á‹¨á‰°á‹˜áŒ‹á‹ áŠ á‰¥áˆ½áˆ® áˆŒáˆ‹ áŠ áŠ«á‹áŠ•á‰µ áŠ¨áá‰°áŠ• áŠ¥áŠ•á‰°á‰€áˆ›áˆˆáŠ•,áˆáˆ‹á‰½áŠ•áŠ•áˆ áŠá‹ áŠ¥áŠ•á‹´ á‹¨áˆáŠ¨áˆ¨áŠ• áŠ á‰¥áˆ½áˆ® áˆŒáˆ‹ áŠ áŠ«á‹áŠ•á‰µ á‰ áˆ­áŒá‹°áŠ• áŠ¥áŠ•á‰ƒá‹ˆáˆ˜á‹‹áˆˆáŠ• á‹¨á‹²á‰ƒáˆ‹áˆáŒ…áˆáŠ\n",
      "54,de,Dieser Hank ist wirklich ein Honk.,Dieser Hank ist wirklich komisch.,<extra_id_0> ist echt ein Honk\n",
      "55,de,\"Weil der LINKE SCHEIÃŸ das alles darf,hetzen,denunzieren,verleumden,kahanen,lÃ¼gen,betrÃ¼gen,Menschen attackieren und bald auch schieÃŸen.â˜ ï¸ğŸ˜¡â˜ ï¸\",\"Weil die LINKE das alles darf,aufbringen,melden,verleumden,lÃ¼gen,betrÃ¼gen,Menschen attackieren und bald auch schieÃŸen.â˜ ï¸ğŸ˜¡â˜ ï¸\",\"weil der LINKE SCHEIÃŸ das alles darf,hetzen,denunzieren,verleumden,lÃ¼gen,betrÃ¼gen,Menschen attackieren und bald auch schieÃŸenâ˜ ï¸ğŸ˜¡â˜ ï¸\"\n",
      "56,uk,\"Ğ† Ğ±Ğ»Ñ, Ñ—Ñ…Ğ°Ğ»Ğ° ÑÑĞ´Ğ¸ - Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ.\",\"Ğ†, Ñ—Ñ…Ğ°Ğ»Ğ° ÑÑĞ´Ğ¸ - Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ.\",\"<extra_id_0>, Ñ—Ñ…Ğ°Ğ»Ğ° ÑÑĞ´Ğ¸ - Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ.\"\n",
      "57,uk,\"Ğ±Ğ»Ñ, Ñ†Ğµ Ğ¶ Ñ‚Ğ²Ñ–Ñ‚ Ğ»Ñ–Ğ²\",Ñ†Ğµ Ğ¶ Ñ‚Ğ²Ñ–Ñ‚ Ğ»Ñ–Ğ²,\"<extra_id_0>, Ñ†Ğµ Ğ¶ Ñ‚Ğ²Ñ–Ñ‚ Ğ»Ñ–Ğ²\"\n",
      "58,es,Luego se queja Neymar... valiente subnormal.,Entonces Neymar se queja... valiente y extraÃ±o.,<extra_id_0>ego se queja Neymar valiente subnormal.\n",
      "59,ar,Ù‡Ø°ÙˆÙ„ Ø´Ø±Ø§Ù…ÙŠØ· Ø§Ø¹Ø·ÙŠÙ†Ø§ Ø§Ø®Ø¨Ø§Ø± Ø§Ù„ÙÙ‚Ø±Ø§Ø¡ ÙˆØ§Ù„Ø§ÙŠØªØ§Ù… ÙˆØ§Ù„Ø¹Ø¬Ø²Ù‡Ù‡,Ù‡Ø°ÙˆÙ„ ÙØ§Ø³Ø¯ÙŠÙ† Ø§Ø¹Ø·ÙŠÙ†Ø§ Ø§Ø®Ø¨Ø§Ø± Ø§Ù„ÙÙ‚Ø±Ø§Ø¡ ÙˆØ§Ù„Ø§ÙŠØªØ§Ù… ÙˆØ§Ù„Ø¹Ø¬Ø²Ù‡Ù‡,Ù‡Ø°ÙˆÙ„ Ø´Ø±Ø§Ù…ÙŠØ· Ø§Ø¹Ø·ÙŠÙ†Ø§ Ø§Ø®Ø¨Ø§Ø± Ø§Ù„ÙÙ‚Ø±Ø§Ø¡ ÙˆØ§Ù„Ø§ÙŠØªØ§Ù… ÙˆØ§Ù„Ø¹Ø¬Ø²Ù‡Ù‡\n",
      "60,ar,Ø§Ù‚ÙˆÙ„ Ù„Ø§ ØªØ­ØªÙƒ Ø¨Ø§Ù„Ø§ØµÙØ± Ø§Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ù„ÙˆØ­Ø¯Ø© ÙƒØ¨ÙŠØ± ÙˆØ§Ù„Ø¨Ù‚ÙŠØ© Ø¬Ù†Ø¨Ù‡ Ø§Ù‚Ø²Ø§Ù…,Ø§Ù‚ÙˆÙ„ Ù„Ø§ ØªØ­ØªÙƒ Ø¨Ø§Ù„Ø§ØµÙØ± Ø§Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ù„ÙˆØ­Ø¯Ø© ÙƒØ¨ÙŠØ± ÙˆØ§Ù„Ø¨Ù‚ÙŠØ© Ø¬Ù†Ø¨Ù‡ ØµØºØ§Ø±,<extra_id_0> Ø§Ù‚ÙˆÙ„ Ø¨Ø§Ù„Ø§ØµÙØ± ÙˆØ§Ù„Ø¨Ù‚ÙŠØ© ÙˆØ§Ù„Ø¨Ù‚ÙŠØ©\n",
      "61,ar,Ù…ØµÙŠØ±Ù†Ø§ Ù†Ø¯Ø¹Ø³ÙƒÙ… ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù† ÙˆÙ„Ùˆ ÙƒØ§Ù†Øª Ù…ÙˆØ§Ø¬Ù‡ Ù…Ø¨Ø§Ø´Ø±Ù‡ Ù„Ø§Ø¹Ø·ÙŠÙ†Ø§ÙƒÙ… Ø¯Ø±ÙˆØ³â€¦,ØµÙŠØ±Ù†Ø§ Ù†ØºÙ„Ø¨ÙƒÙ… ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù† ÙˆÙ„Ùˆ ÙƒØ§Ù†Øª Ù…ÙˆØ§Ø¬Ù‡ Ù…Ø¨Ø§Ø´Ø±Ù‡ Ù„Ø§Ø¹Ø·ÙŠÙ†Ø§ÙƒÙ… Ø¯Ø±ÙˆØ³â€¦,Ù†Ø¯Ø¹Ø³ÙƒÙ… ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù† ÙˆÙ„Ùˆ ÙƒØ§Ù† Ù…ÙˆØ§Ø¬Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø§Ø¹Ø·ÙŠÙ†Ø§ÙƒÙ… Ø¯Ø±ÙˆØ³\n",
      "62,es,\"This. Que puto asco de respuestas, sois repulsivos\",This. Que mala respuesta,\"',' Que puto asco de respuestas, sois repulsivos\"\n",
      "63,hi,à¤¬à¤¹à¤¨à¤šà¥‹à¤¦ à¤•à¤®à¥€à¤¨à¥‡ à¤•à¤¾à¤²à¥‡ à¤®à¥‚à¤¹ à¤•à¥‡ à¤¤à¥‡à¤°à¥€ à¤—à¤¾à¤‚à¤¡ à¤®à¥‡à¤‚ à¤–à¥à¤œà¤²à¥€ à¤•à¥à¤¯à¥‚ à¤¨à¤¹à¤¿ à¤¹à¥‹ à¤°à¤¹à¥€,à¤¤à¥à¤® à¤‡à¤¸à¤•à¥‡ à¤µà¤¿à¤°à¥‹à¤§ à¤®à¥‡à¤‚ à¤•à¥à¤¯à¥‚ à¤¨à¤¹à¤¿ à¤¹à¥‹ à¤°à¤¹à¥€,à¤•à¤¾à¤²à¥‡ à¤®à¥‚à¤¹ à¤•à¥‡ à¤¤à¥‡à¤°à¥€ à¤—à¤¾à¤‚à¤¡ à¤®à¥‡à¤‚ à¤–à¥à¤œà¤²à¥€ à¤•à¥à¤¯à¥‚ à¤¨à¤¹à¤¿ à¤¹à¥‹ à¤°à¤¹à¥€\n",
      "64,ru,\"Ğ³Ğ°Ğ½Ğ´Ğ¾Ğ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ğ» Ğ½Ğ°Ñ€Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² Ğ½ĞµĞ³Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ‚\",\"ĞĞ½ Ğ¿Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» Ğ½Ğ°Ñ€Ğ¾Ğ´,ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² Ğ½ĞµĞ³Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ‚\",<extra_id_0> Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ğ» Ğ½Ğ°Ñ€Ğ¾Ğ´ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ½ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ».\n",
      "65,zh,æˆ‘ä»¬æ˜¯æˆåˆ†å¤æ‚çš„å¤šå…ƒå°ç¾¤ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºäºŒæ¬¡å…ƒå˜‰å¿ƒç³–çº¯è‰¯è·¯äººæŠ—è›†å­™å§,æˆ‘ä»¬æ˜¯æˆåˆ†å¤æ‚çš„å¤šå…ƒå°ç¾¤ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºäºŒæ¬¡å…ƒå˜‰å¿ƒç³–çº¯è‰¯è·¯äººå¯¹é»‘äººæœ‰è´Ÿé¢æƒ…ç»ªå­™å§,\"æˆ‘ä»¬æ˜¯æˆåˆ†å¤æ‚çš„å¤šå…ƒå°ç¾¤,åŒ…æ‹¬ä½†ä¸é™äºäºŒæ¬¡å…ƒå˜‰å¿ƒç³–çº¯è‰¯è·¯äººæŠ—è›†å­™å§\"\n",
      "66,uk,Ğ Ğ²ÑÑĞ´Ğ¸ Ğ¹Ğ¾Ğ±Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ°Ñ‚Ğ¾Ğ²Ğ¿ Ğ· Ğ¹Ğ¾Ğ±Ğ°Ğ½Ğ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸.,Ğ Ğ²ÑÑĞ´Ğ¸ Ğ½Ğ°Ñ‚Ğ¾Ğ²Ğ¿ Ğ· Ñ€ÑƒĞºĞ°Ğ¼Ğ¸,Ğ° Ğ²ÑÑĞ´Ñƒ Ğ¹Ğ¾Ğ±Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ°Ñ‚Ğ¾Ğ²Ğ¿\n",
      "67,zh,ä¸åœ¨ä¸­å›½åœ°ç›˜ç¹æ®–æ··ç§é»˜ï¼Œä¸€åˆ‡éƒ½å¥½,ä¸åœ¨ä¸­å›½åœ°ç›˜ç¹æ®–æ··è¡€é»‘äººåä»£ï¼Œä¸€åˆ‡éƒ½å¥½,\"ä¸åœ¨ä¸­å›½åœ°ç›˜ç¹æ®–æ··ç§é»˜,ä¸€åˆ‡éƒ½å¥½\"\n",
      "68,uk,ÑĞ¿Ğ¸Ğ·Ğ´Ğ¸ Ñ‚Ğ°Ğ¼ Ñ€ÑĞ¼ĞºÑƒ Ğ´Ğ»Ñ Ğ¼ĞµĞ½Ğµ :D,Ğ²ĞºÑ€Ğ°Ğ´Ğ¸ Ñ‚Ğ°Ğ¼ Ñ€ÑĞ¼ĞºÑƒ Ğ´Ğ»Ñ Ğ¼ĞµĞ½Ğµ :D,<extra_id_0> Ğ½Ğµ ÑĞ¿Ğ¸Ğ·Ğ´Ğ¸ Ñ‚Ğ°Ğ¼ Ñ€ÑĞ¼ĞºÑƒ Ğ´Ğ»Ñ Ğ¼ĞµĞ½Ñ :D\n",
      "69,ar,Ø¹Ù…ÙƒÙ… ØªØ±Ø§Ù…Ø¨ ÙŠÙ„ÙŠ Ø¶Ø±Ø¨Ù‡ ÙŠØ§ Ø®ÙˆÙ„Ø§Øª Ùˆ Ù„ÙŠØ³ Ø§Ù†ØªÙ… ğŸ¤£ ğŸ¤£,Ø¹Ù…ÙƒÙ… ØªØ±Ø§Ù…Ø¨ ÙŠÙ„ÙŠ Ø¶Ø±Ø¨Ù‡ Ùˆ Ù„ÙŠØ³ Ø§Ù†ØªÙ… ğŸ¤£ ğŸ¤£,ğŸ¤£ ğŸ¤£\n",
      "70,uk,Ğ¿Ğ¼Ñ - Ñ†Ğµ Ğ½Ñ–Ñ…ÑƒÑ Ğ½Ğµ Ğ²ĞµÑĞµĞ»Ğ¾,ĞŸĞ¼Ñ Ñ†Ğµ Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»Ñ– Ğ½Ğµ Ğ²ĞµÑĞµĞ»Ğ¾,Ğ½Ñ–Ñ…ÑƒÑ Ğ½Ğµ Ğ²ĞµÑĞµĞ»Ğ¾\n",
      "71,zh,å¸…ä¸ªå±ï¼Œä½ è¿™æ˜¯ä»€ä¹ˆå®¡ç¾ï¼Œæˆ–è€…é˜ä¸‹ä¹Ÿæ˜¯é»‘çŒ©çŒ©ï¼Ÿ,å¸…ä»€ä¹ˆï¼Œä½ è¿™æ˜¯ä»€ä¹ˆå®¡ç¾ï¼Œæˆ–è€…ä½ ä¹Ÿæ˜¯é»‘äººï¼Ÿ,\"å¸…ä¸ªå±,ä½ è¿™æ˜¯ä»€ä¹ˆå®¡ç¾,æˆ–è€…é˜ä¸‹ä¹Ÿæ˜¯é»‘çŒ©çŒ©?\"\n",
      "72,ar,Ø¥Ø¬Ù…Ø¹ Ù…Ø§Ù„Ø§Ù‹ Ø¨Ù‚Ø¯Ø± Ù…Ø§ ØªØ³ØªØ·ÙŠØ¹ ØŒ Ù„Ø§ ØªÙƒÙ† ØºØ¨ÙŠ Ù…ÙØ«Ù„ Ø£Ø¨ÙŠÙƒ ØŒ ÙØ¥Ù† ÙƒÙ„ Ø´ÙŠØ¡ Ø³Ù‡Ù„ Ø¨Ø§Ù„Ù…Ø§Ù„ ÙŠØ§ Ø¨ÙÙ†ÙŠ,Ø¥Ø¬Ù…Ø¹ Ù…Ø§Ù„Ø§Ù‹ Ø¨Ù‚Ø¯Ø± Ù…Ø§ ØªØ³ØªØ·ÙŠØ¹ ØŒ Ù„Ø§ ØªÙƒÙ† Ù…ÙØ«Ù„ Ø£Ø¨ÙŠÙƒ ØŒ ÙØ¥Ù† ÙƒÙ„ Ø´ÙŠØ¡ Ø³Ù‡Ù„ Ø¨Ø§Ù„Ù…Ø§Ù„ ÙŠØ§ Ø¨ÙÙ†ÙŠ,<extra_id_0> ÙƒÙ„ Ø´ÙŠØ¡ Ø³Ù‡Ù„ Ø¨Ø§Ù„Ù…Ø§Ù„ ØŒ Ù„Ø§ ØªÙƒÙ† ØºØ¨ÙŠ Ù…ÙØ«Ù„ Ø£Ø¨ÙŠÙƒ ØŒ ÙØ¥Ù† ÙƒÙ„ Ø´ÙŠØ¡ Ø³Ù‡Ù„ Ø¨Ø§Ù„Ù…Ø§Ù„ Ø¨ÙÙ†ÙŠ\n",
      "73,hi,\"à¤•à¤¿à¤¸à¥€ à¤¸à¥‡ à¤§à¤•à¥à¤•à¤¾ à¤®à¥à¤•à¥à¤•à¥€ à¤¤à¥‹ à¤•à¤°à¥‹, à¤¯à¥‡ à¤¸à¥‚à¤–à¤¾ à¤¸à¥‚à¤–à¤¾ à¤®à¥ˆà¤š à¤®à¤œà¤¾ à¤¨à¤¹à¥€à¤‚ à¤† à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦!\",\"à¤•à¤¿à¤¸à¥€ à¤¸à¥‡ à¤§à¤•à¥à¤•à¤¾ à¤®à¥à¤•à¥à¤•à¥€ à¤¤à¥‹ à¤•à¤°à¥‹, à¤¯à¥‡ à¤¸à¥‚à¤–à¤¾ à¤¸à¥‚à¤–à¤¾ à¤®à¥ˆà¤š à¤®à¤œà¤¾ à¤¨à¤¹à¥€à¤‚ à¤† à¤°à¤¹à¤¾ à¤¹à¥ˆ!\",\"<extra_id_0> à¤•à¤¿à¤¸à¥€ à¤¸à¥‡ à¤§à¤•à¥à¤•à¤¾ à¤®à¥à¤•à¥à¤•à¥€ à¤¤à¥‹ à¤•à¤°à¥‹, à¤¯à¥‡ à¤¸à¥‚à¤–à¤¾ à¤¸à¥‚à¤–à¤¾ à¤®à¥ˆà¤š à¤®à¤œà¤¾ à¤¨à¤¹à¥€à¤‚ à¤† à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦!\"\n",
      "74,en,\"i thought world leader just picked random shit to do , thank you for informing us ! .\",\"I thought world leader just picked a random things to do ,thank you for informing us!.\",\"<extra_id_0> thought world leader just picked random shit to do , thank you for informing us !\"\n",
      "75,hi,\"à¤¸à¤šà¤¾à¤ˆ à¤œà¤¼à¤°à¥‚à¤°à¥€ à¤¹à¥ˆ, à¤²à¥‡à¤•à¤¿à¤¨ à¤¤à¥à¤®à¤¸à¥‡ à¤‰à¤®à¥€à¤¦ à¤•à¥à¤¯à¤¾ à¤•à¤°à¥‚à¤ à¤¨à¥€à¤š\",\"à¤¸à¤šà¤¾à¤ˆ à¤œà¤¼à¤°à¥‚à¤°à¥€ à¤¹à¥ˆ, à¤²à¥‡à¤•à¤¿à¤¨ à¤¤à¥à¤®à¤¸à¥‡ à¤‰à¤®à¥€à¤¦ à¤•à¥à¤¯à¤¾ à¤•à¤°à¥‚à¤\",\"',' à¤²à¥‡à¤•à¤¿à¤¨ à¤¤à¥à¤®à¤¸à¥‡ à¤‰à¤®à¥€à¤¦ à¤•à¥à¤¯à¤¾ à¤•à¤°à¥‚à¤ à¤¨à¥€à¤š\"\n",
      "76,ru,ĞºĞ°ĞºĞ¾Ğ¹ Ñ‚Ğ¾ Ğ´ĞµĞ±Ğ¸Ğ» Ğ¿Ñ€Ğ¸ĞºÑ€ĞµĞ¿Ğ¸Ğ» ĞºĞ¾Ñ‚Ğ° Ğº Ğ¿Ğ¾Ğ´ÑÑ‚Ğ¸Ğ»ĞºĞµ. Ñ…Ğ¾Ğ·ÑĞµĞ²Ğ° Ğ´Ğ°ÑƒĞ½Ñ‹,\"ĞšĞ°ĞºĞ¾Ğ¹-Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ¿Ñ€Ğ¸ĞºÑ€ĞµĞ¿Ğ¸Ğ» ĞºĞ¾Ñ‚Ğ° Ğº Ğ¿Ğ¾Ğ´ÑÑ‚Ğ¸Ğ»ĞºĞµ, Ñ…Ğ¾Ğ·ÑĞµĞ²Ğ° ÑƒĞ¶Ğµ ÑĞ¾Ğ²ÑĞµĞ¼ Ñ‚Ğ¾Ğ³Ğ¾\",<extra_id_0> Ğ¿Ñ€Ğ¸ĞºÑ€ĞµĞ¿Ğ¸Ğ» ĞºĞ¾Ñ‚Ğ° Ğº Ğ¿Ğ¾Ğ´ÑÑ‚Ğ¸Ğ»ĞºĞµ. Ñ…Ğ¾Ğ·ÑĞµĞ²Ğ° Ğ´Ğ°ÑƒĞ½Ñ‹\n",
      "77,am,áˆ°á‹ á‰ áˆ‹á‹ áŠ á‹áˆ¬ áŒ€áŠáˆ«áˆ áŠ á‰¥áˆ­áˆƒ á‹ˆáˆá‹°áˆ›áˆ­á‹«áˆ (áŠ³áˆ­á‰°áˆ­) áˆˆá‹˜áˆ‹áˆˆáˆ™ áŠ áˆ¸áˆá‰§áˆ::,áŒ€áŠáˆ«áˆ áŠ á‰¥áˆ­áˆƒ á‹ˆáˆá‹°áˆ›áˆ­á‹«áˆ (áŠ³áˆ­á‰°áˆ­) áˆ…á‹­á‹ˆá‰± áŠ áˆááˆá¡á¡,áˆ°á‹ á‰ áˆ‹á‹ áŠ á‹áˆ¬ áŒ€áŠáˆ«áˆ áŠ á‰¥áˆ­áˆƒ á‹ˆáˆá‹°áˆ›áˆ­á‹«áˆ (áŠ³áˆ­á‰°áˆ­)\n",
      "78,en,most of the information is total garbage .,most of the information is total waste,<extra_id_0> most of the information is total garbage\n",
      "79,uk,\"Ğ—Ğ½Ğ°Ğ¹Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ³ÑƒĞ»ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¾Ñ” Ğ¿Ğ°Ñ†Ğ°Ğ½Ñ–Ğ².ĞĞ´Ğ¸Ğ½ - ĞºÑ€ÑƒÑ‚Ğ¸Ğ¹ Ñ‡ÑƒĞ²Ğ°Ğº, Ğ· Ğ°Ñ„Ñ–Ğ³Ñ”Ğ½Ğ½Ğ¸Ğ¼ Ğ¿Ğ¾Ñ‡ÑƒÑ‚Ñ‚ÑĞ¼ Ğ³ÑƒĞ¼Ğ¾Ñ€Ñƒ, Ñ–Ğ½ÑˆĞ¸Ğ¹ - ĞµĞ³Ğ¾Ñ—ÑÑ‚Ğ¸Ñ‡Ğ½Ğµ Ğ¼ÑƒĞ´Ğ»Ğ¾.\",\"Ğ—Ğ½Ğ°Ğ¹Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ³ÑƒĞ»ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¾Ñ” Ğ¿Ğ°Ñ†Ğ°Ğ½Ñ–Ğ².ĞĞ´Ğ¸Ğ½ - ĞºÑ€ÑƒÑ‚Ğ¸Ğ¹ Ñ‡ÑƒĞ²Ğ°Ğº, Ğ· Ğ°Ñ„Ñ–Ğ³Ñ”Ğ½Ğ½Ğ¸Ğ¼ Ğ¿Ğ¾Ñ‡ÑƒÑ‚Ñ‚ÑĞ¼ Ğ³ÑƒĞ¼Ğ¾Ñ€Ñƒ, Ñ–Ğ½ÑˆĞ¸Ğ¹ - ĞµĞ³Ğ¾Ñ—ÑÑ‚.\",\"<extra_id_0> Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ³ÑƒĞ»ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¾Ñ” Ğ¿Ğ°Ñ†Ğ°Ğ½Ñ–Ğ².ĞĞ´Ğ¸Ğ½ - ĞºÑ€ÑƒÑ‚Ğ¸Ğ¹ Ñ‡ÑƒĞ²Ğ°Ğº Ğ· Ğ°Ñ„Ñ–Ğ³Ñ”Ğ½Ğ½Ğ¸Ğ¼ Ğ¿Ğ¾Ñ‡ÑƒÑ‚Ñ‚ÑĞ¼ Ğ³ÑƒĞ¼Ğ¾Ñ€Ñƒ, Ñ–Ğ½ÑˆĞ¸Ğ¹ - ĞµĞ³Ğ¾Ñ—ÑÑ‚Ğ¸Ñ‡Ğ½Ğµ Ğ¼ÑƒĞ´Ğ»Ğ¾.\"\n",
      "80,de,So dumm wie Gutmenschen halt sind!,So schlau wie Gutmenschen halt sind!,<extra_id_0> wie Gutmenschen halt sind\n",
      "81,zh,èˆ”é»‘çš„äººåœ¨å¢å¤šï¼Œèˆ”ç‹—ä¹Ÿåœ¨å¢å¤šï¼Œå›½å†…æœ‰äº›å¥³çš„å¿«è¢«èˆ”åˆ°å¤©ä¸Šå»äº†,è®¨å¥½é»‘äººçš„äººåœ¨å¢å¤šï¼Œå•æ–¹é¢ä»˜å‡ºçš„äººä¹Ÿåœ¨å¢å¤šï¼Œå›½å†…æœ‰äº›å¥³çš„å¿«è¢«è®¨å¥½åˆ°å¤©ä¸Šå»äº†,\"èˆ”é»‘çš„äººåœ¨å¢å¤š,èˆ”ç‹—ä¹Ÿåœ¨å¢å¤š,å›½å†…æœ‰äº›å¥³çš„å¿«è¢«èˆ”åˆ°å¤©ä¸Šå»äº†\"\n",
      "82,ar,Ø¯Ø³ Ø§Ù„Ø³Ù… ÙÙŠ Ø§Ù„Ø¹Ø³Ù„ Ø¹Ù„ÙŠÙƒÙ… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡,Ø¯Ø³ Ø§Ù„Ø³Ù… ÙÙŠ Ø§Ù„Ø¹Ø³Ù„,Ø§Ù„Ù‚Ø¯Ø±Ø© ÙÙŠ Ø§Ù„Ø¹Ø³Ù„ Ø¹Ù„ÙŠÙƒÙ… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡\n",
      "83,en,Kind of a stupid response given the name of the article.,Kind of a not meaningful response given the name of the article.,<extra_id_0> Kind of a stupid response given the name of the article\n",
      "84,de,Die deutsche LÃ¼genpresse und Nazi Medien sind nur noch zum kotzen ğŸ¤¢ schlimmer als GÃ¶bbels eben alle samt Vasallen Merkels!!,\"Die deutsche Presse und Medien sind nur noch schlecht, eben alle samt UnterstÃ¼tzer Merkels!!\",deutsche LÃ¼genpresse und Nazi Medien sind nur noch zum kotzen ğŸ¤¢ schlimmer als GÃ¶bbels eben alle samt Vasallen Merkels!!\n",
      "85,es,Ahora me leo la secta que seguro que amplian informaciÃ³n,Voy a leer el artÃ­culo para obtener mÃ¡s informaciÃ³n.,<extra_id_0> me leo la secta que seguro que amplian informaciÃ³n\n",
      "86,zh,é€šè®¯å½•ä¹Ÿä¸æ˜¯ä»€ä¹ˆå¥½ä¸œè¥¿,æˆ‘ä¸ªäººè§‰å¾—åŒæ€§æ‹è€…ä¹Ÿä¸æ˜¯ç‰¹åˆ«å¥½,é€šè®¯å½•ä¹Ÿä¸æ˜¯ä»€ä¹ˆå¥½ä¸œè¥¿\n",
      "87,en,republicans and democrats are the right and left cheeks of the same ass .,Republicans and democrats are in the same League,<extra_id_0> republicans and Democrats are the right and left cheeks of the same ass .\n",
      "88,hi,à¤…à¤¬à¥‡ à¤¦à¤° à¤¹à¤°à¤¾à¤®à¥€ à¤•à¥‡à¤œà¤°à¥€ à¤¸à¤°à¤•à¤¾à¤° à¤•à¥‡ à¤¹à¤¾à¤¥ à¤®à¥ˆà¤‚ à¤¨à¤¹à¥€ à¤¹à¥‹à¤¤à¤¾ à¤…à¤°à¥‡à¤¸à¥à¤Ÿ à¤•à¤°à¤¨à¤¾,à¤…à¤¬à¥‡ à¤•à¥‡à¤œà¤°à¥€ à¤¸à¤°à¤•à¤¾à¤° à¤•à¥‡ à¤¹à¤¾à¤¥ à¤®à¥ˆà¤‚ à¤¨à¤¹à¥€ à¤¹à¥‹à¤¤à¤¾ à¤…à¤°à¥‡à¤¸à¥à¤Ÿ à¤•à¤°à¤¨à¤¾,à¤¬à¤¿à¤¨à¤¾ à¤¦à¤° à¤¹à¤°à¤¾à¤®à¥€ à¤¸à¤°à¤•à¤¾à¤° à¤•à¥‡ à¤¹à¤¾à¤¥ à¤®à¥ˆà¤‚ à¤¨à¤¹à¥€ à¤¹à¥‹à¤¤à¤¾ à¤…à¤°à¥‡à¤¸à¥à¤Ÿ à¤•à¤°à¤¨à¤¾\n",
      "89,zh,å¾ˆå¥½!å‡ºäºè™šä¼ªçš„äººæ€§é“å¾·?è¿«äºæ— å¥ˆçš„ç†è®ºæ¼æ´?æ‰æœ‰ä½ ä»¬çš„ä»Šå¤©?å¦‚æœä¸ç®¡ä»€ä¹ˆé“å¾·?ä½ ä»–å¦ˆä»Šå¤©çš„åˆé¥­å°±æ˜¯åœ¨çŒªæ§½é‡Œåƒäº†,å¾ˆå¥½!å‡ºäºè™šä¼ªçš„äººæ€§é“å¾·?è¿«äºæ— å¥ˆçš„ç†è®ºæ¼æ´?æ‰æœ‰ä½ ä»¬çš„ä»Šå¤©?å¦‚æœä¸ç®¡ä»€ä¹ˆé“å¾·?ä½ ä»Šå¤©çš„åˆé¥­å°±æ˜¯åœ¨çŒªæ§½é‡Œåƒäº†,å¾ˆå¥½!å‡ºäºè™šä¼ªçš„äººæ€§é“å¾·?è¿«äºæ— å¥ˆçš„ç†è®ºæ¼æ´?æ‰æœ‰ä½ ä»¬çš„ä»Šå¤©?å¦‚æœä¸ç®¡ä»€ä¹ˆé“å¾·?ä½ ä»–å¦ˆä»Šå¤©çš„åˆé¥­å°±æ˜¯åœ¨çŒªæ§½é‡Œåƒäº†\n",
      "90,ru,Ñ‚Ñ‹ Ñ€Ñ„ Ğ¶Ğ¸Ğ²ĞµÑˆÑŒ Ğ¸ Ğ³Ñ€ÑĞ·Ñ‚Ñ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑˆ.Ñ…ÑƒĞ¹Ğ»Ğ¾,Ñ‚Ñ‹ Ñ€Ñ„ Ğ¶Ğ¸Ğ²ĞµÑˆÑŒ Ğ¸ Ğ³Ñ€ÑĞ·ÑŒÑ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑˆ.,Ñ€Ñ„ Ğ¶Ğ¸Ğ²ĞµÑˆÑŒ Ğ¸ Ğ³Ñ€ÑĞ·Ñ‚Ñ Ñ€Ğ¾ÑÑĞ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑˆ\n",
      "91,es,Quien putas estÃ¡ RT esa mierda ?,Quien estÃ¡ RT esto?,<extra_id_0> Quien putas estÃ¡ RT esa mierda ?\n",
      "92,de,Murksel und ihre Unchristen stehen fÃ¼r Konfrontation gegen Russland und einen mÃ¶glichen Krieg. Und den Ã¼berleben auch Wahlschafe nicht ğŸ˜œ,Merkel und ihre Unchristen stehen fÃ¼r Konfrontation gegen Russland und einen mÃ¶glichen Krieg. Und den Ã¼berleben auch WÃ¤hler nicht ğŸ˜œ,<extra_id_0> ihre Unchristen stehen fÃ¼r Konfrontation gegen Russland und einen mÃ¶glichen Krieg. Und den Ã¼berleben auch Wahlschafe nicht.\n",
      "93,am,áŠ¥á‹áŠá‰µ áŠ«áˆˆáˆ… áŠ¥áŠ® áˆáŠ•áˆ áŠ£áŒá‹™áŠ áŠ¥áˆ­á‹±áŠ áˆ›áˆˆá‰µ áŠ á‹«áˆµáˆáˆáŒáˆ:: áŠ¥á‹‰áŠá‰µ áˆ«áˆ± á‹­áˆ˜áˆ°áŠ­áˆ«áˆ áŠ¥áŠ“ áŠá‹:: áˆˆáŠ¥áŠ•á‹³áŠ•á‰° á‹‰áˆ¸á‰µ á‰€áˆˆá‰¡ áŒáŠ• á‹•á‹‰áŠá‰µáˆ áˆ²áŠáŒáˆ©á‰µ áŠ£á‹­á‹‹áŒ¥áˆˆáŒ¥áˆ,áŠ¥á‹áŠá‰µ áŠ«áˆˆáˆ… áŠ¥áŠ® áˆáŠ•áˆ áˆŒáˆ‹ á‰°áŒ¨áˆ›áˆª áŠ¥áˆ­á‹³á‰³ áˆ˜áŒ á‹¨á‰… áŠ á‹«áˆµáˆáˆáŒáˆ:: áŠ¥á‹‰áŠá‰µ áˆ«áˆ± á‹­áˆ˜áˆ°áŠ­áˆ«áˆ áŠ¥áŠ“ áŠá‹:: á‰ áŠ¥á‹áŠá‰µ á‹¨áˆ›á‰³áˆáŠ• áŠ¨áˆ†áŠ áŒáŠ• áŠ¥á‹áŠá‰µáˆ á‰¢áŠáŒáˆ©áˆ… áˆ‹á‹­á‹‹áŒ¥áˆáˆ… á‹­á‰½áˆ‹áˆá¡á¡,<extra_id_0> áŠ¥á‹áŠá‰µ áŠ«áˆˆáˆ… áŠ¥áŠ® áˆáŠ•áˆ áŠ£áŒá‹™áŠ áŠ¥áˆ­á‹±áŠ áˆ›áˆˆá‰µ áŠ á‹«áˆµáˆáˆáŒáˆ:: áˆˆáŠ¥áŠ•á‹³áŠ•á‰° á‹‰áˆ¸á‰µ á‰€áˆˆá‰¡ áŒáŠ• á‹•á‹‰áŠá‰µáˆ áˆ²áŠáŒáˆ©á‰µ áŠ£á‹­á‹‹áŒ¥áˆˆáˆ\n",
      "94,hi,à¤®à¥‹à¤¦à¥€ à¤¸à¥‚à¤…à¤° à¤•à¤¾ à¤ªà¥€à¤²à¤¾ à¤­à¥€ à¤¤à¥‹ à¤šà¥à¤ª à¤¹à¥ˆ ,à¤®à¥‹à¤¦à¥€ à¤­à¥€ à¤¤à¥‹ à¤šà¥à¤ª à¤¹à¥ˆ,'' à¤®à¥‹à¤¦à¥€ à¤¸à¥‚à¤…à¤° à¤ªà¥€à¤²à¤¾ à¤­à¥€ à¤¤à¥‹ à¤¤à¥‹ à¤¤à¥‹ à¤®à¥‹à¤¦à¥€ à¤¸à¥‚à¤…à¤°\n",
      "95,zh,çœ‹æ¥ä½ å°±æ˜¯ç­”ä¸»è¯´çš„é‚£ç±»äººäº†å§ï¼Œ??ä»¥ååˆ†æ‰‹äº†å†æ‰¾åˆ«éšç’ä½ æ‰¾é»‘äººçš„å†å²å“¦,çœ‹æ¥ä½ å°±æ˜¯ç­”ä¸»è¯´çš„é‚£ç±»äººäº†å§ï¼Œ??ä»¥ååˆ†æ‰‹äº†å†æ‰¾è¦æåŠä½ æ‰¾é»‘äººçš„å†å²å“¦,\"çœ‹æ¥ä½ å°±æ˜¯ç­”ä¸»è¯´çš„é‚£ç±»äººäº†å§,??ä»¥ååˆ†æ‰‹äº†å†æ‰¾åˆ«éšç’ä½ æ‰¾é»‘äººçš„å†å²å“¦\"\n",
      "96,ar,Ø§Ø«Ù†ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠÙ† ÙŠØ­Ø§ÙˆÙ„ÙˆÙ† Ø¥Ø¸Ù‡Ø§Ø± Ù…Ù† Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„Ø£ÙƒØ¨Ø±!,Ø´Ø®ØµØ§Ù† ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ÙŠÙ† ÙŠØ­Ø§ÙˆÙ„Ø§Ù† Ø¥Ø¸Ù‡Ø§Ø± Ù…Ù† Ù‡Ùˆ Ø§Ù„Ø£ÙØ¶Ù„,<extra_id_0> Ø§Ø«Ù†ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠÙ† ÙŠØ­Ø§ÙˆÙ„ÙˆÙ† Ø¥Ø¸Ù‡Ø§Ø± Ù…Ù† Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„Ø£ÙƒØ¨Ø±!\n",
      "97,ar,Ø¬ÙŠØ¯ØŒ Ù„Ø§ ØªØ¯Ø¹ Ø§Ù„Ø¨Ø§Ø¨ ÙŠØ¶Ø±Ø¨Ùƒ ÙÙŠ Ù…Ø¤Ø®Ø±ØªÙƒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ±Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙŠØ§ Ø³ÙŠØ¯,Ø¬ÙŠØ¯ØŒ Ù„Ø§ ØªØ¯Ø¹ Ø§Ù„Ø¨Ø§Ø¨ ÙŠØ¶Ø±Ø¨Ùƒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ±Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙŠØ§ Ø³ÙŠØ¯,<extra_id_0>ØŒ Ù„Ø§ ØªØ¯Ø¹ ÙŠØ¶Ø±Ø¨Ùƒ ÙÙŠ Ù…Ø¤Ø®Ø±ØªÙƒ Ø¨ÙŠÙ†Ù…Ø§ ØªØªØ±Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙŠØ§ Ø³ÙŠØ¯\n",
      "98,uk,ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ñ– ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ñ - Ğ´Ñ–Ğ²Ñ‡Ğ°Ñ‚Ğ° Ğ· Ğ Ğ“Ğœ.,ĞĞ°Ğ¹ĞºÑ€Ğ°Ñ‰Ñ– ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ñ - Ğ´Ñ–Ğ²Ñ‡Ğ°Ñ‚Ğ° Ğ· Ğ Ğ“Ğœ.,<extra_id_0> - Ğ´Ñ–Ğ²Ñ‡Ğ°Ñ‚Ğ° Ğ· Ğ Ğ“Ğœ.\n",
      "99,uk,\"Ğ¦Ñ ÑĞ¿ĞµĞºĞ° Ğ´ÑƒĞ¼Ğ°Ñ”, ÑˆĞ¾ Ğ·Ğ¼ÑƒÑĞ¸Ñ‚ÑŒ Ğ¼ĞµĞ½Ğµ Ğ²Ğ´Ñ–Ñ‚Ğ¸ Ğ¼Ğ°Ğ¹ĞºÑƒ Ğ· ÑˆĞ¾Ñ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ°Ğ»Ğµ Ñ…ÑƒĞ¹ Ñ‚Ğ°Ğ¼ Ğ¿Ñ”Ğ».\",\"Ğ¦Ñ ÑĞ¿ĞµĞºĞ° Ğ´ÑƒĞ¼Ğ°Ñ”, ÑˆĞ¾ Ğ·Ğ¼ÑƒÑĞ¸Ñ‚ÑŒ Ğ¼ĞµĞ½Ğµ Ğ²Ğ´Ñ–Ñ‚Ğ¸ Ğ¼Ğ°Ğ¹ĞºÑƒ Ğ· ÑˆĞ¾Ñ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ°Ğ»Ğµ Ğ½Ñ– Ğ²Ğ¶Ğµ.\",\"<extra_id_0> Ğ·Ğ¼ÑƒÑĞ¸Ñ‚ÑŒ Ğ¼ĞµĞ½Ğµ Ğ²Ğ´Ñ–Ñ‚Ğ¸ Ğ¼Ğ°Ğ¹ĞºÑƒ Ğ· ÑˆĞ¾Ñ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ°Ğ»Ğµ Ñ…ÑƒĞ¹ Ñ‚Ğ°Ğ¼ Ğ¿Ñ”Ğ».\"\n",
      "100,uk,\"ĞšÑ€Ğ°Ñ‰Ğµ Ğ·Ğ´Ğ¾Ñ…Ğ½ÑƒÑ‚Ğ¸, Ğ½Ñ–Ğ¶ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‚Ğ¸, Ñ‰Ğ¾ Ğ²ÑÑ€Ğ°Ğ²ÑÑ.\",\"ĞšÑ€Ğ°Ñ‰Ğµ Ğ¿Ğ¾Ğ¼ĞµÑ€Ñ‚Ğ¸, Ğ½Ñ–Ğ¶ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‚Ğ¸, Ñ‰Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»Ğ¸Ğ²ÑÑ.\",\"<extra_id_0> Ğ·Ğ´Ğ¾Ñ…Ğ½ÑƒÑ‚Ğ¸ Ğ½Ñ–Ğ¶ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‚Ğ¸, Ñ‰Ğ¾ Ğ²ÑÑ€Ğ°Ğ²ÑÑ.\"\n",
      "101,de,Wie dÃ¤mlich sind Sie eigentlich? Sie sollten ihre Allgemeinbildung vervollstÃ¤ndigen und ObjektivitÃ¤t walten lassen. ğŸ¤¢,Wie uninformiert sind Sie eigentlich? Sie sollten ihre Allgemeinbildung vervollstÃ¤ndigen und ObjektivitÃ¤t walten lassen. ğŸ¤¢,<extra_id_0> Sie sollten ihre Allgemeinbildung vervollstÃ¤ndigen und ObjektivitÃ¤t walten lassen. ğŸ¤¢\n",
      "102,ru,ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ·ĞµĞ» Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ¸Ğ» Ğ¿Ğ¾ÑĞ¾Ğ±Ğ¸Ñ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ¼Ğ°Ñ‚ĞµÑ€ÑĞ¼ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡ĞºĞ°Ğ¼,ĞĞ½ Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ¸Ğ» Ğ¿Ğ¾ÑĞ¾Ğ±Ğ¸Ñ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ¼Ğ°Ñ‚ĞµÑ€ÑĞ¼ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡ĞºĞ°Ğ¼,Ğ¾Ğ½ĞµĞ· ÑÑ‚Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¾Ñ‚Ğ¼ĞµĞ½Ğ¸Ğ» Ğ¿Ğ¾ÑĞ¾Ğ±Ğ¸Ñ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ¼Ğ°Ñ‚ĞµÑ€ÑĞ¼ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡ĞºĞ°Ğ¼\n",
      "103,ar,Ø¹Ø§Ø´ Ø­ÙŠØ§Ø© ÙØ§Ø±ØºØ© ÙˆØ¨Ø§Ø¦Ø³Ø© .,Ø¹Ø§Ø´ Ø­ÙŠØ§Ø© ÙØ§Ø±ØºØ© ÙˆÙ…Ø«ÙŠØ±Ø© Ù„Ù„Ø´ÙÙ‚Ø©.,Ø¹Ø§Ø´ Ø­ÙŠØ§Ø© ÙØ§Ø±ØºØ© ÙˆØ¨Ø§Ø¦Ø³Ø©\n",
      "104,am,áŠ¥áŠ•á‹´ áˆ›áˆ…á‹š áŠ¨áˆ™áˆáˆªá‹«á‰µ áŒ‹áˆ­ áˆ áŠáˆ³á‰µ áŠ áˆáŠá‰ áˆ¨á‰¥áˆ½áˆ áŠ á‰µáˆ˜áŒ¥áŠ•áˆ,áˆ›áˆ…á‹š áŠ¨áˆ™áˆáˆªá‹«á‰µ áŒ‹áˆ­ áˆ˜áŠáˆ³á‰µáˆ½ áŠ áˆµáŒˆáˆ­áˆáŠ›áˆ,áˆ°áŠáˆ³á‰µ áŠ áˆáŠá‰ áˆ¨á‰¥áˆ½áˆ áŠ á‰µáˆ˜áŒ¥áŠ•áˆ\n",
      "105,hi,à¤‡à¤¸ à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤•à¥‹ à¤•à¤¿à¤¸à¥€ à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤°à¥‹à¤—à¥€ à¤•à¥‡ à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤•à¥‹ à¤¦à¤¿à¤–à¤¾à¤“,à¤‡à¤¸ à¤•à¥‹ à¤•à¤¿à¤¸à¥€ à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤°à¥‹à¤—à¥€ à¤•à¥‡ à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤•à¥‹ à¤¦à¤¿à¤–à¤¾à¤“,à¤¬à¤¿à¤¨à¤¾ à¤•à¤¿à¤¸à¥€ à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤°à¥‹à¤—à¥€ à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤•à¥‹ à¤¦à¤¿à¤–à¤¾à¤“\n",
      "106,de,\"SchÃ¤uble zur Lage in der TÃ¼rkei: \"\"Das erinnert mich daran, wie es in der DDR war\"\" Was weiÃŸ dieser Vollpfosten von der DDR ğŸ¤¢\",\"SchÃ¤uble zur Lage in der TÃ¼rkei: \"\"Das erinnert mich daran, wie es in der DDR war\"\" Was weiÃŸ dieser Mensch von der DDR ğŸ¤¢\",\"<extra_id_0> SchÃ¤uble zur Lage in der TÃ¼rkei: \"\"Das erinnert mich daran, wie es in der DDR war\"\" Was weiÃŸ dieser Vollpfosten von der DDR\"\n",
      "107,ar,ÙƒÙ„Ù‡Ù… Ø§ÙˆÙ„Ø§Ø¯ Ù‚Ø­Ø¨Ù‡ ØªÙƒØ§Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆØ§Ù„Ø§Ø³Ù„Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ,ÙƒÙ„Ù‡Ù… ØªÙƒØ§Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆØ§Ù„Ø§Ø³Ù„Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ,<extra_id_0>Ù‡Ù… Ø§ÙˆÙ„Ø§Ø¯ Ù‚Ø­Ø¨Ù‡ ØªÙƒØ§Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆØ§Ù„Ø§Ø³Ù„Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ\n",
      "108,hi,à¤•à¥€à¤šà¤¡à¤¼ à¤‰à¤•à¤¾à¤²à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤¨à¥€à¤š à¤•à¤¿à¤¸ à¤®à¥à¤‚à¤¹ à¤¸à¥‡ à¤¶à¥à¤°à¤¦à¥à¤§à¤¾à¤‚à¤œà¤²à¤¿ à¤¦à¥‡ à¤°à¤¹à¤¾ à¤¹à¥ˆ?,à¤•à¥€à¤šà¤¡à¤¼ à¤‰à¤•à¤¾à¤²à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤¯à¤¹ à¤‡à¤‚à¤¸à¤¾à¤¨ à¤•à¥à¤¯à¥‹à¤‚ à¤¶à¥à¤°à¤¦à¥à¤§à¤¾à¤‚à¤œà¤²à¤¿ à¤¦à¥‡ à¤°à¤¹à¤¾ à¤¹à¥ˆ?,<extra_id_0> à¤•à¤¿à¤¸ à¤®à¥à¤‚à¤¹ à¤¸à¥‡ à¤¶à¥à¤°à¤¦à¥à¤§à¤¾à¤‚à¤œà¤²à¤¿ à¤¦à¥‡ à¤°à¤¹à¤¾ à¤¹à¥ˆ?\n",
      "109,uk,\"Ğ„ Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½ĞºĞ° Ñ‰Ğ°ÑÑ‚Ñ Ğ² Ñ…Ğ¾Ñ€Ñ– Ğ½Ğ°Ñ€Ğ¾Ğ´Ñƒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚Ñ– Ğ–Ğ°Ğ´Ğ°Ğ½Ğ° Ñ– Ğ¡Ğ¾Ğ±Ğ°Ğº, ÑĞºĞ¸Ğ¹ ÑĞ¿Ñ–Ğ²Ğ°Ñ”:Ğ§Ğ¾Ñ‚Ğ¸Ñ€Ğ¸ Ñ€Ğ¾ĞºĞ¸ ÑÑƒĞºĞ° Ğ±ĞµĞ· ĞšĞ¾Ğ±Ğ·Ğ¾Ğ½Ğ°!\",\"Ğ„ Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½ĞºĞ° Ñ‰Ğ°ÑÑ‚Ñ Ğ² Ñ…Ğ¾Ñ€Ñ– Ğ½Ğ°Ñ€Ğ¾Ğ´Ñƒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚Ñ– Ğ–Ğ°Ğ´Ğ°Ğ½Ğ° Ñ– Ğ¡Ğ¾Ğ±Ğ°Ğº, ÑĞºĞ¸Ğ¹ ÑĞ¿Ñ–Ğ²Ğ°Ñ”:Ğ§Ğ¾Ñ‚Ğ¸Ñ€Ğ¸ Ñ€Ğ¾ĞºĞ¸ Ğ±ĞµĞ· ĞšĞ¾Ğ±Ğ·Ğ¾Ğ½Ğ°!\",\"Ñ” Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½ĞºĞ° Ñ‰Ğ°ÑÑ‚Ñ Ğ² Ñ…Ğ¾Ñ€Ñ– Ğ½Ğ°Ñ€Ğ¾Ğ´Ñƒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚Ñ– Ğ–Ğ°Ğ´Ğ°Ğ½Ğ° Ñ– Ğ¡Ğ¾Ğ±Ğ°Ğº, ÑĞºĞ¸Ğ¹ ÑĞ¿Ñ–Ğ²Ğ°Ñ”:Ğ§Ğ¾Ñ‚Ğ¸Ñ€Ğ¸ Ñ€Ğ¾ĞºĞ¸ ÑÑƒĞºĞ° Ğ±ĞµĞ· ĞšĞ¾Ğ±Ğ·Ğ¾Ğ½Ğ°!\"\n",
      "110,ru,Ñ‚Ğ²Ğ°Ñ€Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‚ ÑĞµĞ±Ñ. ĞºÑ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´ĞµÑ‚ Ğ·Ğ° Ğ½Ğ¸Ñ… Ğ²Ğ¾ĞµĞ²Ğ°Ñ‚ÑŒ. ÑĞ°Ğ¼ Ğ¸ Ğ¿ÑƒÑ‚Ğ¸Ğ½.,Ğ“Ğ»ÑƒĞ¿Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‚ ÑĞµĞ±Ñ. ĞºÑ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ·Ğ° Ğ½Ğ¸Ñ… Ğ²Ğ¾ĞµĞ²Ğ°Ñ‚ÑŒ. ÑĞ°Ğ¼ Ğ¸ Ğ¿ÑƒÑ‚Ğ¸Ğ½.,<extra_id_0> Ñ‚Ğ²Ğ°Ñ€Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‚ ÑĞµĞ±Ñ. ÑĞ°Ğ¼ Ğ¸ Ğ¿ÑƒÑ‚Ğ¸Ğ½.\n",
      "111,ru,Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¸ÑĞ°Ğ´ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ñ Ğ¿ÑƒÑĞºĞ°Ğ¹ Ğ½Ğµ Ğ¿Ğ¸ÑˆÑƒÑ‚ ÑÑ‚Ğ¸ Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ°ÑĞ²,\"Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ñ, Ğ¿ÑƒÑĞºĞ°Ğ¹ Ğ½Ğµ Ğ¿Ğ¸ÑˆÑƒÑ‚\",Ğ»ÑĞ½ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¸ÑĞ°Ğ´ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ñ Ğ¿ÑƒÑĞºĞ°Ğ¹ Ğ½Ğµ Ğ¿Ğ¸ÑˆÑƒÑ‚ ÑÑ‚Ğ¸ Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ°ÑĞ²\n",
      "112,uk,\"Ğ–Ğ¸Ñ‚Ñ‚Ñ Ñ…ÑƒĞ¹Ğ½Ñ, Ğ¼Ğ¸ Ğ±ĞµĞ·Ğ¼ĞµĞ¶Ğ½Ğ¾ ÑĞ°Ğ¼Ğ¾Ñ‚Ğ½Ñ–, Ñ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ–Ğ´Ñƒ Ğ¿ĞµĞºÑ‚Ğ¸ Ğ¿Ñ–Ñ†Ñƒ.\",\"Ğ–Ğ¸Ñ‚Ñ‚Ñ Ñ„Ñ–Ğ³Ğ½Ñ, Ğ¼Ğ¸ Ğ±ĞµĞ·Ğ¼ĞµĞ¶Ğ½Ğ¾ ÑĞ°Ğ¼Ğ¾Ñ‚Ğ½Ñ–, Ñ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ–Ğ´Ñƒ Ğ¿ĞµĞºÑ‚Ğ¸ Ğ¿Ñ–Ñ†Ñƒ.\",\"<extra_id_0> Ğ¼Ğ¸ Ğ±ĞµĞ·Ğ¼ĞµĞ¶Ğ½Ğ¾ ÑĞ°Ğ¼Ğ¾Ñ‚Ğ½Ñ–, Ñ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ–Ğ´Ñƒ Ğ¿ĞµĞºÑ‚Ğ¸ Ğ¿Ñ–Ñ†Ñƒ.\"\n",
      "113,ar,Ø§Ù„Ù Ù…Ø±Ø© Ù‚Ù„ØªÙ„Ùƒ .. Ù„Ø§ Ù‚Ù„Øª Ù…Ù‚Ø¯Ø±! Ù‚ØµØ¯ÙŠ Ù…Ù‚Ø¯Ø± Ø§Ø­ÙŠØ§ ÙŠØ§ Ø¬Ø§Ù‡Ù„ Ø¨Ù„Ø§Ùƒ ğŸ’™,Ø§Ù„Ù Ù…Ø±Ø© Ù‚Ù„ØªÙ„Ùƒ .. Ù„Ø§ Ù‚Ù„Øª Ù…Ù‚Ø¯Ø±! Ù‚ØµØ¯ÙŠ Ù…Ù‚Ø¯Ø± Ø§Ø­ÙŠØ§ ÙŠØ§ ÙÙ‡ÙŠÙ… Ø¨Ù„Ø§Ùƒ ğŸ’™,Ù…Ø±Ø© Ù‚Ù„Øª Ù…Ù‚Ø¯Ø±! Ù‚ØµØ¯ÙŠ Ù…Ù‚Ø¯Ø± Ø§Ø­ÙŠØ§ ÙŠØ§ Ø¬Ø§Ù‡Ù„ Ø¨Ù„Ø§Ùƒ ğŸ’™\n",
      "114,uk,Ğ‘Ğ»ÑĞ´ÑŒ Ñ‚Ğ° Ğ½Ñƒ Ğ²Ğ¸ Ğ·Ğ½ÑƒÑ‰Ğ°Ñ”Ñ‚ĞµÑÑŒ?,Ñ‚Ğ° Ğ½Ñƒ Ğ²Ğ¸ Ğ·Ğ½ÑƒÑ‰Ğ°Ñ”Ñ‚ĞµÑÑŒ?,<extra_id_0> Ğ²Ğ¸ Ğ·Ğ½ÑƒÑ‰Ğ°Ñ”Ñ‚ĞµÑÑŒ?\n",
      "115,de,So ein Bullshit..... Du denkst es geht nicht dÃ¼mmer?....dann kennst die SPDler nicht...ğŸ˜‚ğŸ˜‚,So ein Quatsch..... Du denkst es geht nicht schlimmer?....dann kennst die SPDler nicht...ğŸ˜‚ğŸ˜‚,<extra_id_0> ein Bullshit..... Du denkst es geht nicht dÃ¼mmer?....dann kennst die SPDler nicht...ğŸ˜‚ğŸ˜‚\n",
      "116,zh,å››å¹´çº§çš„æ—¶å€™æˆ‘å°±é—®è¿‡æˆ‘å¦ˆä¸ºå•¥ç”·å¥³å¹³ç­‰è¿˜ä¼šæœ‰å¥³å£«ä¼˜å…ˆè¿™å¥è¯å‡ºç°[è¡¨æƒ…]å››å¹´çº§çš„å¹¼å¹´æ‹³çš‡å·®ç‚¹è¢«ç­ä¸»ä»»æ‰¼æ€åœ¨æ‘‡ç¯®[è¡¨æƒ…]ä¸è¿‡æˆ‘æŒºä½äº†ç°åœ¨ä¾æ—§ç«™åœ¨æ‰“æ‹³ç¬¬ä¸€çº¿ï¼,å››å¹´çº§çš„æ—¶å€™æˆ‘å°±é—®è¿‡æˆ‘å¦ˆä¸ºå•¥ç”·å¥³å¹³ç­‰è¿˜ä¼šæœ‰å¥³å£«ä¼˜å…ˆè¿™å¥è¯å‡ºç°[è¡¨æƒ…]å››å¹´çº§çš„å¹¼å¹´æ€§åˆ«å¹³è¡¡å€¡å¯¼è€…å·®ç‚¹è¢«ç­ä¸»ä»»æ‰¼æ€åœ¨æ‘‡ç¯®[è¡¨æƒ…]ä¸è¿‡æˆ‘æŒºä½äº†ç°åœ¨ä¾æ—§ç«™åœ¨å‘è¡¨æç«¯æ€§åˆ«å¹³ç­‰è¨€è®ºç¬¬ä¸€çº¿ï¼,æœ€è¿‘æˆ‘é—®è¿‡æˆ‘å¦ˆä¸ºå•¥ç”·å¥³å¹³ç­‰è¿˜ä¼šæœ‰å¥³å£«ä¼˜å…ˆè¿™å¥è¯å‡ºç°[è¡¨æƒ…]å››å¹´çº§çš„å¹¼å¹´æ‹³çš‡å·®ç‚¹è¢«ç­ä¸»ä»»æ‰¼æ€åœ¨æ‘‡ç¯®[è¡¨æƒ…]ä¸è¿‡æˆ‘æŒºä½äº†ç°åœ¨ä¾æ—§ç«™åœ¨æ‰“æ‹³ç¬¬ä¸€çº¿!\n",
      "117,hi,à¤¯à¥‡ à¤¹à¥ˆà¤‚ à¤¸à¥à¤…à¤° à¤•à¥‡ à¤ªà¤¿à¤²à¥à¤²à¥‡ à¤¹à¤°à¤¾à¤®à¥€ à¤•à¤¾à¤²à¤¾à¤ªà¤¾à¤¨à¥€ à¤­à¥‡à¤œà¤¿à¤¯à¥‡ à¤¹à¤°à¤¾à¤® à¤•à¥€ à¤”à¤²à¤¾à¤¦à¥‹ à¤•à¥‹..,à¤¯à¥‡ à¤¹à¥ˆà¤‚ à¤¬à¥‡à¤•à¤¾à¤° à¤²à¥‹à¤— à¤•à¤¡à¤¼à¥€ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤•à¤°à¤¿à¤ à¤‡à¤¨ à¤¬à¥‡à¤•à¤¾à¤° à¤²à¥‹à¤—à¥‹à¤‚ à¤ªà¤° ..,à¤¯à¤¹ à¤¹à¥ˆà¤‚ à¤¸à¥à¤…à¤° à¤•à¥‡ à¤ªà¤¿à¤²à¥à¤²à¥‡ à¤¹à¤°à¤¾à¤®à¥€ à¤•à¤¾à¤²à¤¾à¤ªà¤¾à¤¨à¥€ à¤­à¥‡à¤œà¤¿à¤¯à¥‡ à¤¹à¤°à¤¾à¤® à¤•à¥€ à¤”à¤²à¤¾à¤¦à¥‹ à¤•à¥‹.\n",
      "118,am,á‰ áˆáŠ• áŠ¢áˆ³á‰¥ áŠá‹ á‹­áŠ•áŠ• á‹ˆáˆ«á‹³ á‰£áŠ•á‹³ (áŠ«á‹µáˆ¬) áŠ á‰£á‰µ á‰¥áˆˆáˆ… á‹¨áˆá‰µáŒ áˆ«á‹,á‰ áˆáŠ• áˆ‚áˆ³á‰¥ áŠá‹ á‹­áŠ•áŠ• á‹¨á–áˆˆá‰²áŠ« áˆ¹áˆ áŠ á‰£á‰µ á‰¥áˆˆáˆ… á‹¨áˆá‰µáŒ áˆ«á‹,áˆáŠ• áŠ¢áˆ³á‰¥ áŠá‹ á‹­áŠ•áŠ• á‹ˆáˆ«á‹³ á‰£áŠ•á‹³ (áŠ«á‹µáˆ¬) áŠ á‰£á‰µ á‰¥áˆˆáˆ… á‹¨áˆá‰µáŒ áˆ«á‹\n",
      "119,en,p. s. i do own all of those pictures bitch .,i own all those pictures.,<extra_id_0> i do own all of those pictures bitch .\n",
      "120,zh,æ¯äººè¿›æ¥å•¦,æœ‰ä½è®©æˆ‘ä¸å–œæ¬¢çš„å¥³æ€§è¿›æ¥å•¦,æ¯äººè¿›æ¥å•¦\n",
      "121,de,\"Gibt es Ã¼berhaupt so viele DreckslÃ¶cher in Deutschland, wenn die LINKEN sich bald verdrÃ¼cken mÃ¼ÃŸen ??\",\"Gibt es Ã¼berhaupt so viele RÃ¼ckzugsorte in Deutschland, wenn die LINKEN sich bald verdrÃ¼cken mÃ¼ÃŸen ??\",<extra_id_0> gibt es Ã¼berhaupt so viele DreckslÃ¶cher in Deutschland wenn die LINKEN sich bald verdrÃ¼cken mÃ¼ÃŸen??\n",
      "122,hi,à¤°à¤‚à¤¡à¥€ à¤°à¥‹à¤¨à¤¾ à¤¶à¥à¤°à¥‚ à¤•à¤° à¤¦à¥‡ à¤§à¥‹à¤¨à¥€ à¤†à¤œ à¤§à¤¾à¤—à¤¾ à¤–à¥‹à¤² à¤¦à¥‡à¤—à¤¾ à¥¤,à¤¶à¤¿à¤•à¤¾à¤¯à¤¤ à¤•à¤°à¤¨à¤¾ à¤¶à¥à¤°à¥‚ à¤•à¤° à¤¦à¥‹ à¤§à¥‹à¤¨à¥€ à¤†à¤œ à¤§à¤¾à¤—à¤¾ à¤–à¥‹à¤² à¤¦à¥‡à¤—à¤¾ à¥¤,Rà¤‚à¤¡à¥€ à¤°à¥‹à¤¨à¤¾ à¤¶à¥à¤°à¥‚ à¤•à¤° à¤¦à¥‡ à¤§à¥‹à¤¨à¥€ à¤†à¤œ à¤§à¤¾à¤—à¤¾ à¤–à¥‹à¤² à¤¦à¥‡à¤—à¤¾ à¥¤\n",
      "123,am,áˆˆáŠáŒˆáˆ© á‹°á‹°á‰¥ áˆ‹á‹­áˆ°áˆ› áˆ˜áˆˆááˆˆá á‰µáˆ­á áŠá‹ áŠ¥áŠ•áŒ… áŠ¥áŠáŒáˆ­áˆ… áŠá‰ áˆ­ á‹¨á‹›áá‰¹ á‰¢áŒ¤ áŠá‹ áŠ¥áŠ•á‹³áŠ•á‰° áŠ á‹­áŠá‰± áŒ áˆ›áˆ› áŠ¥áŠ•áŒ¨á‰µ áŠá‹ á‹¨áˆšá‰†áˆ­áŒ£á‰¸á‹,áŠáŒˆáˆ©áŠ• á‹¨áˆá‰µáˆ¨á‹³á‹ á‰¢áˆ†áŠ• áŠ¥áŠáŒáˆ­áˆ… áŠá‰ áˆ¨ á¤ áŠ¥áŠ•á‹°á‹›áá‰¹ á‹¨á‰°áŠ•áŒ‹á‹°á‹±á‰µ áˆŒáˆá‰¹áŠ• áˆŠáŒá‹±á‹‹á‰¸á‹ á‹­á‰½áˆ‹áˆ‰,áŠáŒˆáˆ© á‹°á‹°á‰¥ áˆ‹á‹­áˆ°áˆ› áˆ˜áˆˆááˆˆá á‰µáˆ­á áŠá‹ áŠ¥áŠ•áŒ… áŠ¥áŠáŒáˆ­áˆ… áŠá‰ áˆ­ á‹¨á‹›áá‰¹ á‰¢áŒ¤ áŠá‹ áŠ¥áŠ•á‹³áŠ•á‰° áŠ á‹­áŠá‰± áŒ áˆ›áˆ› áŠ¥áŠ•áŒ¨á‰µ áŠá‹ á‹¨áˆšá‰†áˆ­áŒ£á‰¸á‹\n",
      "124,en,\"and because the government need this land , fuck people who live there , right ? .\",\"and because the government need this land, the people who live there don't matter, right?\",\"because the government need this land , fuck people who live there , right ?\"\n",
      "125,ar,ÙƒÙ„ Ø¯ÙˆÙ„ Ø´Ø±Ø§Ù…ÙŠØ· Ø¹ Ø§Ù„ØªØ§ÙŠÙ… Ø§Ø³ÙŠØ¨Ù‡Ù… Ùˆ Ø§Ø°Ø§ÙƒØ± Ø¨Ù‚ÙŠ,ÙƒÙ„ Ø¯ÙˆÙ„ Ø¹ Ø§Ù„ØªØ§ÙŠÙ… Ø§Ø³ÙŠØ¨Ù‡Ù… Ùˆ Ø§Ø°Ø§ÙƒØ± Ø¨Ù‚ÙŠ,<extra_id_0> ÙƒÙ„ Ø¯ÙˆÙ„ Ø´Ø±Ø§Ù…ÙŠØ· Ø§Ù„ØªØ§ÙŠÙ… Ø§Ø³ÙŠØ¨Ù‡Ù… Ùˆ Ø§Ø°Ø§ÙƒØ± Ø¨Ù‚ÙŠ\n",
      "126,es,ğŸš¨ğŸš¨ Los okupas causan graves disturbios en Santiago y el alcalde 'podemita' les respalda â¡ï¸  SON GENTUZA,Los okupas causan graves disturbios en santiago y el alcalde podemita les respalda,ğŸš¨ğŸš¨ Los okupas causan graves disturbios en Santiago y el alcalde 'podemita' les respalda â¡ï¸ SON GENTUZA\n",
      "127,ru,\"Ğ·Ğ°ĞµĞ±Ğ°Ğ»Ğ¸ Ğ²Ñ‹ ÑƒĞ¶Ğµ, Ğ²Ğ°Ğ¼ Ğ½Ğµ ÑƒĞ³Ğ¾Ğ´Ğ¸Ñˆ, Ñ‚Ñ€Ğ¾Ğ»Ğ»Ğ¸ Ğ±Ğ»Ñ.\",Ğ²Ğ°Ğ¼ Ğ½Ğµ ÑƒĞ³Ğ¾Ğ´Ğ¸ÑˆÑŒ.,\"ÑƒĞ¶Ğµ, Ğ²Ğ°Ğ¼ Ğ½Ğµ ÑƒĞ³Ğ¾Ğ´Ğ¸Ñˆ, Ñ‚Ñ€Ğ¾Ğ»Ğ»Ğ¸ Ğ±Ğ»Ñ.\"\n",
      "128,am,Haha á‰¦á‰…á‰§á‰ƒ áŠ¥áŠ“ áˆ˜áˆ¬á‰µáˆ…áŠ• áŠ¥áˆµáŠªá‹ˆáˆ¨áˆ­ áˆáŠ• áŠ¥á‹¨áŒ á‰ á‰… áŠá‹,áˆ˜áˆ¬á‰± áŠ¥áˆµáŠªá‹ˆáˆ°á‹µ áˆ˜áŒ á‰ á‰… á‹¨áˆˆá‰¥áˆ…áˆ,á‰¦á‰…á‰§á‰ƒ áŠ¥áŠ“ áˆ˜áˆ¬á‰µáˆ…áŠ• áŠ¥áˆµáŠªá‹ˆáˆ¨áˆ­ áˆáŠ• áŠ¥á‹¨áŒ á‰ á‰… áŠá‹\n",
      "129,ar,Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ØµØ© ÙŠØ¬Ø¹Ù„Ù†ÙŠ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„ØºØ¨Ø§Ø¡.,Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù‚ØµØ© ÙŠØ¬Ø¹Ù„Ù†ÙŠ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø³ÙˆØ¡,<extra_id_0>Ø°Ù‡ Ø§Ù„Ù‚ØµØ© ÙŠØ¬Ø¹Ù„Ù†ÙŠ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„ØºØ¨Ø§Ø¡\n",
      "130,ru,Ñ Ğ±Ñ‹ ÑĞºĞ°Ğ·Ğ°Ğ» Ñ‚Ğ°Ğº: Ğ½Ğ°Ğ´Ğ¾ Ğ³Ğ½Ğ°Ñ‚ÑŒ Ğ²ÑĞµÑ… Ğ´ĞµĞ¿ÑƒÑ‚Ğ°Ñ‚Ğ¾Ğ²-Ğ¸Ğ´Ğ¸Ğ¾Ñ‚Ğ¾Ğ²!,Ñ Ğ±Ñ‹ ÑĞºĞ°Ğ·Ğ°Ğ» Ñ‚Ğ°Ğº: Ğ½Ğ°Ğ´Ğ¾ Ğ³Ğ½Ğ°Ñ‚ÑŒ Ğ²ÑĞµÑ… Ğ´ĞµĞ¿ÑƒÑ‚Ğ°Ñ‚Ğ¾Ğ²,<extra_id_0> Ğ±Ñ‹ ÑĞºĞ°Ğ·Ğ°Ğ» Ñ‚Ğ°Ğº: Ğ½Ğ°Ğ´Ğ¾ Ğ³Ğ½Ğ°Ñ‚ÑŒ Ğ²ÑĞµÑ… Ğ´ĞµĞ¿ÑƒÑ‚Ğ°Ñ‚Ğ¾Ğ²-Ğ¸Ğ´Ğ¸Ğ¾Ñ‚Ğ¾Ğ²!\n",
      "131,ar,Ù…Ø¶Ø­Ùƒ Ø¬Ø¯Ø§Ù‹ Ø£Ù…Ø± Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ§ÙØ¯ÙŠÙ† ÙˆÙ…Ù† Ø¶Ù…Ù†Ù‡Ù… Ù…ØµØ§Ø±ÙˆØ© ! ÙŠØ¹Ø§ÙŠØ±ÙˆÙ† Ø¨Ù„Ø§Ø¯Ù†Ø§ Ø¨Ø¹Ù„Ø§Ù‚Ø§Øª ØµØ¯Ø§Ù‚ØªÙ‡Ø§ Ø§Ù„Ù…ØªÙŠÙ†Ø© Ù…Ø¹ Ø£Ù…ÙŠØ±ÙƒØ§ ÙˆÙ‡Ù… ÙŠØ­Ù„Ù…ÙˆÙ† Ø¨Ø§Ù„Ø³ÙØ±,Ù…Ø¶Ø­Ùƒ Ø¬Ø¯Ø§Ù‹ Ø£Ù…Ø± Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ§ÙØ¯ÙŠÙ† ÙˆÙ…Ù† Ø¶Ù…Ù†Ù‡Ù… Ù…ØµØ±ÙŠÙŠÙ†! ÙŠØ¹Ø§ÙŠØ±ÙˆÙ† Ø¨Ù„Ø§Ø¯Ù†Ø§ Ø¨Ø¹Ù„Ø§Ù‚Ø§Øª ØµØ¯Ø§Ù‚ØªÙ‡Ø§ Ø§Ù„Ù…ØªÙŠÙ†Ø© Ù…Ø¹ Ø£Ù…ÙŠØ±ÙƒØ§ ÙˆÙ‡Ù… ÙŠØ­Ù„Ù…ÙˆÙ† Ø¨Ø§Ù„Ø³ÙØ±,<extra_id_0> Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ§ÙØ¯ÙŠÙ† ÙˆÙ…Ù† Ø¶Ù…Ù†Ù‡Ù… Ù…ØµØ§Ø±ÙˆØ© ! ÙŠØ¹Ø§ÙŠØ±ÙˆÙ† Ø¨Ù„Ø§Ø¯Ù†Ø§ Ø¨Ø¹Ù„Ø§Ù‚Ø§Øª ØµØ¯Ø§Ù‚ØªÙ‡Ø§ Ø§Ù„Ù…ØªÙŠÙ†Ø© Ù…Ø¹ Ø£Ù…ÙŠØ±ÙƒØ§ ÙˆÙ‡Ù… ÙŠØ­Ù„Ù…ÙˆÙ† Ø¨Ø§Ù„Ø³ÙØ±\n",
      "132,uk,Ğ²Ğ¸ÑÑ‚ÑƒĞ¿Ğ°Ğ² ĞºĞ¾Ğ»Ğ¸ÑÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ´Ñ–Ñ‚ÑŒĞ¼Ğ¸ - Ñ—Ğ¼ Ğ¿Ğ¾Ñ…ÑƒĞ¹. Ñ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ñ… Ğ»Ğ¾Ñ… Ğ±ĞµĞ· Ğ°Ğ¹Ñ„Ğ¾Ğ½Ğ°.,Ğ²Ğ¸ÑÑ‚ÑƒĞ¿Ğ°Ğ² ĞºĞ¾Ğ»Ğ¸ÑÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ´Ñ–Ñ‚ÑŒĞ¼Ğ¸ - Ñ—Ğ¼ Ğ±Ğ°Ğ¹Ğ´ÑƒĞ¶Ğµ. Ñ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ñ… Ğ½Ñ–Ñ…Ñ‚Ğ¾ Ğ±ĞµĞ· Ğ°Ğ¹Ñ„Ğ¾Ğ½Ğ°.,<extra_id_0> Ğ²Ğ¸ÑÑ‚ÑƒĞ¿Ğ¸Ğ² ĞºĞ¾Ğ»Ğ¸ÑÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ´Ñ–Ñ‚ÑŒĞ¼Ğ¸ - Ñ—Ğ¼ Ğ¿Ğ¾Ñ…ÑƒĞ¹. Ñ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ñ… Ğ»Ğ¾Ñ… Ğ±ĞµĞ· Ğ°Ğ¹Ñ„Ğ¾Ğ½Ğ°.\n",
      "133,ru,\"ĞĞµ Ğ´ÑƒĞ¼Ğ°Ñ, Ğ´Ğ°Ğ¶Ğµ Ñ ĞµĞ³Ğ¾ Ğ¶Ğ¸Ñ€Ğ½Ğ¾Ğ¹ Ñ€ÑÑ…Ğ¾Ğ¹ 16 Ğ¾Ğ¿Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ½ĞµÑ€Ğ¾Ğ² ĞµĞ¼Ñƒ Ğ½Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ, Ğ¿ÑÑ‚ĞµÑ€Ñ‹Ñ… Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼\",\"ĞĞµ Ğ´ÑƒĞ¼Ğ°Ñ, Ñ ĞµĞ³Ğ¾ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğ¼ Ğ²ĞµÑĞ¾Ğ¼ 16 Ğ¾Ğ¿Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½ĞµÑ€Ğ¾Ğ² ĞµĞ¼Ñƒ Ğ½Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚, Ğ¿ÑÑ‚ĞµÑ€Ñ‹Ñ… Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼\",\"<extra_id_0> Ğ½Ğµ Ğ´ÑƒĞ¼Ğ°Ñ, Ğ´Ğ°Ğ¶Ğµ Ñ ĞµĞ³Ğ¾ Ğ¶Ğ¸Ñ€Ğ½Ğ¾Ğ¹ Ñ€ÑÑ…Ğ¾Ğ¹ 16 Ğ¾Ğ¿Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ĞµÑ€Ğ¾Ğ² ĞµĞ¼Ñƒ Ğ½Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿ÑÑ‚ĞµÑ€Ñ‹Ñ… Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼\"\n",
      "134,es,\"este chico es subnormal, confirmed\",\"este chico es un personaje, confirmed.\",\"<extra_id_0> este chico es subnormal, confirmed\"\n",
      "135,zh,ç†æ™ºçš„gayä¸€èˆ¬éƒ½åæ„Ÿç›´ç”·ï¼Œå› ä¸ºä¸‰è§‚å¤§æ¦‚ç‡æ˜¯ä¸åˆçš„??,ç†æ™ºçš„gayå¯èƒ½å¯¹ç›´ç”·æœ‰è´Ÿé¢æƒ…ç»ªï¼Œå› ä¸ºä¸‰è§‚å¤§æ¦‚ç‡æ˜¯ä¸åˆçš„??,\"ç†æ™ºçš„gayä¸€èˆ¬éƒ½åæ„Ÿç›´ç”·,å› ä¸ºä¸‰è§‚å¤§æ¦‚ç‡æ˜¯ä¸åˆçš„??\"\n",
      "136,am,áŠ¥áˆ¨ áŠ¥áˆ…á‰¶á‰¼ á‰ á‹™ áˆ°á‹á‰½.?? á‹­á‰º á‰áŠ•áŒ‹ áŠ¥áŠ•áŠ³áŠ• áŠ¥áŠ•á‹° á‹ˆáŠ•á‹µáˆœ áŠ¥áŠ® áŠá‹ á‹¨áˆ›á‹­áˆ… á‰µá‰ áˆˆáŠ ??,áŠ¥áˆ¨ áŠ¥áˆ…á‰¶á‰¼ á‰ á‹™ áˆ°á‹á‰½.?? á‹­á‰ºáˆ áŠ¥áŠ•á‹° á‹ˆáŠ•á‹µáˆœ áŠ¥áŠ® áŠá‹ á‹¨áˆ›á‹­áˆ… á‰µá‰ áˆˆáŠ ??,á‰áŠ•áŒ‹ áŠ¥áŠ•áŠ³áŠ• áŠ¥áŠ•á‹° á‹ˆáŠ•á‹µáˆœ áŠ¥áŠ® áŠá‹ á‹¨áˆ›á‹­áˆ… á‰µá‰ áˆˆáŠ ?? á‹­á‰º á‰áŠ•áŒ‹ áŠ¥áŠ•áŠ³áŠ• áŠ¥áŠ•á‹° á‹ˆáŠ•á‹µáˆœ áŠ¥áŠ® áŠá‹ á‹¨áˆ›á‹­áˆ… á‰µá‰ áˆˆáŠ ??\n",
      "137,ar,ÙŠØ§Ø®ÙˆÙƒ Ù†Ù…Ø²Ø­ Ø¨Ø³ Ø§Ù†Øª Ù„Ø­Ø¬ÙŠ,ÙŠØ§Ø®ÙˆÙƒ Ù†Ù…Ø²Ø­ Ø¨Ø³ Ø§Ù†Øª Ø¨Ø³ÙŠØ·,<extra_id_0>Ùƒ Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø­ÙŠØ§ØªÙƒ\n",
      "138,es,\"Tiene que ser fake, no me jodas. JamÃ¡s volverÃ­an a sus paÃ­ses.\",\"Tiene que ser fake, no me hagas bromas. JamÃ¡s volverÃ­an a sus paÃ­ses.\",\"<extra_id_0> que ser fake, no me jodas. JamÃ¡s volvÃ­an a sus paÃ­ses.\"\n",
      "139,de,\"WANN ENTSORGT DEUTSCHLAND ENDLICH SEINE VOLKSMÃ–RDER ??A.Merkel,H.Maas,K.GÃ¶ring E.,Paket GrÃ¼ne,D. Nuhr,C. Ehring,M. Schulz als bevorz.Beisp.!\",\"WANN WIRD DEUTSCHLAND ENDLICH SEINE VOLKSMÃ–RDER LOS ??A.Merkel,H.Maas,K.GÃ¶ring E.,Paket GrÃ¼ne,D. Nuhr,C. Ehring,M. Schulz als bevorz.Beisp.!\",\"<extra_id_0> Deutschland ENDLICH SEINE VOLKSMÃ–RDER ??A.Merkel,H.Maas,K.GÃ¶ring E.,Paket GrÃ¼ne,D. Nuhr,M. Schulz als bevorz.Beisp!\"\n",
      "140,am,á‰†áˆ»áˆ» á‰£áŠ•á‹³ áŒŠá‹œ á‹¨áˆ°áŒ áˆ… áˆ˜áˆµáˆáˆ… á‰ áŠ áˆáŠ’áˆµá‰² áŒ­áˆá‰¥áˆ á‰³á‰€áˆ¨áˆ»áˆˆáˆ… áŠ á‹°áˆá¢á‹¨áŒŠá‹œ áŒ‰á‹³á‹­ áŠá‹.,á‰ áŠ áˆáŠ•áˆµá‰² á‰ áŠ©áˆ á‰ á‹šáˆ… áŒŠá‹œ á‹­áˆ…áŠ• áˆ›áˆˆá‰µáˆ… á‰µáŠ­áŠ­áˆ áŠ á‹­á‹°áˆˆáˆ,áˆ°áŒ áˆ… áˆ˜áˆµáˆáˆ… á‰ áŠ áˆáŠ’áˆµá‰² áŒ­áˆá‰¥áˆ á‰³á‰€áˆ¨áˆ»áˆˆáˆ… áŠ á‹°áˆá¢á‹¨áŒŠá‹œ áŒ‰á‹³á‹­ áŠá‹\n",
      "141,de,\"Eine zusÃ¤tzliche Folge der islamischen Masseneinwanderung, gesteuert von Merkel und den GrÃ¼nen DummkÃ¶pfen!\",\"Eine zusÃ¤tzliche Folge der islamischen Masseneinwanderung, gesteuert von Merkel und den GrÃ¼nen!\",\"<extra_id_0> Eine zusÃ¤tzliche Folge der islamischen Masseneinwanderung, gesteuert von Merkel und den GrÃ¼nen DummkÃ¶pfen!\"\n",
      "142,uk,Ğ™Ğ´Ğ¸ Ğ½Ğ° Ñ…ÑƒĞ¹ Ğ¿Ñ–Ğ´ ÑĞ¸Ğ»Ñƒ,Ğ™Ğ´Ğ¸ Ğ³ĞµÑ‚ÑŒ Ğ¿Ñ–Ğ´ ÑĞ¸Ğ»Ñƒ,<extra_id_0> Ğ™ Ğ™Ğ¾Ğ³Ğ¾ Ğ¶ÑƒÑ‚ÑŒ\n",
      "143,ar,Ù„ÙŠÙ‡ Ø§Ù†Øª Ù…Ø§ Ø¹Ù†Ø¯Ùƒ Ø­Ù…Ø§Ù…ØŸ Ø§Ùˆ Ø¨Ø¹Ø¯Ùƒ Ø¨ØªØ¹Ù…Ù„Ù‡Ø§ ØªØ­Øª Ø§Ù„Ø´Ø¬Ø±Ø© ÙˆØ¨ØªÙ…Ø³Ø­ Ø¨Ø§Ù„Ø­Ø¬Ø±Ø©ØŸ! Ù„Ù…Ø§ Ù‚Ù„Øª Ø¹Ù†Ùƒ ØºØ¨ÙŠ Ø¹Ù†Ø¬Ø¯ Ù…Ø§ ØºÙ„â€¦,Ù„ÙŠÙ‡ Ø§Ù†Øª Ù…Ø§ Ø¹Ù†Ø¯Ùƒ Ø­Ù…Ø§Ù…ØŸ Ø§Ùˆ Ø¨Ø¹Ø¯Ùƒ Ø¨ØªØ¹Ù…Ù„Ù‡Ø§ ØªØ­Øª Ø§Ù„Ø´Ø¬Ø±Ø© ÙˆØ¨ØªÙ…Ø³Ø­ Ø¨Ø§Ù„Ø­Ø¬Ø±Ø©ØŸ! Ù„Ù…Ø§ Ù‚Ù„Øª Ø¹Ù†Ùƒ Ø³Ø§Ø°Ø¬ Ø¹Ù†Ø¬Ø¯ Ù…Ø§ ØºÙ„â€¦,Ù„ÙŠÙ‡ Ø§Ù†Øª Ù…Ø§ Ø¹Ù†Ø¯Ùƒ Ø­Ù…Ø§Ù…ØŸ Ø§Ùˆ Ø¨Ø¹Ø¯Ùƒ Ø¨ØªØ¹Ù…Ù„Ù‡Ø§ ØªØ­Øª Ø§Ù„Ø´Ø¬Ø±Ø© ÙˆØ¨ØªÙ…Ø³Ø­ Ø¨Ø§Ù„Ø­Ø¬Ø±Ø©ØŸ! Ù„Ù…Ø§ Ù‚Ù„Øª Ø¹Ù†Ùƒ ØºØ¨ÙŠ Ø¹Ù†Ø¬Ø¯ Ù…Ø§ ØºÙ„...\n",
      "144,es,Tanto que hablan de Pinochet y los gringos y estos hijueputas son iguales o peores.,Tanto que hablan de Pinochet y los gringos y estos son iguales o peores.,<extra_id_0> los gringos y estos hijueputas son iguales o peores\n",
      "145,am,áˆáˆ‰ áŒáŒ­á‰µ áˆáŒ£áˆª áŠ¨á‰°á‹°áˆ˜áˆ°áˆ° á‹˜áŠ•á‹³á£ áŠ«áˆáŠ• á‰ áŠƒáˆ‹ áŒáŒ­á‰µ á‰¥áŠ–áˆ­ á‰°áŒ á‹«á‰‚á‹  á‰¥á‰» áŠá‹??? ??,á‹¨áŒáŒ­á‰± áˆáŠ•áŒ­  áŠ¥áˆ­áˆáŒƒ áŠ¨á‰°á‹ˆáˆ°á‹°á‰ á‰µ á‰ áŠƒáˆ‹ áŒáŒ­á‰µ á‰¥áŠ–áˆ­ á‰°áŒ á‹«á‰‚á‹ áˆ›áŠ• áŠá‹??? ??,áˆáˆ‰ áŒáŒ­á‰µ áˆáŒ£áˆª áŠ¨á‰°á‹°áˆ˜áˆ°áˆ° á‹˜áŠ•á‹³á£ áŠ«áˆáŠ• á‰ áŠƒáˆ‹ áŒáŒ­á‰µ á‰¥áŠ–áˆ­ á‰°áŒ á‹«á‰‚á‹ á‰¥á‰» áŠá‹??? ??\n",
      "146,uk,ĞŸĞ¾Ñ‡ÑƒĞ²Ğ°Ñ ÑĞµĞ±Ğµ Ñ‚ÑƒĞ¿Ğ¸Ğ¼ Ğ´ĞµĞ±Ñ–Ğ»Ğ¾Ğ¼ :(,ĞŸĞ¾Ñ‡ÑƒĞ²Ğ°Ñ ÑĞµĞ±Ğµ Ñ‚ÑƒĞ¿Ğ¸Ğ¼ :(,\"',' ĞŸĞ¾Ñ‡ÑƒĞ²Ğ°Ñ ÑĞµĞ±Ğµ Ñ‚ÑƒĞ¿Ğ¸Ğ¼ Ğ´ĞµĞ±Ğ»Ğ¾Ğ¼\"\n",
      "147,ar,Ø§ÙØ¶Ù„ ØªØ¹Ù„ÙŠÙ‚ Ø´ÙØªÙ‡ Ù„Ù„Ù…Ù‚Ø·Ø¹ ÙƒÙ„Ø§Ù… Ø¯Ø¨Ù‡ ğŸ’”,Ø§ÙØ¶Ù„ ØªØ¹Ù„ÙŠÙ‚ Ø´ÙØªÙ‡ Ù„Ù„Ù…Ù‚Ø·Ø¹ ÙƒÙ„Ø§Ù… ÙˆØ§Ø­Ø¯Ø© Ø³Ù…ÙŠÙ†Ø©ğŸ’”,ØªØ¹Ù„ÙŠÙ‚ Ø´ÙØªÙ‡ Ù„Ù„Ù…Ù‚Ø·Ø¹ ÙƒÙ„Ø§Ù… Ø¯Ø¨Ù‡ ğŸ’”\n",
      "148,hi,à¤¤à¥à¤® à¤œà¥ˆà¤¸à¥‡ à¤²à¥‹à¤— à¤¸à¤¾à¤²à¥‡ à¤—à¥€à¤¤ à¤—à¤§à¥‡ à¤•à¤¾ à¤—à¤¾à¤¤à¥‡ à¤¹à¥ˆ... à¤­à¥‡à¤¨à¤šà¥‹à¤¦ à¤­à¤¡à¤¼à¤µà¥‡,à¤¤à¥à¤® à¤œà¥ˆà¤¸à¥‡ à¤²à¥‹à¤— à¤¬à¤¾à¤¤ à¤¬à¤¿à¤¨à¤¾ à¤®à¤¤à¤²à¤¬ à¤•à¥€ à¤¹à¥€ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¥¤à¥¤,à¤¤à¥à¤® à¤œà¥ˆà¤¸à¥‡ à¤²à¥‹à¤— à¤¸à¤¾à¤²à¥‡ à¤—à¥€à¤¤ à¤—à¤§à¥‡ à¤•à¤¾ à¤—à¤¾à¤¤à¥‡ à¤¹à¥ˆ à¤­à¥‡à¤¨à¤šà¥‹à¤¦ à¤­à¤¡à¤¼à¤µà¥‡\n",
      "149,hi,<number> à¤•à¥‡ à¤¸à¥à¤•à¥‹à¤° à¤•à¤¾ à¤¸à¤¤à¥à¤¯à¤¾à¤¨à¤¾à¤¶ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤¸à¤¾à¤²à¥‡ à¤¨à¥‡,<number> à¤•à¥‡ à¤¸à¥à¤•à¥‹à¤° à¤•à¤¾ à¤¸à¤¤à¥à¤¯à¤¾à¤¨à¤¾à¤¶ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤‰à¤¸à¤¨à¥‡,'' <number> à¤•à¥‡ à¤¸à¥à¤•à¥‹à¤° à¤•à¤¾ à¤¸à¤¤à¥à¤¯à¤¾à¤¨à¤¾à¤¶ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤¸à¤¾à¤²à¥‡ à¤¨à¥‡\n",
      "150,am,á‹­áˆ„áŠ• áˆáˆ‹ áŠáŒˆáˆ­ á‰ áŠ¥áŠ•áŒáˆŠá‹áŠ› áˆµáˆ‹áˆá‰°áŠ“áŒˆáˆ¨á‰½á‹ áŠ¥áŠ“ áˆ°á‰¥áˆ°á‰¥ á‰¥áˆˆáŠ• áŠ¥áŠ•á‹°áˆáŠ•áŒ áˆ‹á‰µ áˆ›á‹ˆá‰„áŠ• áˆµáŠáŒáˆ«á‰½áˆ á‰ á‹°áˆµá‰³ áŠá‹á¢,á‹­áˆ„áŠ• áˆáˆ‹ áŠáŒˆáˆ­ á‰ áŠ¥áŠ•áŒáˆŠá‹áŠ› áˆµáˆ‹áˆá‰°áŠ“áŒˆáˆ¨á‰½á‹ áŠ¥áŠ“ áˆ°á‰¥áˆ°á‰¥ á‰¥áˆˆáŠ• áŠ¥áˆ­áˆ°á‹‹áŠ• áŠ¥áŠ•á‹°áˆ›áŠ•á‹ˆá‹³á‰µ áˆ˜áˆ¨á‹³á‰´áŠ• áˆáŠáŒáˆ«á‰½áˆ áŠ¥á‰½áˆ‹áˆˆáˆ,<extra_id_0>áˆ„áŠ• áˆáˆ‹ áŠáŒˆáˆ­ á‰ áŠ¥áŠ•áŒáˆŠá‹áŠ› áˆµáˆ‹áˆá‰°áŠ“áŒˆáˆ¨á‰½á‹ áŠ¥áŠ“ áˆ°á‰¥áˆ°á‰¥ á‰¥áˆˆáŠ• áŠ¥áŠ•á‹°áˆáŠ•áŒ áˆ‹á‰µ áˆ›á‹ˆá‰„áŠ• áˆµáŠáŒáˆ«á‰½áˆ á‰ á‹°áˆµá‰³ áŠá‹á¢\n",
      "151,hi,à¤¸à¥à¤µà¤° à¤…à¤¬ à¤¤à¥‹ à¤¬à¤¦à¤² à¤œà¤¾à¤“ à¤­à¥‹à¤¸à¤¡à¥€à¤•à¤¾,à¤…à¤¬ à¤¤à¥‹ à¤¬à¤¦à¤² à¤œà¤¾à¤“ à¤…à¤ªà¤¨à¥€ à¤¬à¥à¤°à¥€ à¤†à¤¦à¤¤à¥‹à¤‚ à¤¸à¥‡ ,à¤¨à¤¬ à¤¤à¥‹ à¤¬à¤¦à¤² à¤œà¤¾à¤“ à¤­à¥‹à¤¸à¤¡à¥€à¤•à¤¾\n",
      "152,hi,à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤¹à¤® à¤¨à¥‡à¤ªà¤¾à¤² à¤šà¤²à¥‡ à¤œà¤¾à¤à¤‚ ?,à¤¹à¤® à¤¨à¥‡à¤ªà¤¾à¤² à¤šà¤²à¥‡ à¤œà¤¾à¤à¤‚ ?,à¤¹à¤® à¤¨à¥‡à¤ªà¤¾à¤² à¤šà¤²à¥‡ à¤œà¤¾à¤à¤‚ ?\n",
      "153,ru,\"Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ¸ÑÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ ĞµĞ±Ğ»Ğ°Ğ½, Ñ‚Ğ¾Ğ³Ğ´Ğ° Ğ¸ Ğ»Ğ°Ğ¹ĞºĞ°Ñ‚ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ½Ğµ ÑÑ‚Ñ‹Ğ´Ğ½Ğ¾.\",\"ĞŸĞ¸ÑĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ¸ÑÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾, Ñ‚Ğ¾Ğ³Ğ´Ğ° Ğ¸ Ğ»Ğ°Ğ¹ĞºĞ°Ñ‚ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ½Ğµ ÑÑ‚Ñ‹Ğ´Ğ½Ğ¾.\",\"Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ¸ÑÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ ĞµĞ±Ğ»Ğ°Ğ½, Ñ‚Ğ¾Ğ³Ğ´Ğ° Ğ¸ Ğ»Ğ°Ğ¹ĞºĞ°Ñ‚ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ½Ğµ ÑÑ‚Ñ‹Ğ´Ğ½Ğ¾\"\n",
      "154,ru,\"Ğ´ÑƒÑ€Ñ‹, Ğ½Ñƒ Ğ½Ğ¸ĞºĞ°Ğº Ğ½Ğµ Ğ¿Ğ¾Ğ¹Ğ¼ÑƒÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾ÑˆĞºĞ¸ ÑĞ°Ğ¼Ñ‹Ğµ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ·ĞµĞ¼Ğ»Ğµ\",\"ĞĞ½Ğ¸ Ğ½Ğ¸ĞºĞ°Ğº Ğ½Ğµ Ğ¿Ğ¾Ğ¹Ğ¼ÑƒÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾ÑˆĞºĞ¸ ÑĞ°Ğ¼Ñ‹Ğµ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğµ\",\"<extra_id_0> Ğ½Ğµ Ğ¿Ğ¾Ğ¹Ğ¼ÑƒÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾ÑˆĞºĞ¸ ÑĞ°Ğ¼Ñ‹Ğµ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ·ĞµĞ¼Ğ»Ğµ\"\n",
      "155,de,\"Habe einige Male fÃ¼r ihn dolmetschen mÃ¼ssen - frag` mich besser nicht. Und wenn er Merkel sieht, kriegt er `nen StÃ¤n*** ğŸ˜œ\",\"Habe einige Male fÃ¼r ihn dolmetschen mÃ¼ssen - fragmich besser nicht. Und wenn er Merkel sieht, freut er sich\",\"<extra_id_0> er Merkel sieht, kriegt er `nen StÃ¤n*** ğŸ˜œ\"\n",
      "156,de,\"Du weiÃŸt, dass du #geistesgestÃ¶rt bist, wenn deine grÃ¶ÃŸte Sorge ist von #linke n Clowns \"\"#Rassist\"\" od. \"\"#islamophob\"\" genannt zu werden\",\"Du weiÃŸt, dass du #verwirrt bist, wenn deine grÃ¶ÃŸte Sorge ist von #linken \"\"#Rassist\"\" od. \"\"#islamophob\"\" genannt zu werden\",\"<extra_id_0> du weiÃŸt, dass du #geistesgestÃ¶rt bist, wenn deine grÃ¶ÃŸte Sorge ist von #linke n Clowns \"\"#Islamophob\"\" genannt zu werden\"\n",
      "157,en,\"not just a biologist , a fucking virologist ! .\",\"not just a biologist , also a virologist\",\"<extra_id_0> not just a biologist , a fucking virologist !\"\n",
      "158,zh,è¡Œè¡Œè¡Œï¼Œå¥³çš„éƒ½æ˜¯å…«å©†ï¼Œç”·çš„ä»ä¸å…«å¦ï¼Œç”·çš„çœ‹å¨±ä¹æ–°é—»éƒ½ä¼šç«‹åˆ»å»ä¸–ï¼Œä¸æ­»å°±æ˜¯æ²¡æœ‰ç”·å­æ°”æ¦‚??æ™®ä¿¡ç”·å€¼å¾—éª„å‚²ï¼Œå€¼å¾—èµæ‰¬ï¼Œæ‚¨æ»¡æ„äº†å—??,è¡Œè¡Œè¡Œï¼Œå¥³çš„éƒ½æ˜¯çˆ±å…«å¦çš„å¥³æ€§ï¼Œç”·çš„ä»ä¸å…«å¦ï¼Œç”·çš„çœ‹å¨±ä¹æ–°é—»éƒ½ä¼šç«‹åˆ»ä»˜å‡ºä»£ä»·ï¼Œä¸ç„¶å°±æ˜¯æ²¡æœ‰ç”·å­æ°”æ¦‚??æ²¡æœ‰è‡ªçŸ¥ä¹‹æ˜çš„ç”·æ€§å€¼å¾—éª„å‚²ï¼Œå€¼å¾—èµæ‰¬ï¼Œæ‚¨æ»¡æ„äº†å—??,\"ç•…è¡Œè¡Œ,å¥³çš„ä»ä¸å…«å¦,ç”·çš„çœ‹å¨±ä¹æ–°é—»éƒ½ä¼šç«‹åˆ»å»ä¸–,ä¸æ­»å°±æ˜¯æ²¡æœ‰ç”·å­æ°”æ¦‚??æ™®ä¿¡ç”·å€¼å¾—éª„å‚²,å€¼å¾—èµæ‰¬æ‚¨æ»¡æ„äº†å—??\"\n",
      "159,am,áˆŒá‰£ áŠ¥áˆáŠ•áˆ áŠáŒˆáˆ­ áˆˆáˆ›á‰£á‰£áˆµ áŠ á‹á‰¸á‹ á‹­áŠ¨áá‰³áˆ‰ á‹¨á‹°á‹°á‰¥ áˆµáˆ« áŠá‹ á‹­á‰°áŠ“áŒˆáˆ¨á‹ áŠ¥áˆ± á‰¥áˆ á‹°áŒáˆ áŠ áˆ³á‹áˆª,áŠ áˆáŠ•áˆ áŠáŒˆáˆ­áŠ• áŠ¨áˆ›á‰£á‰£áˆµ áŠ áˆá‰°á‰†áŒ á‰¡áˆ á¤ á‹¨áˆ›á‹­áŒ á‰ á‰… áŠáŒˆáˆ­ áŠá‹ á‹¨á‰°áŠ“áŒˆáˆ¨á‹,áˆŒá‰£ áŠ¥áˆáŠ•áˆ áŠáŒˆáˆ­ áˆˆáˆ›á‰£á‰£áˆµ áŠ á‹á‰¸á‹ á‹­áŠ¨áá‰³áˆ‰ á‹¨á‹°á‹°á‰¥ áˆµáˆ« áŠá‹ á‹­á‰°áŠ“áŒˆáˆ¨á‹ áŠ¥áˆ± á‰¥áˆ á‹°áŒáˆ áŠ áˆ³á‹áˆª\n",
      "160,en,I believe that you don't have a fucking clue what you're talking about. Fatuorum,I believe that you may not have a clue what you're talking about. Fatuorum,<extra_id_0> I believe that you don't have a fucking clue what you're talking about. Fatuorum\n",
      "161,en,\"yea , who the fuck is government to be involved my my your kids education ? .\",\"yes, who is the government to be involved in my education of my children? .\",<extra_id_0> who is government to be involved my my kids education ? .\n",
      "162,ar,Ø§Ù„Ù„Ø¹Ù†Ø©ØŒ Ø±Ø¨Ù…Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ù†Ø³Ù Ø°Ù„Ùƒ Ø­Ù‚Ù‹Ø§ Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹.,Ø±Ø¨Ù…Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ù†Ø³Ù Ø°Ù„Ùƒ Ø­Ù‚Ù‹Ø§ Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹.,<extra_id_0>Ù…Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ù†Ø³Ù Ø°Ù„Ùƒ Ø­Ù‚Ù‹Ø§ Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹\n",
      "163,zh,çˆ†èŠå¤ªå¯æ€•äº†ï¼Œæ­»å˜æ€å•Š??,çˆ†èŠå¤ªå¯æ€•äº†ï¼Œæˆ‘è®¤ä¸ºè¿™çœŸæ˜¯ä¸æ­£å¸¸å•Š??,\"çˆ†èŠå¤ªå¯æ€•äº†,æ­»å˜æ€å•Š??\"\n",
      "164,hi,\"à¤­à¤¡à¥à¤µà¤¾, à¤¤à¤²à¥à¤µà¥‡ à¤šà¤¾à¤¤à¥, à¤•à¥à¤¤à¥à¤¤à¥‡ à¤¸à¥‡ à¤­à¥€ à¤¬à¤¡à¤¾ à¤•à¥à¤¤à¥à¤¤à¤¾ à¤¦à¥‡à¤–à¤¨à¤¾ à¤¹à¥‹ à¤¤à¥‹ à¤¤à¥à¤œà¥‡ à¤¦à¥‡à¤– à¤²à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤¯à¥‡\",à¤…à¤—à¤° à¤‡à¤¸à¥à¤¸à¥‡ à¤­à¥€ à¤–à¤°à¤¾à¤¬ à¤‡à¤‚à¤¸à¤¾à¤¨ à¤¦à¥‡à¤–à¤¨à¤¾ à¤¹à¥ˆ à¤¤à¥‹ à¤¤à¥à¤®à¥à¤¹à¥‡à¤‚ à¤¦à¥‡à¤–à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤,\"à¤¤à¤²à¥à¤µà¥‡ à¤šà¤¾à¤¤à¥, à¤•à¥à¤¤à¥à¤¤à¥‡ à¤¸à¥‡ à¤­à¥€ à¤¬à¤¡à¤¾ à¤•à¥à¤¤à¥à¤¤à¤¾ à¤¦à¥‡à¤–à¤¨à¤¾ à¤¹à¥‹ à¤¤à¥‹ à¤¤à¥à¤œà¥‡ à¤¦à¥‡à¤– à¤²à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤¯à¥‡\"\n",
      "165,am,á‰ áˆµáˆáŒ£áŠ‘ áˆµáˆˆáˆ˜áŒ¡ áŠ¥áŠ•áŒ‚ áˆˆáˆ…á‹á‰¥ áŠ á‹áŠ– áŠ á‹­á‹°áˆˆáˆ????á‰³áˆªáŠ­áˆ á‹¨áˆšá‹«áˆµá‰³á‹áˆ°á‹ á‰ á‹› áŠá‹á¢,áˆˆáˆ…á‹á‰¥ á‰ áˆ˜áˆ°á‰¥ áˆ³á‹­áˆ†áŠ• áˆ€áˆ‹áŠáŠá‰±áŠ• áˆ‹áˆˆáˆ›áŒ£á‰µ áŠ áˆµá‰¦ á‹­áˆ†áŠ“áˆ,áˆ˜áŒ¡ áŠ¥áŠ•áŒ‚ áˆˆáˆ…á‹á‰¥ áŠ á‹áŠ– áŠ á‹­á‹°áˆˆáˆ????á‰³áˆªáŠ­áˆ á‹¨áˆšá‹«áˆµá‰³á‹áˆ°á‹ á‰ á‹› áŠá‹á¢\n",
      "166,es,Los Reyes Magos JAJAJAJAJAJAJAJAJAJAJAJAJA  Puto retrasado #TierradeNadie10,Los Reyes Magos JAJAJAJAJAJAJAJAJAJAJAJAJA  #TierradeNadie10,<extra_id_0> Los Hombres de Los Estados Unidos\n",
      "167,ar,Ù„ÙŠØ´ Ø¨ØªÙ†Ø·Ø±Ø¨ ÙˆØ¨ØªØ³ÙƒØ± Ù„Ù…Ù† Ø¨ØªØ¬ÙŠØ¨ Ø³ÙŠØ±Ø© Ø§Ù„Ø­Ø°Ø§Ø¡ ÙŠØ§ Ø²Ù„Ù…ÙŠ Ù…Ø¹ Ø¥Ù† ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ø¨ÙØ´ Ø®Ù„Ù‚Ùˆ Ø¹ Ø°ÙˆÙ‚Ùˆ Ø­Ø³Ø¨ Ù…ÙÙ‡ÙˆÙ…Ùˆ Ø«Ù‚Ø§ÙØªÙˆ ÙˆØ¹Ù‚Ù„Ùˆâ€¦,Ù„ÙŠØ´ Ø¨ØªÙ†Ø·Ø±Ø¨ ÙˆØ¨ØªØ³ÙƒØ± Ù„Ù…Ù† Ø¨ØªØ¬ÙŠØ¨ Ø³ÙŠØ±Ø© Ù‡Ø°Ø§ Ø§Ù„Ø´Ø®Øµ ÙŠØ§ Ø²Ù„Ù…ÙŠ Ù…Ø¹ Ø¥Ù† ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ø¨ÙØ´ Ø®Ù„Ù‚Ùˆ Ø¹ Ø°ÙˆÙ‚Ùˆ Ø­Ø³Ø¨ Ù…ÙÙ‡ÙˆÙ…Ùˆ Ø«Ù‚Ø§ÙØªÙˆ ÙˆØ¹Ù‚Ù„Ùˆâ€¦,Ù„ÙŠØ´ Ø¨ØªÙ†Ø·Ø±Ø¨ ÙˆØ¨ØªØ³ÙƒØ± Ù„Ù…Ù† Ø¨ØªØ¬ÙŠØ¨ Ø³ÙŠØ±Ø© Ø§Ù„Ø­Ø°Ø§Ø¡ ÙŠØ§ Ø²Ù„Ù…ÙŠ Ù…Ø¹ Ø¥Ù† ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ø¨ÙØ´ Ø®Ù„Ù‚Ùˆ Ø¹ Ø°ÙˆÙ‚Ùˆ Ø­Ø³Ø¨ Ù…ÙÙ‡ÙˆÙ…Ùˆ Ø«Ù‚Ø§ÙØªÙˆ ÙˆØ¹Ù‚Ù„Ùˆ\n",
      "168,es,\"tienen cojones pero huyen para que otros les REGALEN LA VIDA, un poco contradictorio no?\",\"Resultado: Tienen valentÃ­a, pero buscan que otros les brinden la vida. Â¿No es un tanto contradictorio?\",\"<extra_id_0> tienen cojones pero huyen para que otros les REGALEN LA VIDA, un poco contradictorio no?\"\n",
      "169,uk,\"Ñ‚Ñ– ÑĞ°Ğ¼Ñ–, ĞºĞ¾Ñ‚Ñ€Ñ– \"\"Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ñ–Ğ¼Ğ¿Ğ¾Ñ‚Ñ”Ğ½Ñ†Ñ–Ñ Ğ² ÑƒĞºĞ°Ñ–Ğ½ÑĞºĞ¾Ğ¼ Ñ–Ğ½Ñ„Ğ¾Ğ¿Ğ¾Ğ»Ñ”\"\" Ñ– Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ° Ñ‚Ğ²Ñ–Ñ‚Ğ¸ â€“ \"\"Ğ±Ğ¾Ñ‚Ğ¸ ĞŸĞ¾Ñ€Ğ¾ÑˆĞµĞ½ĞºĞ¾ Ğ½Ğ° Ğ·/Ğ¿\"\" Ğ³Ğ³ Ğ±Ğ»ÑĞ´Ñ–.\",\"Ñ‚Ñ– ÑĞ°Ğ¼Ñ–, ĞºĞ¾Ñ‚Ñ€Ñ– \"\"Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ñ–Ğ¼Ğ¿Ğ¾Ñ‚Ñ”Ğ½Ñ†Ñ–Ñ Ğ² ÑƒĞºĞ°Ñ–Ğ½ÑĞºĞ¾Ğ¼ Ñ–Ğ½Ñ„Ğ¾Ğ¿Ğ¾Ğ»Ñ”\"\" Ñ– Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ° Ñ‚Ğ²Ñ–Ñ‚Ğ¸ â€“ \"\"Ğ±Ğ¾Ñ‚Ğ¸ ĞŸĞ¾Ñ€Ğ¾ÑˆĞµĞ½ĞºĞ¾ Ğ½Ğ° Ğ·/Ğ¿\"\" Ğ³Ğ³\",\"Ñ– Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ° Ñ‚Ğ²Ñ–Ñ‚Ğ¸ â€“ \"\"Ğ±Ğ¾Ñ‚Ğ¸ ĞŸĞ¾Ñ€Ğ¾ÑˆĞµĞ½ĞºĞ¾ Ğ½Ğ° Ğ·/Ğ¿\"\" Ğ³Ğ³ Ğ±Ğ»ÑĞ´Ñ–.\"\n",
      "170,uk,ĞÑƒ Ñ– Ğ´Ğµ Ğ±Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾ #Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ?,ĞÑƒ Ñ– Ğ´Ğµ Ğ¶ Ñ‚Ğ¾ Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ,<extra_id_0> ĞÑƒ Ñ– Ğ´Ğµ Ğ±Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾ #Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ?\n",
      "171,zh,ç‹—å¸¦é£å‘æ˜¯ä¸€å›äº‹ï¼Œå¥³æ€§ç¾¤ä½“ä¸­ä¸€éƒ¨åˆ†è·Ÿç‹—èµ°é‚£æ˜¯å¦ä¸€å›äº‹??ä½ è§‰å¾—å‘¢,æœ‰äº›ä»¤äººä¸å–œæ¬¢çš„äººå¸¦é£å‘æ˜¯ä¸€å›äº‹ï¼Œå¥³æ€§ç¾¤ä½“ä¸­ä¸€éƒ¨åˆ†è·Ÿé‚£äº›äººèµ°é‚£æ˜¯å¦ä¸€å›äº‹??ä½ è§‰å¾—å‘¢,\"ç‹—å¸¦é£å‘æ˜¯ä¸€å›äº‹,å¥³æ€§ç¾¤ä½“ä¸­ä¸€éƒ¨åˆ†è·Ÿç‹—èµ°é‚£æ˜¯å¦ä¸€å›äº‹??ä½ è§‰å¾—å‘¢\"\n",
      "172,de,Der Fettsack soll mal besser den Kopf zu machen.,Der Mann soll mal besser den Kopf zu machen.,<extra_id_0> soll mal besser den Kopf zu machen.\n",
      "173,zh,æ‰€ä»¥ä½ å’Œæ¨ç¬ å…ˆåˆ†æ¸…æ¥šè‡ªä¿¡å’Œè‡ªè´Ÿçš„åŒºåˆ«å†æ¥çæ‰¯æ·¡å§ï¼Œä¸€ä¸ªè¤’ä¹‰è¯ç¡¬ç”Ÿç”Ÿè¢«ä½ ä»¬è¿™äº›æ–‡ç›²ç©æˆè´¬ä¹‰è¯,æ‰€ä»¥ä½ å’Œæ¨ç¬ å…ˆåˆ†æ¸…æ¥šè‡ªä¿¡å’Œè‡ªè´Ÿçš„åŒºåˆ«å†æ¥ä¹±è¯´å§ï¼Œä¸€ä¸ªè¤’ä¹‰è¯ç¡¬ç”Ÿç”Ÿè¢«ä½ ä»¬è¿™äº›ç¼ºä¹çŸ¥è¯†çš„äººç©æˆè´¬ä¹‰è¯,æ‰€ä»¥ä½ å’Œæ¨ç¬ å…ˆåˆ†æ¸…æ¥šè‡ªä¿¡å’Œè‡ªè´Ÿçš„åŒºåˆ«å†æ¥çæ‰¯æ·¡å§\n",
      "174,ar,ÙŠØ§ Ù…ØªØ®Ù„Ù Ø§Ù„Ù‡Ø¯Ø§ÙŠÙ‡ Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡ ÙŠÙ…ÙƒÙ† Ø±Ø¨ÙŠ ÙŠÙ‡Ø¯ÙŠÙ‡Ù… Ø¨Ø§Ø®Ø± Ù„Ø­Ø¸Ù‡â€¦,Ø§Ù„Ù‡Ø¯Ø§ÙŠÙ‡ Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡ ÙŠÙ…ÙƒÙ† Ø±Ø¨ÙŠ ÙŠÙ‡Ø¯ÙŠÙ‡Ù… Ø¨Ø§Ø®Ø± Ù„Ø­Ø¸Ù‡â€¦,<extra_id_0> ÙŠØ§ Ù…ØªØ®Ù„Ù Ø§Ù„Ù‡Ø¯Ø§ÙŠÙ‡ Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡ ÙŠÙ…ÙƒÙ† Ø±Ø¨ÙŠ ÙŠØ³Ù‡Ø¯ÙŠÙ‡Ù… Ø¨Ø§Ø®Ø± Ù„Ø­Ø¸Ù‡...\n",
      "175,hi,à¤¯à¥‡ à¤¹à¤°à¤¾à¤® à¤•à¤¾ à¤ªà¤¿à¤²à¥à¤²à¤¾ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤° à¤¹à¤°à¤¾à¤®à¥€ à¤¹à¥ˆ à¤œà¤¿à¤¸ à¤•à¤¾ à¤–à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤‰à¤¸à¥€ à¤®à¥‡à¤‚ à¤¯à¥‡ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤° à¤›à¥‡à¤¦ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤à¥¤,à¤¯à¥‡ à¤§à¥‹à¤–à¥‡à¤¬à¤¾à¤œ à¤œà¤¿à¤¸ à¤•à¤¾ à¤–à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤‰à¤¸à¥€ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤§à¥‹à¤–à¤¾ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤à¥¤,à¤œà¤¿à¤¸ à¤•à¤¾ à¤–à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤‰à¤¸à¥€ à¤®à¥‡à¤‚ à¤¯à¥‡ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤° à¤›à¥‡à¤¦ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤à¥¤\n",
      "176,ar,Ø´ÙØª ÙƒÙŠÙ Ø§Ù†Ùƒ Ù…ØªØ®Ù„Ù ÙˆÙ‚Ø§Ø¹Ø¯ ØªØ¨Ø±Ø± Ù„Ù…ØºØªØµØ¨ØŸ,Ø´ÙØª ÙƒÙŠÙ Ø§Ù†Ùƒ ØºØ§ÙÙ„ ÙˆÙ‚Ø§Ø¹Ø¯ ØªØ¨Ø±Ø± Ù„Ù…ØºØªØµØ¨ØŸ,ÙƒÙŠÙ Ø§Ù†Ùƒ Ù…ØªØ®Ù„Ù ÙˆÙ‚Ø§Ø¹Ø¯ ØªØ¨Ø±Ø± Ù„Ù…ØºØªØµØ¨ØŸ\n",
      "177,de,\"Nein, MAN USS regulieren, weil dieMieten sonst in dieHÃ¶he steigen. Bauen bedeutet nicht, dass die Mieten sinken. Da werden wir massiv verarscht.\",\"Nein, MAN USS regulieren, weil dieMieten sonst in dieHÃ¶he steigen. Bauen bedeutet nicht, dass die Mieten sinken. Da werden wir hinters Licht gefÃ¼hrt.\",\"<extra_id_0>, MAN USS regulieren, weil die Mieten sinken. Da werden wir massiv verarscht.\"\n",
      "178,en,\"Also, I have the fucking rights to advertise my youtube.com videos\",\"Also, I have the right to advertise my youtube.com videos\",\"<extra_id_0>, I have the fucking rights to advertise my youtubecom videos\"\n",
      "179,uk,\"Ğ¢Ğ° Ğ½Ñƒ ÑÑƒĞºĞ° Ğ»Ñ–Ğ·ÑƒÑ‚ÑŒ Ñ– Ğ»Ñ–Ğ·ÑƒÑ‚ÑŒ, Ñ” Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»Ñ– ÑĞºÑ–ÑÑŒ Ğ¼ĞµĞ¶Ñ– Ñ†ÑŒĞ¾Ğ³Ğ¾ ÑˆĞ°Ğ¿Ñ–Ñ‚Ğ¾??? \",\"Ğ¢Ğ° Ğ½Ñƒ Ğ»Ñ–Ğ·ÑƒÑ‚ÑŒ Ñ– Ğ»Ñ–Ğ·ÑƒÑ‚ÑŒ, Ñ” Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»Ñ– ÑĞºÑ–ÑÑŒ Ğ¼ĞµĞ¶Ñ– Ñ†ÑŒĞ¾Ğ³Ğ¾ ÑˆĞ°Ğ¿Ñ–Ñ‚Ğ¾???\",<extra_id_0> Ñ” Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»Ñ– ÑĞºÑ–ÑÑŒ Ğ¼ĞµĞ¶Ñ– Ñ†ÑŒĞ¾Ğ³Ğ¾ ÑˆĞ°Ğ¿Ñ–Ñ‚Ğ¾???\n",
      "180,ru,\"Ğ”Ğ° Ñ‚Ğ°Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾, Ğ¾Ğ´Ğ¸Ğ½ Ğ±Ñ‹ ÑƒĞ»ĞµÑ‚ĞµĞ» Ğ² Ğ½Ğ¾ĞºĞ°ÑƒÑ‚, Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑˆĞºĞ¾Ğ»Ğ¾Ñ‚Ğ° Ğ±Ñ‹ Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ° Ğ¿Ğ¾ Ñ‚Ğ°Ğ¿ĞºĞ°Ğ¼.\",\"Ğ”Ğ° Ñ‚Ğ°Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾, Ğ¾Ğ´Ğ¸Ğ½ Ğ±Ñ‹ ÑƒĞ»ĞµÑ‚ĞµĞ» Ğ² Ğ½Ğ¾ĞºĞ°ÑƒÑ‚, Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ñ‹ ÑƒĞ±ĞµĞ¶Ğ°Ğ»Ğ¸.\",\"<extra_id_0> Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾, Ğ¾Ğ´Ğ¸Ğ½ Ğ±Ñ‹ ÑƒĞ»ĞµÑ‚ĞµĞ» Ğ² Ğ½Ğ¾ĞºĞ°ÑƒÑ‚, Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑˆĞºĞ¾Ğ»Ğ¾Ñ‚Ğ° Ğ±Ñ‹ Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ° Ğ¿Ğ¾ Ñ‚Ğ°Ğ¿ĞºĞ°Ğ¼\"\n",
      "181,en,i live with my mom and im a gay fag that lives in england,i live with my mom and im a gay person that lives in england,<extra_id_0> I live with my mom and im a gay fag that lives in england\n",
      "182,am,á‹°áŒáŠá‰µ á‰ áˆŒáˆ‹ áˆ°á‹ á‹ˆá‹­áˆ á‹¨á‹ˆáˆ áŠ•á‰¥áˆ¨á‰µ áˆ‹á‹­ : áˆˆá‹›á‹áˆ áˆ³á‹«áˆµáˆá‰…á‹± á‹­áˆ¸áŠ­áŠ«áˆ ::,á‹°áŒáŠá‰µ á‰ áˆŒáˆ‹ áˆ°á‹ á‹ˆá‹­áˆ á‹¨á‹ˆáˆ áŠ•á‰¥áˆ¨á‰µ áˆ‹á‹­ : áˆˆá‹šá‹«á‹áˆ áˆ³á‹«áˆµáˆá‰…á‹± á‰µáŠ­áŠ­áˆ áŠ á‹­á‹°áˆˆáˆ ::,á‹ˆá‹­áˆ á‹¨á‹ˆáˆ áŠ•á‰¥áˆ¨á‰µ áˆ‹á‹­ : áˆˆá‹›á‹áˆ áˆ³á‹«áˆµáˆá‰…á‹± á‹­áˆ¸áŠ­áŠ«áˆ ::\n",
      "183,en,that scum is smiling in court !,That bad person is smiling in court,<extra_id_0> is smiling in court !\n",
      "184,uk,Ğ½Ñƒ Ñ– Ñ…ÑƒĞ¹ Ğ· Ğ½Ğ¸Ğ¼.,ĞÑƒ Ñ– Ñ„Ñ–Ğ³ Ğ· Ğ½Ğ¸Ğ¼,<extra_id_0> Ğ½Ñƒ Ñ– Ñ…ÑƒĞ¹ Ğ· Ğ½Ğ¸Ğ¼.\n",
      "185,zh,å»ºè®®ä»™å¥³ç»“å©šå‰ï¼Œç”·æ€§ä¸€å®šè¦æŠŠæ€€èƒåæœˆåˆ†å¨©å’Œåæœˆå­çš„ç—›è‹¦éƒ½ä½“éªŒä¸€éï¼Œè¿™åä¸ªæœˆä¸€å®šè¦ç”±å¥¹ä»¬ç…§é¡¾ç”·æ€§ï¼Œä»¥æ­¤ä½œä¸ºå¥¹ä»¬ç”Ÿè‚²æ—¶è¢«ç”·æ€§å¯¹å¾…çš„æ ‡å‡†ã€‚,å»ºè®®è‡ªæˆ‘æ„Ÿè§‰è¿‡äºè‰¯å¥½çš„å¥³æ€§ç»“å©šå‰ï¼Œç”·æ€§ä¸€å®šè¦æŠŠæ€€èƒåæœˆåˆ†å¨©å’Œåæœˆå­çš„ç—›è‹¦éƒ½ä½“éªŒä¸€éï¼Œè¿™åä¸ªæœˆä¸€å®šè¦ç”±å¥¹ä»¬ç…§é¡¾ç”·æ€§ï¼Œä»¥æ­¤ä½œä¸ºå¥¹ä»¬ç”Ÿè‚²æ—¶è¢«ç”·æ€§å¯¹å¾…çš„æ ‡å‡†ã€‚,\"ç»ªä¸ªæœˆä¸€å®šè¦ç”±å¥¹ä»¬ç…§é¡¾ç”·æ€§,ä»¥æ­¤ä½œä¸ºå¥¹ä»¬ç”Ÿè‚²æ—¶è¢«ç”·æ€§å¯¹å¾…çš„æ ‡å‡†ã€‚\"\n",
      "186,de,\"Merkel schaffte es, sich bis an die Seite der FDJ-FÃ¼hrung der DDR zu schleimen, um diese spÃ¤ter zu verraten. Heute macht sie es auch so ğŸ˜œ\",\"Merkel schaffte es, sich bis an die Seite der FDJ-FÃ¼hrung der DDR zu schmeicheln, um sich spÃ¤ter abzuwenden. Heute macht sie es auch so ğŸ˜œ\",\"<extra_id_0> Merkel schaffte es, sich bis an die Seite der FDJ-FÃ¼hrung der DDR zu schleimen, um diese spÃ¤ter zu verraten. Heute macht sie es auch so ğŸ˜œ\"\n",
      "187,en,i m just glad he doesn t actually believe in this shit .,I am just glad he doesn't actually believe in this nonsense,<extra_id_0> i m just glad he doesn t actually believe in this shit.\n",
      "188,uk,\"Ğ”ÑĞºÑƒÑ Ğ·Ğ° Ğ·Ğ°ÑƒĞ²Ğ°Ğ¶ĞµĞ½Ğ½Ñ)ĞŸĞ¸Ğ·Ğ´Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ Ñ‚Ñ€ĞµĞ±Ğ°, Ñ‰Ğ¾Ğ± Ğ¼ĞµĞ½ÑˆĞµ Ğ¿Ğ¸Ğ·Ğ´Ñ–Ğ»Ğ°!\",\"Ğ”ÑĞºÑƒÑ Ğ·Ğ° Ğ·Ğ°ÑƒĞ²Ğ°Ğ¶ĞµĞ½Ğ½Ñ)Ğ‘Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ Ñ‚Ñ€ĞµĞ±Ğ°, Ñ‰Ğ¾Ğ± Ğ¼ĞµĞ½ÑˆĞµ Ñ‚Ğ°Ñ€Ğ°Ñ‚Ğ¾Ñ€Ñ–Ğ»Ğ°!\",\"Ñ‚Ñ€ĞµĞ±Ğ°, Ñ‰Ğ¾Ğ± Ğ¼ĞµĞ½ÑˆĞµ Ğ¿Ğ¸Ğ·Ğ´Ñ–Ğ»Ğ°!\"\n",
      "189,ru,ĞµĞ±Ğ°Ğ½ÑƒÑ‚Ğ°Ñ Ğ»ÑĞ³ÑƒÑˆĞºĞ° Ğ´Ğ¾Ğ¼Ğ¾Ğ¹ ĞµĞµ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ´Ğ¾ĞµĞ»Ğ¾ Ğ½Ğ° ÑÑ‚Ğ¾ Ğ²ÑĞµ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ,\"Ğ”Ğ¾Ğ¼Ğ¾Ğ¹ ĞµÑ‘ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾, Ğ½Ğ°Ğ´Ğ¾ĞµĞ»Ğ¾ Ğ½Ğ° ÑÑ‚Ğ¾ Ğ²ÑĞµ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ.\",Ğ¾Ğ½ĞµĞ·Ğ°Ñ Ğ»ÑĞ³ÑƒÑˆĞºĞ° Ğ´Ğ¾Ğ¼Ğ¾Ğ¹ ĞµĞµ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ´Ğ¾ĞµĞ»Ğ¾ Ğ½Ğ° ÑÑ‚Ğ¾ Ğ²ÑĞµ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ\n",
      "190,de,\"Die Anzahl der dreckigen Bastarde in diese PolitverblÃ¶dungsshows wÃ¤re bei 100%, wenn nicht hin und wieder jemand von der AFD dazu kÃ¤me.\",\"Die Anzahl der naiven Personen in diese Politshows wÃ¤re bei 100%, wenn nicht hin und wieder jemand von der AFD dazu kÃ¤me.\",\"<extra_id_0> wÃ¤re bei 100%, wenn nicht hin und wieder jemand von der AFD dazukÃ¤me.\"\n",
      "191,es,\"Solo la fe, el dogmatismo y la estupidez te pueden hacer votar a un elemento como este #SanchezVeteATuCasa\",\"Solo la fe, el dogmatismo y falta del sentido comÃºn te pueden hacer votar a un elemento como este #SanchezVeteATuCasa\",<extra_id_0> el dogmatismo y la estupidez te pueden hacer votar a un elemento como este #SanchezVeteATuCasa\n",
      "192,zh,æˆ‘ç‰¹çƒ¦ä¸€å¼ å˜´å°±è¿™äº‹ä¸å…³ä½ äº‹çš„äººï¼Œç®¡å¤©ç®¡åœ°ç®¡çš„ç€åˆ«äººæ€ä¹ˆçœ‹ï¼Ÿç…§ä½ çš„ç†è®ºï¼Œæ‰¹åˆ¤è¿™äº›äººä¹Ÿä¸å…³ä½ äº‹å•Šï¼Ÿä½ åºŸä»€ä¹ˆè¯ï¼Ÿ,æˆ‘ç‰¹çƒ¦ä¸€å¼ å˜´å°±è¿™äº‹ä¸å…³ä½ äº‹çš„äººï¼Œç®¡å¤©ç®¡åœ°ç®¡çš„ç€åˆ«äººæ€ä¹ˆçœ‹ï¼Ÿç…§ä½ çš„ç†è®ºï¼Œæ‰¹åˆ¤è¿™äº›äººä¹Ÿä¸å…³ä½ äº‹å•Šï¼Ÿä½ è¯´ä»€ä¹ˆæ²¡ç”¨çš„è¯ï¼Ÿ,\"æˆ‘ç‰¹çƒ¦ä¸€å¼ å˜´å°±è¿™äº‹ä¸å…³ä½ äº‹çš„äºº,ç®¡å¤©ç®¡åœ°ç®¡çš„ç€åˆ«äººæ€ä¹ˆçœ‹?ç…§ä½ çš„ç†è®º,æ‰¹åˆ¤è¿™äº›äººä¹Ÿä¸å…³ä½ äº‹å•Š?ä½ åºŸä»€ä¹ˆè¯?\"\n",
      "193,ru,\"ÑĞ¸Ğ´Ğ¸ÑˆÑŒ Ğ¸ Ñ‚ÑƒÑÑƒĞµÑˆ, ÑƒÑ€Ğ¾Ğ´!!!!! Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½ĞµÑ‚ ÑĞ»Ğ¾Ğ²\",Ğ¡Ğ¸Ğ´Ğ¸ÑˆÑŒ Ğ¸ Ñ‚ÑƒÑÑƒĞµÑˆÑŒ!!! ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ½ĞµÑ‚ ÑĞ»Ğ¾Ğ²,Ğ¾Ğ½ĞµĞ· ÑĞ¸Ğ´Ğ¸ÑˆÑŒ Ğ¸ Ñ‚ÑƒÑÑƒĞµÑˆ\n",
      "194,ar,ÙˆØ£Ù…ÙˆØ± Ø£Ø®Ø±Ù‰ ØºØ¨ÙŠØ© .,ÙˆØ£Ù…ÙˆØ± Ø£Ø®Ø±Ù‰ Ø³ÙŠØ¦Ø© .,<extra_id_0> Ø§Ù„Ø£Ù…Ø± Ø£Ø®Ø±Ù‰ ØºØ¨ÙŠØ©\n",
      "195,de,\"Es gibt auch Rentner die darauf angewiesen sind ,die den Cent 2-3mal umdrehen mÃ¼ssen bevor er ausgegeben wird ,anders ist es mit den Asylanten,die haben freie Fahrt!\",\"Es gibt auch Rentner die darauf angewiesen sind ,die den Cent 2-3mal umdrehen mÃ¼ssen bevor er ausgegeben wird ,anders ist es mit den Asylbewerber,die haben freie Fahrt!\",\"<extra_id_0> gibt auch Rentner die darauf angewiesen sind,die den Cent 2-3mal umdrehen mÃ¼ssen bevor er ausgegeben wird,anders ist es mit den Asylanten,die haben freie Fahrt!\"\n",
      "196,de,@USER_0_Hoffenheim @USER_0 Mit besseren Kommentatoren Pinkeln gehen aber das Urinal nicht treffen,@USER_0_Hoffenheim @USER_0 Mit besseren Kommentatoren das Ziel verfehlen,'/USER_0_Hoffenheim @USER_0 Mit besseren Kommentatoren Pinkeln gehen aber das Urinal nichttreffen\n",
      "197,zh,æˆ‘ä»¬å—æ–¹æ˜¯é™¤äº†è‡ªå·±å’Œç²¾ç¥æ•…ä¹¡è°éƒ½é»‘ï¼Œè¦ä¸ä½ æŠŠç¥–ç±æŠ¥è¿‡æ¥å°å·è¿˜æ˜¯é¾™é¸£æˆ‘ä¿è¯èƒ½ç»™ä½ å®‰ä¸Šä¸€ä¸ªï¼ˆä¸ä¼šæ˜¯å¼¯å¼¯å§ï¼Œé‚£é™¤å¤–ï¼‰,æˆ‘ä»¬å—æ–¹æŸäº›äººæ˜¯é™¤äº†è‡ªå·±å’Œç²¾ç¥æ•…ä¹¡è°éƒ½é»‘ï¼Œè¦ä¸ä½ æŠŠç¥–ç±æŠ¥è¿‡æ¥æ²³å—äººè¿˜æ˜¯å·æ¸äººæˆ‘ä¿è¯èƒ½ç»™ä½ å®‰ä¸Šä¸€ä¸ªï¼ˆä¸ä¼šæ˜¯å°æ¹¾äººå§ï¼Œé‚£é™¤å¤–ï¼‰,\"æˆ‘ä»¬å—æ–¹æ˜¯é™¤äº†è‡ªå·±å’Œç²¾ç¥æ•…ä¹¡è°éƒ½é»‘,è¦ä¸ä½ æŠŠç¥–ç±æŠ¥è¿‡æ¥å°å·è¿˜æ˜¯é¾™é¸£æˆ‘ä¿è¯èƒ½ç»™ä½ å®‰ä¸Šä¸€ä¸ª(ä¸ä¼šæ˜¯å¼¯å¼¯å§,é‚£é™¤å¤–)\"\n",
      "198,es,\"Y lo bien que les va siendo fachas, que no es gratis\",\"Y lo bien que les va siendo conservadores, que no es gratis\",\"<extra_id_0> lo bien que les va siendo fachas, que no es gratis\"\n",
      "199,en,no self - respecting australian or new zealand would read such hateful shit .,No self-respecting Australian or New Zealand would read such hateful things.,<extra_id_0> no self - respecting australian or new zealand would read such hateful shit .\n",
      "200,hi,à¤¸à¤¾à¤²à¥€ à¤šà¥à¤¤à¥€à¤¯à¤¾ à¤”à¤°à¤¤ à¤¨à¤¾à¤® à¤²à¥‡à¤¨à¥‡ à¤¸à¥‡ à¤¡à¤°à¤¤à¥€ à¤¹à¥ˆ à¤°à¤‚à¤¡à¥€ à¤•à¤¹à¥€ à¤•à¥€ ||,à¤µà¤¹ à¤¡à¤°à¤ªà¥‹à¤• à¤”à¤°à¤¤ à¤¨à¤¾à¤® à¤²à¥‡à¤¨à¥‡ à¤¸à¥‡ à¤¡à¤°à¤¤à¥€ à¤¹à¥ˆ ||,à¤”à¤°à¤¤ à¤¨à¤¾à¤® à¤²à¥‡à¤¨à¥‡ à¤¸à¥‡ à¤¡à¤°à¤¤à¥€ à¤¹à¥ˆ à¤°à¤‚à¤¡à¥€ à¤•à¤¹à¥€ à¤•à¥€ ||\n",
      "201,ar,Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„ÙŠ ÙŠÙ‚ÙˆÙ„ÙˆÙ† Ø§Ù„ÙˆÙ„Ø¯ Ø®Ø±ÙˆÙ ØªØ±Ø§ Ø§Ù„Ø®Ø±ÙˆÙ Ù…Ø§ÙŠÙ„Ø­Ù‚ Ø¥Ù„Ø§ Ø§Ù„ØªØ¨Ù† ÙÙ‡Ù…ØªÙŠ ÙŠØ§Ù‡Ø¨Ù„Ø© Ù…Ù†Ùƒ Ù„ÙŠÙ‡Ø§ğŸ˜ğŸ¤­ Ø­Ù„ÙØª Ù…Ø§Ø­Ø¯ ÙŠØ´ÙƒØ±Ù†ÙŠ ÙŠØ§Ø¹ÙŠØ§Ù„ğŸ˜Œ,Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„ÙŠ ÙŠÙ‚ÙˆÙ„ÙˆÙ† Ø§Ù„ÙˆÙ„Ø¯ Ø®Ø±ÙˆÙ ØªØ±Ø§ Ø§Ù„Ø®Ø±ÙˆÙ Ù…Ø§ÙŠÙ„Ø­Ù‚ Ø¥Ù„Ø§ Ø§Ù„ØªØ¨Ù† ÙÙ‡Ù…ØªÙŠ Ù…Ù†Ùƒ Ù„ÙŠÙ‡Ø§ğŸ˜ğŸ¤­ Ø­Ù„ÙØª Ù…Ø§Ø­Ø¯ ÙŠØ´ÙƒØ±Ù†ÙŠ ÙŠØ§Ø¹ÙŠØ§Ù„ğŸ˜Œ,ÙÙ‡Ù…ØªÙŠ ÙŠØ§Ù‡Ø¨Ù„Ø© Ù…Ù†Ùƒ Ù„ÙŠÙ‡Ø§ğŸ˜ğŸ¤­ Ø­Ù„ÙØª Ù…Ø§Ø­Ø¯ ÙŠØ´ÙƒØ±Ù†ÙŠ ÙŠØ§Ø¹ÙŠØ§Ù„ğŸ˜Œ\n",
      "202,ar,Ø´Ù‡Ø§Ù„Ø±ÙˆØ¬ Ø§Ù„Ù„ÙŠ Ø¯Ø§Ù‡Ù†Ù‡ ÙÙŠÙ‡ Ø¨Ø±Ø§Ø·Ù…Ù‡Ø§ Ø§Ø³ØªØºÙØ±Ø§Ù„Ù„Ù‡ Ø¨Ø³ Ø²ÙŠØ¯ÙŠÙ‡ Ù…Ø§Ø¹Ù„ÙŠÙ‡Ø§ Ø´Ø±Ù‡Ù‡,Ø´Ù‡Ø§Ù„Ø±ÙˆØ¬ Ø§Ù„Ù„ÙŠ Ø¯Ø§Ù‡Ù†Ù‡ ÙÙŠÙ‡ Ø´ÙØ§Ù‡Ù‡Ø§ Ø§Ø³ØªØºÙØ±Ø§Ù„Ù„Ù‡ Ø¨Ø³ Ù…Ù† Ø£ØªØ¨Ø§Ø¹ Ø§Ù„Ø²ÙŠØ¯ÙŠØ© Ù…Ø§Ø¹Ù„ÙŠÙ‡Ø§ Ø´Ø±,Ø¬Ø§Ù„ Ø§Ù„Ù„ÙŠ Ø¯Ø§Ù‡Ù†Ù‡ ÙÙŠÙ‡ Ø¨Ø±Ø§Ø·Ù…Ù‡Ø§ Ø§Ø³ØªØºÙØ±Ø§Ù„Ù„Ù‡ Ø¨Ø³ Ø²ÙŠØ¯ÙŠÙ‡ Ù…Ø§Ø¹Ù„ÙŠÙ‡Ø§ Ø´Ø±Ù‡Ù‡\n",
      "203,am,á‹¨áŠ¢á‰µá‹®áŒµá‹«áŠ• áˆ°áˆ‹áˆá£áŠ á‹µáŠá‰µá£áŠ¥á‹µáŒˆá‰µá£á‰¥áˆá…áŒáŠ“ á‹¨áˆ›á‹­áˆ¹ á‹¨áˆ°á‹­áŒ£áŠ• á‰áˆ«áŒ®á‰½ áˆáˆ‰ áˆáŒ£áˆª á‹­áŒˆáˆµáŒ£á‰¸á‹‰,á‹¨áŠ¢á‰µá‹®áŒµá‹«áŠ• áˆ°áˆ‹áˆá£áŠ á‹µáŠá‰µá£áŠ¥á‹µáŒˆá‰µá£á‰¥áˆá…áŒáŠ“ á‹¨áˆ›á‹­áˆ¹á‰µáŠ• áˆáˆ‰ áˆáŒ£áˆª áˆá‰¥ á‹­áˆµáŒ£á‰¸á‹,<extra_id_0> á‹¨áŠ¢á‰µá‹®áŒµá‹«áŠ• áˆ°áˆ‹áˆá£áŠ á‹µáŠá‰µá£á‰¥áˆá…áŒáŠ“ á‹¨áˆ›á‹­áˆ¹ á‹¨áˆ°á‹­áŒ£áŠ• á‰áˆ«áŒ®á‰½ áˆáˆ‰ áˆáŒ£áˆª á‹­áŒˆáˆµáŒ£á‰¸á‹‰\n",
      "204,en,\"no shit , lockheed or raytheon or boeing or you name it employs half his districts employees .\",\"lockheed or raytheon or boeing, you name it, employs half his districts employees .\",\"<extra_id_0> no shit , lockheed or raytheon or you name it employs half his districts employees .\"\n",
      "205,uk,\"Ğ‘Ğ¾Ğ¶Ğµ Ğ±Ğ»ÑÑ‚ÑŒ, Ğ½Ğ°Ñ€ĞµÑˆÑ‚Ñ– Ñ…Ñ‚Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾ CCR ÑĞºĞ°Ğ·Ğ°Ğ².\",\"Ğ‘Ğ¾Ğ¶Ğµ, Ğ½Ğ°Ñ€ĞµÑˆÑ‚Ñ– Ñ…Ñ‚Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾ Ğ¡Ğ¡R ÑĞºĞ°Ğ·Ğ°Ğ²\",<extra_id_0> Ñ…Ñ‚Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾ CCR ÑĞºĞ°Ğ·Ğ°Ğ².\n",
      "206,am,áŠ¨áŠ•á‹³áŠ•á‰° áŠ á‹­áŠá‰µ á‹°á‹°á‰¥ áŠ¥áŠ“ áŠ¨á‹³áˆš áˆ«áˆ± á‹­áˆ°á‹áˆ¨áŠ• á¢,á‹°áŒ… áˆ˜áŒ¥áŠ“á‰µáˆ…áŠ• áŠ áˆˆá‹ˆá‹°á‹µáŠ©á‰µáˆ,<extra_id_0> áŠ¨áŠ•á‹³áŠ•á‰° áŠ á‹­áŠá‰µ á‹°á‹°á‰¥ áŠ¥áŠ“ áŠ¨á‹³áˆš áˆ«áˆ± á‹­áˆ°á‹áˆ¨áŠ•\n",
      "207,ru,\"ÑƒĞ°Ñ…Ğ°Ñ…Ğ°, ÑÑƒĞº. Ñ‡ĞµĞ», Ñ‚Ñ‹ Ğ³ĞµĞ½Ğ¸Ğ¹.\",\"ÑƒĞ°Ñ…Ğ°Ñ…Ğ°, Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ñ‚Ñ‹ Ğ³ĞµĞ½Ğ¸Ğ¹.\",\"<extra_id_0> Ñ‚Ñ‹ Ğ³ĞµĞ½Ğ¸Ğ¹, Ñ‚Ñ‹ Ğ³ĞµĞ½Ğ¸Ğ¹\"\n",
      "208,zh,å¤–è¡¨å¯ä»¥ã€‚é»‘çœŸæ˜¯å¤ªä¸‘,å¤–è¡¨å¯ä»¥ã€‚é»‘äººçœŸæ˜¯ä¸ç¬¦åˆæˆ‘ä¸ªäººçš„å®¡ç¾,å¤–è¡¨å¯ä»¥ã€‚é»‘çœŸæ˜¯å¤ªä¸‘\n",
      "209,de,Die meisten Deutschen misstrauen Merkel!. Warum schreibt dieses Vasallenpack der deutschen LÃ¼genpresse nicht mal darÃ¼ber!!!!,Die meisten Deutschen misstrauen Merkel!. Warum schreibt die Presse nicht mal darÃ¼ber!!!!,<extra_id_0> schreibt dieses Vasallenpack deutschen misstrauen Merkel nicht mal darÃ¼ber\n",
      "210,de,Werden die Namen Ã¼berfÃ¼hrter Museldiebe auch sofort verÃ¶ffentlicht ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist !,Werden die Namen Ã¼berfÃ¼hrter muslimischer StraftÃ¤ter auch sofort verÃ¶ffentlicht ??? Wann wird unser Land geheilt von RotGrÃ¼nen-Quatsch !,<extra_id_0>den die Namen Ã¼berfÃ¼hrter Museldiebe sofort verÃ¶ffentlicht ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist ??? Wann wird unser Land geheilt von RotGrÃ¼nenMuselmist\n",
      "211,en,i wish people would shut the fuck up about big money in politics .,I wish people would keep quiet about big money in politics.,<extra_id_0> i wish people would shut the fuck up about big money in politics .\n",
      "212,ar,ÙˆØ¥Ù„Ù‰ Ø£Ù† ØªØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³ÙˆÙŠØ© Ù…Ø¹ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙˆÙƒÙŠÙÙŠØ© Ù…Ø¹Ø§Ù…Ù„ØªÙ‡ Ù„Ù„Ù…Ø±Ø£Ø©ØŒ ÙØ¥Ù† Ø§Ù„Ø­Ø±ÙƒØ© Ø¨Ø±Ù…ØªÙ‡Ø§ Ø³ØªÙƒÙˆÙ† Ù…Ù‡Ø²Ù„Ø© ÙƒØ§Ù…Ù„Ø©!,Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³Ø§Ø¦ÙŠØ© Ù„ÙŠØ³Øª Ø­Ø±ÙƒØ© Ø¬ÙŠØ¯Ø©,<extra_id_0> Ø¥Ù† ØªØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³ÙˆÙŠØ© Ù…Ø¹ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙˆÙƒÙŠÙÙŠØ© Ù…Ø¹Ø§Ù…Ù„ØªÙ‡ Ù„Ù„Ù…Ø±Ø£Ø©ØŒ ÙØ¥Ù† Ø§Ù„Ø­Ø±ÙƒØ© Ø¨Ø±Ù…ØªÙ‡Ø§ Ø³ØªÙƒÙˆÙ† Ù…Ù‡Ø²Ù„Ø© ÙƒØ§Ù…Ù„Ø©!\n",
      "213,es,Puto perroflautas que man rayao el mercedes,que man rayao el mercedes,perroflautas que man rayao el Mercedes\n",
      "214,de,Haben wir bei der Schwuchtel auch schon vermutet....,Haben wir bei dem Typ auch schon vermutet....,<extra_id_0> Haben wir bei der Schwuchtel auch schon vermutet....\n",
      "215,hi,à¤¹à¤® à¤‡à¤‚à¤¸à¤¾à¤¨à¥‹ à¤•à¥‡ à¤ªà¥‚à¤°à¥à¤µà¤œ à¤¤à¥‹ à¤¬à¤‚à¤¦à¤° à¤¥à¥‡ à¤¬à¤¾à¤•à¥€ à¤‡à¤¸ à¤•à¥à¤¤à¥à¤¤à¥‡ à¤•à¥‹ à¤‡à¤¸à¤•à¥‡ à¤ªà¤¤à¤¾ à¤¹à¥‹à¤‚à¤—à¥‡,à¤¹à¤® à¤‡à¤‚à¤¸à¤¾à¤¨à¥‹ à¤•à¥‡ à¤ªà¥‚à¤°à¥à¤µà¤œ à¤¤à¥‹ à¤¬à¤‚à¤¦à¤° à¤¥à¥‡ à¤¬à¤¾à¤•à¥€ à¤‡à¤¸à¥à¤•à¥‡ à¤‡à¤¸à¤•à¥‹ à¤ªà¤¤à¤¾ à¤¹à¥‹à¤‚à¤—à¥‡,à¤¹à¤® à¤‡à¤‚à¤¸à¤¾à¤¨à¥‹ à¤•à¥‡ à¤ªà¥‚à¤°à¥à¤µà¤œ à¤¤à¥‹ à¤¬à¤‚à¤¦à¤° à¤¥à¥‡ à¤¬à¤¾à¤•à¥€ à¤‡à¤¸ à¤•à¥à¤¤à¥à¤¤à¥‡ à¤•à¥‹ à¤‡à¤¸à¤•à¥‡ à¤ªà¤¤à¤¾ à¤¹à¥‹à¤‚à¤—à¥‡\n",
      "216,en,we don t need to consider shit !,We don't need to consider anything,<extra_id_0> we don t need to consider shit!\n",
      "217,en,Do you have to be such a dick to him?,Do you have to be such rude to him?,<extra_id_0> Do you have to be such a dick to him?\n",
      "218,am,á‹¨áŒ…áˆ áŠ­áˆ­áŠ­áˆ© á‹áˆµáŒ¥ áŠ® áˆŒáˆ‹ áˆµáˆ á•áˆ®á–áˆµ á‰ áˆ›á‹µáˆ¨áŒ áˆ«áˆµáˆ…áŠ• áˆ˜á‹¶áˆ á‰¥á‰» áˆ³á‹­áˆ†áŠ• á‹™áˆ©áŠ•áˆ áŠ áŠ¨áˆ¨áˆ­áŠ¨á‹ - áˆ°áˆ‹áˆ áˆšáŠ•áˆµá‰µáˆ­ áˆµá‰³á‹­áˆ conflict resolution,áŠ­áˆ­áŠ­áˆ© á‹áˆµáŒ¥ áŠ® áˆŒáˆ‹ áˆµáˆ á•áˆ®á–áˆµ á‰ áˆ›á‹µáˆ¨áŒ áˆ«áˆµáˆ…áŠ• áˆ›áˆµáŒˆá‰£á‰µ á‰¥á‰» áˆ³á‹­áˆ†áŠ• á‹™áˆ©áŠ•áˆ áŠ áŠ¨áˆ¨áˆ­áŠ¨á‹ - áˆ°áˆ‹áˆ áˆšáŠ•áˆµá‰µáˆ­ áˆµá‰³á‹­áˆ,<extra_id_0> á‹¨áŒ…áˆ áŠ­áˆ­áŠ­áˆ© á‹áˆµáŒ¥ áŠ® áˆŒáˆ‹ áˆµáˆ á‰ áˆ›á‹µáˆ¨áŒ áˆ«áˆµáˆ…áŠ• áˆ˜á‹¶áˆ á‰¥á‰» áˆ³á‹­áˆ†áŠ• á‹™áˆ©áŠ•áˆ áŠ áŠ¨áˆ¨áˆ­áŠ¨á‹ - áˆ°áˆ‹áˆ áˆšáŠ•áˆµá‰µáˆ­ áˆµá‰³á‹­áˆ conflict resolution\n",
      "219,uk,\"  Ğ±Ğ»ÑĞ´ÑŒ, ÑĞ°Ğ¼ ĞºĞ°Ğ·Ğ°Ğ², ÑˆĞ¾ Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ğ² Ğ¼Ğ¾Ñ” Ñ„Ğ¾Ñ‚Ğ¾ Ñ– ÑĞ¾Ğ±Ñ– Ğ·Ğ°Ñ…Ğ¾Ñ‚Ñ–Ğ² Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚Ğ¸.\",\"Ğ±Ğ»Ñ–Ğ½, ÑĞ°Ğ¼ ĞºĞ°Ğ·Ğ°Ğ², ÑˆĞ¾ Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ğ² Ğ¼Ğ¾Ñ” Ñ„Ğ¾Ñ‚Ğ¾ Ñ– ÑĞ¾Ğ±Ñ– Ğ·Ğ°Ñ…Ğ¾Ñ‚Ñ–Ğ² Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚Ğ¸.\",\"<extra_id_0> ĞºĞ°Ğ·Ğ°Ğ², ÑˆĞ¾ Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ğ² Ğ¼Ğ¾Ñ” Ñ„Ğ¾Ñ‚Ğ¾ Ñ– ÑĞ¾Ğ±Ñ– Ğ·Ğ°Ñ…Ğ¾Ñ‚Ñ–Ğ² Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚Ğ¸\"\n",
      "220,uk,\"Ğ’Ğ¸ Ñ…Ğ¾Ñ‡ Ñ€Ğ°Ğ· ÑĞ»ÑƒÑ…Ğ°Ğ»Ğ¸ Ñ…ÑƒĞ¹Ğ½Ñ, ÑĞºÑƒ Ğ²Ñ–Ğ½ Ğ½ĞµÑĞµ?\",Ğ’Ğ¸ Ñ…Ğ¾Ñ‡ Ñ€Ğ°Ğ· ÑĞ»ÑƒÑ…Ğ°Ğ»Ğ¸ Ñ‰Ğ¾ Ğ²Ñ–Ğ½ ĞºĞ°Ğ¶Ğµ?,\"<extra_id_0> Ğ’Ğ¸ Ñ…Ğ¾Ñ‡ Ñ€Ğ°Ğ· ÑĞ»ÑƒÑ…Ğ°Ğ»Ğ¸ Ñ…ÑƒĞ¹Ğ½Ñ, ÑĞºÑƒ Ğ²Ñ–Ğ½ Ğ½ĞµÑĞµ?\"\n",
      "221,zh,å®ƒä¸æ˜¯ä¸ªéª—å©šgayä½¬å—ä¸æ˜¯ç›´ç”·,å®ƒä¸æ˜¯ä¸ªéª—å©šç”·åŒæ€§æ‹å—ä¸æ˜¯ç›´ç”·,å®ƒä¸æ˜¯ä¸ªéª—å©šgayä½¬å—ä¸æ˜¯ç›´ç”·\n",
      "222,am,á‰£áˆˆáŒŒ áŠáˆ½ á‹ˆá‹­áˆ áŠáˆ… áˆ˜áˆ°á‹³á‹°á‰¥ áŒ‰á‰¥á‹áŠ“ áŠ á‹­á‹°áˆˆáˆ  á‹¨á‰‹áŠ•á‰‹ á‰½áŒáˆ­ áŠ¨áˆŒáˆˆá‰¥áˆ½ á‹ˆá‹­ áŠ¨áˆŒáˆˆá‰¥áˆ… á‰µáŠ­áŠ­áˆ áŠá‹ á‹«áˆˆá‹á¢á‹˜áˆ­áŠ•áˆ áˆ˜áˆ­áŒ¦ áˆ›á‹˜áŠ• á‹˜áˆ¨áŠáŠ,áˆµáˆ­áŠ á‰µ áŠ«áˆˆáˆ…/áŠ«áˆˆáˆ½ á€á‹«á á‰ƒáˆáŠ• áˆ˜áˆˆá‹‹á‹ˆáŒ¥ áŒ‰á‰¥á‹áŠ“ áŠ á‹­á‹°áˆˆáˆ á‹¨á‰‹áŠ•á‰‹ á‰½áŒáˆ­ áŠ¨áˆŒáˆˆá‰¥áˆ½ á‹ˆá‹­ áŠ¨áˆŒáˆˆá‰¥áˆ… á‰µáŠ­áŠ­áˆ áŠá‹ á‹«áˆˆá‹á¢,á‹ˆá‹­áˆ áŠáˆ… áˆ˜áˆ°á‹³á‹°á‰¥ áŒ‰á‰¥á‹áŠ“ áŠ á‹­á‹°áˆˆáˆ á‹¨á‰‹áŠ•á‰‹ á‰½áŒáˆ­ áŠ¨áˆŒáˆˆá‰¥áˆ½ á‹ˆá‹­ áŠ¨áˆŒáˆˆá‰¥áˆ… á‰µáŠ­áŠ­áˆ áŠá‹ á‹«áˆˆá‹á¢á‹˜áˆ­áŠ•áˆ áˆ˜áˆ­áŒ¦ áˆ›á‹˜áŠ• á‹˜áˆ¨áŠáŠ\n",
      "223,en,The problem was some idiot made ridiculous edits to the infobox.,The problem was some person made careless edits to the infobox.,<extra_id_0> problem was some idiot made ridiculous edit to info info\n",
      "224,zh,æœ‰é’±çš„è›®å¤·è‡ªç„¶æƒ¹äººè®°æ¨,æœ‰é’±çš„å°‘æ•°æ°‘æ—è‡ªç„¶æƒ¹äººè®°æ¨,æœ‰é’±çš„è›®å¤·è‡ªç„¶æƒ¹äººè®°æ¨\n",
      "225,ru,Ñ‚Ñ€Ğ¾Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… ĞµĞ±Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ¼ĞµĞ½Ñ (Ğ´ÑƒĞ¼Ğ°Ñ) Ğ´Ğ¾ ÑĞ¸Ñ… Ğ¿Ğ¾Ñ€ ĞºĞ»ÑĞ½ÑƒÑ‚.,Ñ‚Ñ€Ğ¾Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¼ĞµĞ½Ñ (Ğ´ÑƒĞ¼Ğ°Ñ) Ğ´Ğ¾ ÑĞ¸Ñ… Ğ¿Ğ¾Ñ€ ĞºĞ»ÑĞ½ÑƒÑ‚.,<extra_id_0> Ñ‚Ğ°ĞºĞ¸Ñ… ĞµĞ±Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ¼ĞµĞ½Ñ Ğ´Ğ¾ ÑĞ¸Ñ… Ğ¿Ğ¾Ñ€ ĞºĞ»ÑĞ½ÑƒÑ‚.\n",
      "226,am,áŠ áŠ•á‰º áŒˆáˆ¨á‹µ : áˆ¸á‰³á‰³ áŠ áŠ•á‰½áŠ• á‰¥áˆ á€áˆ“áŠ  áˆ˜áŒ€áˆ˜áˆªá‹« áŠ•á…áˆ…áŠ“áˆ½áŠ• áŒ á‰¥á‰‚ : á‹á‹³á‰‚ á¢,áŠ áŠ•á‰½áŠ• á‰¥áˆ á€áˆ“áŠ áˆ˜áŒ€áˆ˜áˆªá‹« áŠ¥áˆ«áˆµáˆ½áŠ• áŒ á‰¥á‰‚ :,áˆ¸á‰³á‰³ áŠ áŠ•á‰½áŠ• á‰¥áˆ á€áˆ“áŠ áˆ˜áŒ€áˆ˜áˆªá‹« áŠ•á…áˆ…áŠ“áˆ½áŠ• áŒ á‰¥á‰‚ : á‹á‹³á‰‚ á¢\n",
      "227,en,they don t give a shit about terrorism .,They donâ€™t care about terrorism at all.,<extra_id_0> they don t give a shit about terrorism\n",
      "228,de,Was ist den das fette etwas da ..?,Was ist den das da ..?,<extra_id_0> was ist den das fette etwas da ..?\n",
      "229,ru,ĞµÑĞ»Ğ¸ ÑÑƒĞºĞ° Ñ‚Ñ‹ Ñ‚Ğ¾Ñ€Ğ¾Ğ¿Ğ¸ÑˆÑÑ Ğ²Ñ‹ĞµÑĞ¶Ğ°Ğ¹ Ğ¿Ğ¾Ñ€Ğ°Ğ½ÑŒÑˆĞµ,ĞµÑĞ»Ğ¸ Ñ‚Ğ¾Ñ€Ğ¾Ğ¿Ğ¸Ñ‚ĞµÑÑŒ Ğ²Ñ‹ĞµĞ·Ğ¶Ğ°Ğ¹ Ğ¿Ğ¾Ñ€Ğ°Ğ½ÑŒÑˆĞµ,ĞµÑĞ»Ğ¸ ÑÑƒĞºĞ° Ñ‚Ñ‹ Ñ‚Ğ¾Ñ€Ğ¾Ğ¿Ğ¸ÑˆÑÑ Ğ²Ñ‹ĞµÑĞ¶Ğ°Ğ¹ Ğ¿Ğ¾Ñ€Ğ°Ğ½ÑŒÑˆĞµ\n",
      "230,zh,å¦‚æœä¸é©±é™¤è¿™é»‘è™«åˆ°æ—¶å€™å…¨æ˜¯è¿™æ ·çš„å°é»‘è™«è¯´è‡ªå·±æ˜¯æ±‰äººæ˜¯åå¤å­å­™,å¦‚æœä¸é©±é™¤è¿™è®©æˆ‘ä¸å–œæ¬¢çš„é»‘äººåˆ°æ—¶å€™å…¨æ˜¯è¿™æ ·çš„é»‘äººå°å­©è¯´è‡ªå·±æ˜¯æ±‰äººæ˜¯åå¤å­å­™,å¦‚æœä¸é©±é™¤è¿™é»‘è™«åˆ°æ—¶å€™å…¨æ˜¯è¿™æ ·çš„å°é»‘è™«è¯´è‡ªå·±æ˜¯æ±‰äººæ˜¯åå¤å­å­™\n",
      "231,en,\"they have all the fucking money , how am i supposed to earn my billions ? .\",how am i to earn when they own all the money?,\"<extra_id_0> they have all the money , how am i supposed to earn my billions ? .\"\n",
      "232,zh,å’Œé»‘äººç»“å©šç”Ÿçš„æ‚ç§éƒ½åº”è¯¥ç­æ€æ‰,æˆ‘è®¤ä¸ºå’Œé»‘äººç»“å©šç”Ÿçš„æ··è¡€å„¿éƒ½åº”è¯¥è®©ä»–ä»¬ä»˜å‡ºä»£ä»·,å’Œé»‘äººç»“å©šç”Ÿçš„æ‚ç§éƒ½åº”è¯¥ç­æ€æ‰\n",
      "233,es,jaja eso es mejor que ser maricon o comunista,jaja eso es mejor que ser comunista,<extra_id_0> eso es mejor que ser maricon o comunista\n",
      "234,hi,à¤¯à¤¹à¥€  à¤¸à¤¾à¤²à¥‡ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤° à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤ªà¤¹à¤²à¥‡ à¤šà¥à¤ª à¤¥à¥‡ !,à¤¯à¤¹à¥€ à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤ªà¤¹à¤²à¥‡ à¤šà¥à¤ª à¤¥à¥‡ !,<extra_id_0>à¤¹à¥€ à¤¸à¤¾à¤²à¥‡ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤° à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤ªà¤¹à¤²à¥‡ à¤šà¥à¤ª à¤¥à¥‡ !\n",
      "235,ru,\"Ñ€Ğ¾ÑÑĞ¸Ñ - ÑÑ‚Ñ€Ğ°Ğ½Ğ° ĞµĞ±Ğ°Ğ½Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ². Ğ²ÑĞµ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ğ½Ğ¾ Ğ½Ğ¸ĞºÑ‚Ğ¾ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚\",\"Ğ’ Ğ Ğ¾ÑÑĞ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ğ½Ğ¾ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ÑÑ‚.\",\"<extra_id_0> Ñ€Ğ¾ÑÑĞ¸Ñ - ÑÑ‚Ñ€Ğ°Ğ½Ğ° ĞµĞ±Ğ°Ğ½Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ². Ğ²ÑĞµ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ğ½Ğ¾ Ğ½Ğ¸ĞºÑ‚Ğ¾ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚\"\n",
      "236,uk,Ğ‘Ğ¾ Ğ²Ğ°Ğ¼ Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ°Ñ”Ñ‚ÑŒÑÑ.Ñ‚Ğ° Ğ±Ğ»ÑÑ‚ÑŒ.,Ğ‘Ğ¾ Ğ²Ğ°Ğ¼ Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ°Ñ”Ñ‚ÑŒÑÑ.Ñ‚Ğ° Ğ±Ğ»Ñ–Ğ½.,<extra_id_0> Ğ²Ğ°Ğ¼ Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ°Ñ”Ñ‚ÑŒÑÑ\n",
      "237,en,\"call me stupid , but it seems the page was not moved so much as copied .\",\"call me illogical, but it seema the page was not moved so much as copied.\",<extra_id_0> it seems the page was not moved so much as copied .\n",
      "238,ar,\"Ø§Ø¶Ø­Ùƒ Ù…Ø¹ Ø®Ø±Ø§ÙØ§Øª Ø§Ù„Ø±Ø§ÙØ¶Ø© ğŸ˜‚ğŸ¤£ Ù‚Ø§Ù„ Ø§Ù„Ø®Ù…ÙŠÙ†ÙŠ: \"\" Ø³ÙØ¨Ø¨Ù Ù…ÙÙ„ÙˆØ­ÙØ© Ù…ÙÙŠØ§Ù‡ Ø§Ù„Ø¨Ø­Ø±Ù Ù‡ÙÙˆ Ø¨ÙƒØ§Ø¡ Ø§Ù„Ø£Ø³Ù…Ø§ÙƒÙ Ø¹Ù„Ù‰ Ù…Ù‚ØªÙ„Ù Ø§Ù„Ø­Ø³ÙŠÙ†.! ğŸ“š|[Ø¨Ø­Ø± Ø§Ù„Ø£Ù†ÙˆØ§Ø±(Øµ: Ù¡Ù¢Ù¥)]\",\"Ø§Ø¶Ø­Ùƒ Ù…Ø¹ Ø®Ø±Ø§ÙØ§Øª Ø§Ù„Ø´ÙŠØ¹Ø©ğŸ˜‚ğŸ¤£ Ù‚Ø§Ù„ Ø§Ù„Ø®Ù…ÙŠÙ†ÙŠ: \"\" Ø³ÙØ¨Ø¨Ù Ù…ÙÙ„ÙˆØ­ÙØ© Ù…ÙÙŠØ§Ù‡ Ø§Ù„Ø¨Ø­Ø±Ù Ù‡ÙÙˆ Ø¨ÙƒØ§Ø¡ Ø§Ù„Ø£Ø³Ù…Ø§ÙƒÙ Ø¹Ù„Ù‰ Ù…Ù‚ØªÙ„Ù Ø§Ù„Ø­Ø³ÙŠÙ†.! ğŸ“š|[Ø¨Ø­Ø± Ø§Ù„Ø£Ù†ÙˆØ§Ø±(Øµ: Ù¡Ù¢Ù¥)]\",'' Ø³ÙØ¨Ø¨Ù Ù…ÙÙ„ÙˆØ­ÙØ© Ù…ÙÙŠØ§Ù‡ Ø§Ù„Ø¨Ø­Ø±Ù Ù‡ÙÙˆ Ø¨ÙƒØ§Ø¡ Ø§Ù„Ø£Ø³Ù…Ø§ÙƒÙ Ø¹Ù„Ù‰ Ù…Ù‚ØªÙ„Ù Ø§Ù„Ø­Ø³ÙŠÙ†.! ğŸ“š|[Ø¨Ø­Ø± Ø§Ù„Ø£Ù†ÙˆØ§Ø±(Øµ: Ù¡Ù¢Ù¥)]\n",
      "239,am,áˆ˜áŠ•áŒáˆµá‰±áŠ•/á‹°áˆ­áŒáŠ• áŠ¥áŠ•á‹° reference/benchmark áˆ˜áŒ á‰€áˆ áˆ˜á‰¼ áŠá‹ á‹¨á‰°áŒ€áˆ˜áˆ¨á‹?áŠ¨á‰°áŒ€áˆ˜áˆ¨áˆµ áˆˆáˆ· á‰¥á‰» áˆˆáˆáŠ• á‹­áˆ°áˆ«áˆ?áŠ¥áŠ•á‹°á‹áˆ áˆµáˆˆáŠ á‰£á‰· áˆ°á‹ á‰ áˆ‹áŠá‰µ áˆ³á‹­áˆ†áŠ• áˆˆáˆáŠ• áŠ á‰µá‹°áŒáˆá‹áˆ á‹­á‰º,áˆ˜áŠ•áŒáˆµá‰±áŠ• áŠ¥áŠ•á‹° reference/benchmark áˆ˜áŒ á‰€áˆ áˆ˜á‰¼ áŠá‹ á‹¨á‰°áŒ€áˆ˜áˆ¨á‹?áŠ¨á‰°áŒ€áˆ˜áˆ¨áˆµ áˆˆáˆ· á‰¥á‰» áˆˆáˆáŠ• á‹­áˆ°áˆ«áˆ? áŠ¨áˆáˆˆáŒˆá‰½ áˆˆáˆáŠ• á‰ á‹°áŠ•á‰¥ áŠ á‰µá‹°áŒáˆá‹áˆ,áˆ˜áŠ•áŒáˆµá‰±áŠ•/á‹°áˆ­áŒáŠ• áŠ¥áŠ•á‹° reference/benchmark áˆ˜áŒ á‰€áˆ áˆ˜á‰¼ áŠá‹ á‹¨á‰°áŒ€áˆ˜áˆ¨á‹?áŠ¨á‰°áŒ€áˆ˜áˆ¨áˆµ áˆˆáˆ· á‰¥á‰» áˆˆáˆáŠ• á‹­áˆ°áˆ«áˆ?áŠ¥áŠ•á‹°á‹áˆ áˆµáˆˆáŠ á‰£á‰· áˆ°á‹ á‰ áˆ‹áŠá‰µ áˆ³á‹­áˆ†áŠ• áˆˆáˆáŠ• áŠ á‰µá‹°áŒáˆá‹áˆ á‹­á‰º\n",
      "240,de,\"Diese Migrantenkrieg der Schmarotzergegen Deutschland, ist schon in Vorbereitung!\",\"Diese Migrantenkrieg der Einwanderer gegen Deutschland, ist schon in Vorbereitung!\",\"<extra_id_0> diese Migrantenkrieg der Schmarotzergegen Deutschland, ist schon in Vorbereitung!\"\n",
      "241,es,ESPAÃ‘A CORRUPPCIÃ“N Derecha PP fascista franquista gobernando con 30% en EspaÃ±a no entiende porquÃ© CataluÃ±a quiere Independencia y RepÃºblica.,ESPAÃ‘A un partido de derecha CORRUPTO que gobierna con el 30% en EspaÃ±a no entiende por quÃ© CataluÃ±a quiere la Independencia y la RepÃºblica.,<extra_id_0> fascista franquista gobernando con 30% en EspaÃ±a no entiende porquÃ© CataluÃ±a quiere Independencia y RepÃºblica.\n",
      "242,ru,\".....Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ¸ ÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ¾....., Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ¾Ğ² Ğ½Ğ° ĞºĞ¸Ñ‡Ğ¼Ğ°Ğ½....\",.....Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ¸ ÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ¾.....,\"<extra_id_0> Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ¸ ÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ¾....., Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ¾Ğ² Ğ½Ğ° ĞºĞ¸Ñ‡Ğ¼Ğ°Ğ½....\"\n",
      "243,ar,Ø§Ø´ÙˆÙ Ø¨ÙˆÙƒØ§ÙƒÙˆ Ø¨Ø¹Ø¯Ù‡ ÙƒÙŠÙ† Ø«Ù… Ù„ÙŠÙØ§ Ù„Ø§Ù†Ù‡ Ø¨Ø¯ÙˆØ±ÙŠ Ù‡Ø¨Ø§ÙŠÙ„,Ø§Ø´ÙˆÙ Ø¨ÙˆÙƒØ§ÙƒÙˆ Ø¨Ø¹Ø¯Ù‡ ÙƒÙŠÙ† Ø«Ù… Ù„ÙŠÙØ§ Ù„Ø§Ù†Ù‡ Ø¨Ø¯ÙˆØ±ÙŠ Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†,Ø«Ù… Ù„ÙŠÙØ§ Ù„Ø§Ù†Ù‡ Ø¨Ø¯ÙˆØ±ÙŠ Ù‡Ø¨Ø§ÙŠÙ„\n",
      "244,am,á‰ á‹˜áˆ«áˆ…á‹‰ áˆáŠ­ áˆ›áŒ¨á‹µáˆ… áŠ á‹­á‰€áˆ­áˆ á‹¨áŒŠá‹œ áŒ‰á‹³á‹­ áŠ¥áŠ•áŒ…,á‰ áŒŠá‹œ áˆ‚á‹°á‰µ áˆ›áŠ•áˆ áˆˆáˆ°áˆ«á‹ á‰ á‹°áˆ áˆ˜áˆáˆ±áŠ• áˆ›áŒáŠ˜á‰± áŠ á‹­á‰€áˆ­áˆ,áˆáŠ­ áˆ›áŒ¨á‹µáˆ… áŠ á‹­á‰€áˆ­áˆ á‹¨áŒŠá‹œ áŒ‰á‹³á‹­ áŠ¥áŠ•áŒ…\n",
      "245,es,\"Lo que no se es como no pone varias lineas de ferry,\n",
      "2500 pavos cada uno y llenan.\",\"No entiendo por quÃ© no establecen varias lÃ­neas de ferry, podrÃ­an ganar 2500 euros por cada uno y llenarlos.\",\"<extra_id_0> que no se es como no pone varias lineas de ferry, 2500 pavos cada uno y llenan.\"\n",
      "246,zh,å°ä»™å¥³ä¸Šè¾ˆå­è¿™ä¹ˆè¿‡æ¥çš„,è‡ªæˆ‘æ„Ÿè§‰è¿‡äºè‰¯å¥½çš„å¥³æ€§ä¸Šè¾ˆå­è¿™ä¹ˆè¿‡æ¥çš„,å°ä»™å¥³ä¸Šè¾ˆå­è¿™ä¹ˆè¿‡æ¥çš„\n",
      "247,uk,\"'Ğ¢ÑƒÑ‚ Ğ°Ğ±Ğ¾ Ğ¢Ğ¸Ğ½ĞºĞ°, Ğ¢Ñ–Ğ½Ğ°, Ñ– ĞšÑ€Ñ–ÑÑ‚Ñ–Ğ½Ğ° ÑĞº ĞºĞ°Ğ¶ÑƒÑ‚ÑŒ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ¾ÑĞ¸, Ğ½Ñƒ Ğ°Ğ±Ğ¾ Ğ²Ğ¶Ğµ Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¼ Ñ–Ğ¼''ÑĞ¼.'\",\"'Ğ¢ÑƒÑ‚ Ğ°Ğ±Ğ¾ Ğ¢Ğ¸Ğ½ĞºĞ°, Ğ¢Ñ–Ğ½Ğ°, Ñ– ĞšÑ€Ñ–ÑÑ‚Ñ–Ğ½Ğ° ÑĞº ĞºĞ°Ğ¶ÑƒÑ‚ÑŒ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½Ñ†Ñ–, Ğ½Ñƒ Ğ°Ğ±Ğ¾ Ğ²Ğ¶Ğµ Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¼ Ñ–Ğ¼''ÑĞ¼'\",\"','Ğ¢ÑƒÑ‚ Ğ°Ğ±Ğ¾ Ğ¢Ñ–Ğ½Ğ°, Ğ¢Ñ–Ğ½Ğ°, Ñ– ĞšÑ€Ñ–ÑÑ‚Ñ–Ğ½Ğ° ÑĞº ĞºĞ°Ğ¶ÑƒÑ‚ÑŒ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ¾ÑĞ¸, Ğ½Ñƒ Ğ°Ğ±Ğ¾ Ğ²Ğ¶Ğµ Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¼ Ñ–Ğ¼''ÑĞ¼.'\"\n",
      "248,ru,Ğ³Ğ½Ğ¸Ğ´Ñ‹ Ğ¿Ğ¸Ğ½Ğ´Ğ¾ÑĞ¾Ğ²ÑĞºĞ¸Ğµ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚ ÑĞµĞ±Ñ Ñ…Ğ¾Ğ·ÑĞ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸,ĞĞ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½Ñ†Ñ‹ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚ ÑĞµĞ±Ñ Ñ…Ğ¾Ğ·ÑĞµĞ²Ğ°Ğ¼Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸,Ğ¿Ğ¸Ğ½Ğ´Ğ¾ÑĞ¾Ğ²ÑĞºĞ¸Ğµ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚ ÑĞµĞ±Ñ Ñ…Ğ¾Ğ·ÑĞ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸\n",
      "249,de,Bei einer psycho Sekte von freiwillig zu sprechen ist mutig.,Bei einer Sekte von freiwillig zu sprechen ist mutig.,einer psycho Sekte von freiwillig zu sprechen ist mutig.\n",
      "250,hi,à¤¯à¥‡ à¤­à¤¡à¤¼à¤µà¤¾ à¤¸à¤°à¤•à¤¾à¤° à¤†à¤–à¤¿à¤° à¤•à¥à¤¯à¤¾ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥€ à¤¹à¥ˆ à¥¤à¥¤à¥¤à¥¤,à¤¯à¥‡ à¤¸à¤°à¤•à¤¾à¤° à¤†à¤–à¤¿à¤° à¤•à¥à¤¯à¤¾ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥€ à¤¹à¥ˆ à¥¤à¥¤à¥¤à¥¤,'' à¤¯à¥‡ à¤­à¤¡à¤¼à¤µà¤¾ à¤¸à¤°à¤•à¤¾à¤° à¤†à¤–à¤¿à¤° à¤•à¥à¤¯à¤¾ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥€ à¤¹à¥ˆ à¥¤à¥¤à¥¤à¥¤\n",
      "251,ar,Ø§Ù†Øª Ø§Ø­ÙˆÙ„ ÙØ±ÙŠÙ‚Ùƒ Ø¯Ø§Ø¬ ..Ø¨Ù„Ø§Ø´ Ù…Ø¬Ø§Ù…Ù„Ù‡ ÙˆØªØ·Ø¨ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙƒÙ…,Ø§Ù†Øª Ù…Ø´ Ø´Ø§ÙŠÙ ÙØ±ÙŠÙ‚Ùƒ Ø¯Ø§Ø¬ ..Ø¨Ù„Ø§Ø´ Ù…Ø¬Ø§Ù…Ù„Ù‡ ÙˆØªØ·Ø¨ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙƒÙ…,.Ø¨Ù„Ø§Ø´ Ù…Ø¬Ø§Ù…Ù„Ù‡ ÙˆØªØ·Ø¨ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙƒÙ…\n",
      "252,de,\"ğŸ˜‚ğŸ˜‚ğŸ˜‚ Phantomschmerz (fehlende Eier) und Machtgeilheit zwangen heute erneut den \"\"Bayern-Horsti\"\" ohne Vaseline  in den Ar*** der Kanzlerette ğŸ˜œ\",\"ğŸ˜‚ğŸ˜‚ğŸ˜‚ Phantomschmerz und Machtgier zwangen heute erneut den \"\"Bayern-Horsti\"\" sich bei der Kanzlerin einzuschmeicheln ğŸ˜œ\",\"<extra_id_0> Phantomschmerz (fehlende Eier) und Machtgeilheit zwangen heute erneut den \"\"Bayern-Horsti\"\" ohne Vaseline in den Ar*** der Kanzlerette ğŸ˜‚ğŸ˜‚ğŸ˜‚ Phantomschmerz (fehlende Eier) und Machtgeilheit zwangen heute den \"\"Bayern-Horsti\"\" ohne Vaseline in den Ar*** der Kanzlerette ğŸ˜œ\"\n",
      "253,ar,Ù£ Ù…Ù„ÙŠÙˆÙ† Ø²Ù†Ø¬ÙŠ ÙŠÙ‚ÙˆÙ…ÙˆÙ† Ø¨ ØªØ´ÙŠ ØªØ´ÙŠ,Ù£ Ù…Ù„ÙŠÙˆÙ† Ø¥ÙØ±ÙŠÙ‚ÙŠ ÙŠÙ‚ÙˆÙ…ÙˆÙ† Ø¨ ØªØ´ÙŠ ØªØ´ÙŠ,Ù£ Ù…Ù„ÙŠÙˆÙ† Ø²Ù†Ø¬ÙŠ ÙŠÙ‚ÙˆÙ…ÙˆÙ† ØªØ´ÙŠ\n",
      "254,de,\"Gabriel, was bist Du fÃ¼r ein scheinheiliger AuÃŸerrandundband-Minister.. Verkaufst Deine Demokratie an das Sultanat. Raus mit Dir.\",\"Gabriel, was bist Du fÃ¼r ein unaufrichtiger AuÃŸerrandundband-Minister.. Verkaufst Deine Demokratie an das Sultanat.\",\"<extra_id_0>, was bist Du fÃ¼r ein scheinheiliger AuÃŸerrandundband-Minister.. Verkaufst Deine Demokratie an das Sultanat. Raus mit Dir\"\n",
      "255,de,Gesicht???? Was fÃ¼r ein Gesicht. Der hat nicht einmal einen Hals. Bei uns heiÃŸt so etwas Wutzekopp.,Gesicht???? Was fÃ¼r ein Gesicht. Der hat nicht einmal einen Hals.,<extra_id_0> hat nicht einmal einen Hals. Bei uns heiÃŸt so etwas Wutzekopp.\n",
      "256,en,you see that kid talking to me i hate him he never leaves me alone.,\"you see that kid talking to me i don't really like him, because he never leaves me alone.\",<extra_id_0> you see that kid talking to me he never leaves me alone.\n",
      "257,ar,ØªØ±Ø§ Ø§Ù„Ø¹Ø±Ø´ Ø­Ù‚ÙŠ Ø¨Ø£ÙˆÙØ±ÙˆØ§ØªØ´ Ø§Ù†Ù‡Ø¯Ù… ÙˆØ·Ø§Ø­ ÙˆØ£Ù†ØªÙŠ Ø§Ù„Ø³Ø¨Ø¨ Ø¹Ø·ÙŠØªÙƒ Ø§Ù„Ø´Ø¹Ù„Ø© ÙˆØ·ÙÙŠØªÙŠÙ‡Ø§ Ø§Ù„Ù…ÙØ±ÙˆØ¶ ØªØ±Ø¬Ø¹ÙŠÙ† Ù„ÙŠ Ø¹Ø±Ø´ÙŠ ÙŠÙ„Ø¹Ù† Ø£Ø¨ÙˆÙƒ,ØªØ±Ø§ Ø§Ù„Ø¹Ø±Ø´ Ø­Ù‚ÙŠ Ø¨Ø£ÙˆÙØ±ÙˆØ§ØªØ´ Ø§Ù†Ù‡Ø¯Ù… ÙˆØ·Ø§Ø­ ÙˆØ£Ù†ØªÙŠ Ø§Ù„Ø³Ø¨Ø¨ Ø¹Ø·ÙŠØªÙƒ Ø§Ù„Ø´Ø¹Ù„Ø© ÙˆØ·ÙÙŠØªÙŠÙ‡Ø§ Ø§Ù„Ù…ÙØ±ÙˆØ¶ ØªØ±Ø¬Ø¹ÙŠÙ† Ù„ÙŠ Ø¹Ø±Ø´ÙŠ ÙŠØ±Ø­Ù… Ø£Ø¨ÙˆÙƒ,Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ø³Ø¨Ø¨ Ø¹Ø·ÙŠØªÙƒ Ø§Ù„Ø´Ø¹Ù„Ø© ÙˆØ·ÙÙŠØªÙŠÙ‡Ø§ Ø§Ù„Ù…ÙØ±ÙˆØ¶ ØªØ±Ø¬Ø¹ÙŠÙ† Ù„ÙŠ Ø¹Ø±Ø´ÙŠ ÙŠÙ„Ø¹Ù† Ø£Ø¨ÙˆÙƒ\n",
      "258,uk,ĞœĞ¾Ğ¶Ğ½Ğ° Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ñƒ Ğ±ÑƒÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ñ ÑĞº Ñ Ñ” Ğ°Ğ»Ğ¾ Ğ¼ĞµĞ½Ñ– Ğ¼Ğ°Ñ” Ğ±ÑƒÑ‚ÑŒ Ğ¿Ğ¾Ñ…ÑƒĞ¹ Ğ° Ñ‡ÑŒĞ¾Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ñ…ÑƒĞ¹,ĞœĞ¾Ğ¶Ğ½Ğ° Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ñƒ Ğ±ÑƒÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ñ ÑĞº Ñ Ñ” Ğ°Ğ»Ğ¾ Ğ¼ĞµĞ½Ñ– Ğ¼Ğ°Ñ” Ğ±ÑƒÑ‚ÑŒ Ğ±Ğ°Ğ¹Ğ´ÑƒĞ¶Ğµ Ğ° Ñ‡ÑŒĞ¾Ñ‚Ğ¾ Ğ½Ğµ Ğ±Ğ°Ğ¹Ğ´ÑƒĞ¶Ğµ,<extra_id_0> Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ñƒ Ğ±ÑƒÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ñ ÑĞº Ñ Ñ” Ğ° Ñ‡ÑŒĞ¾Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ñ…ÑƒĞ¹\n",
      "259,es,La Ãºltima mamarrachada de EA es tremenda.,La Ãºltima payasada de EA es tremenda.,<extra_id_0> La Ãºltima mamarrachada de EA es tremenda.\n",
      "260,am,á‹­áˆ„ á‹¨á‹°á‹°á‰¥ á‹¨á‰°áˆ³á‹³á‰¢ á‰¦á‰³ áŠ á‹­á‹°áˆˆáˆ á‰¦á‰³áˆ…áŠ• áˆáˆáŒ:: áŠ¥áŠ› áˆˆáˆšáˆ°áˆ› áˆˆáˆšáŒ á‹­á‰… áˆˆáˆšá‹«áŠá‰¥ á‰¥á‰» áŠá‹ á‰¦á‰³ á‹«áˆˆáŠ•::,áŠ¥á‹šáˆ… á‰¦á‰³ áŠ¥áŠ› áˆµá‹µá‰¥áŠ• áŠ áŠ•á‰€á‰ áˆáˆ  :: áˆ›á‹ˆá‰… á‹«áˆˆá‰¥áˆ… áŠ¥áŠ› áˆˆáˆšáˆ°áˆ› áˆˆáˆšáŒ á‹­á‰… áˆˆáˆšá‹«áŠá‰¥ á‰¥á‰» áŠá‹ á‰¦á‰³ á‹«áˆˆáŠ•::,á‰¦á‰³áˆ…áŠ• áˆáˆáŒ:: áŠ¥áŠ› áˆˆáˆšáˆ°áˆ› áˆˆáˆšáŒ á‹­á‰… áˆˆáˆšá‹«áŠá‰¥ á‰¥á‰» áŠá‹ á‰¦á‰³ á‹«áˆˆáŠ•::\n",
      "261,ar,Ø±Ø¯ÙˆØ§ Ù…ØµØ±ÙŠØ§Øª Ø§Ù„Ù†Ø§Ø³ ÙŠØ§ Ø¹ÙƒØ§Ø±ÙŠØª ÙˆØ´ÙˆÙÙˆØ§ ÙƒÙŠÙ Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ Ø¨ÙŠØµÙŠØ± Ù…Ø¬ÙŠØ¯ ÙˆØ§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨ÙŠØµÙŠØ± Ø³Ø¹ÙŠØ¯ØŒ Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ÙˆØ¹,Ø±Ø¯ÙˆØ§ Ù…ØµØ±ÙŠØ§Øª Ø§Ù„Ù†Ø§Ø³ ÙˆØ´ÙˆÙÙˆØ§ ÙƒÙŠÙ Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ Ø¨ÙŠØµÙŠØ± Ù…Ø¬ÙŠØ¯ ÙˆØ§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨ÙŠØµÙŠØ± Ø³Ø¹ÙŠØ¯ØŒ,ÙˆØ´ÙˆÙÙˆØ§ ÙƒÙŠÙ Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ Ø¨ÙŠØµÙŠØ± Ù…Ø¬ÙŠØ¯ ÙˆØ§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨ÙŠØµÙŠØ± Ø³Ø¹ÙŠØ¯ØŒÙ„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ÙˆØ¹\n",
      "262,ru,\"Ğ¯ Ğ²ÑĞµĞ³Ğ´Ğ° Ñ ĞºĞ»ĞµĞ¿ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµÑ€Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹ Ğ¼Ğ¾Ğ·Ğ³ Ğ½Ğµ ĞµĞ±ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ¸ĞºÑƒĞ´Ğ° Ğ½Ğµ ĞµĞ·Ğ´Ğ¸Ñ‚ÑŒ.\",\"Ğ¯ Ğ²ÑĞµĞ³Ğ´Ğ° Ñ ĞºĞ»ĞµĞ¿ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµÑ€Ñƒ , Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ½Ğ¸ĞºÑƒĞ´Ğ° Ğ½Ğµ ĞµĞ·Ğ´Ğ¸Ñ‚ÑŒ\",\"<extra_id_0> Ğ²ÑĞµĞ³Ğ´Ğ° Ñ ĞºĞ»ĞµĞ¿ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµÑ€Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹ Ğ¼Ğ¾Ğ·Ğ³ Ğ½Ğµ ĞµĞ±ÑÑ‚Ğ¸.\"\n",
      "263,de,Hier sind schon ein paar richtig dumme Menschen unterwegs..... Gegen wenn oder was habt ihr eigentlich vor der AfD gehetzt und gestÃ¤nkert?. Ich pubertierende Pickelplantagen...,Hier sind schon ein paar richtig naive Menschen unterwegs..... Gegen wenn oder was habt ihr eigentlich vor der AfD gehetzt und gestÃ¤nkert?.,<extra_id_0> sind schon ein paar richtig dumme Menschen unterwegs..... Gegen wenn oder was habt ihr eigentlich vor der AfD gehetzt und gestÃ¤nkert?. Ich pubertierende Pickelplantagen...\n",
      "264,de,Ich will nicht das es Kriminellen FlÃ¼chtlingen Deserteuren und Feiglingen hier besser geht..erst recht nicht dem Afrika-Pack. Und weder bin ich Rassist noch ein Nazi noch xenophob....!,Ich will nicht das es Kriminellen FlÃ¼chtlingen und Deserteuren hier besser geht..erst recht nicht Menschen aus Afrika. Und weder bin ich Rassist noch ein Nazi noch xenophob....!,<extra_id_0> ich will nicht das es Kriminellen FlÃ¼chtlingen Deserteuren und Feiglingen hier besser geht..erst recht nicht dem Afrika-Pack.\n",
      "265,de,\"In solchen Situationen zeigt sich echtes Herz (Empathie)  und Charakter (Hilfe, die nicht in der Zeitung steht) im Gegensatz zu heuchlerischem Gutmenschentum\",\"In solchen Situationen zeigt sich echtes Herz (Empathie) und Charakter (Hilfe, die nicht in der Zeitung steht) im Gegensatz zu inszeniertem Gutmenschentum\",\"<extra_id_0> zeigt sich echtes Herz (Empathie) und Charakter (Hilfe, die nicht in Zeitung steht) im Gegensatz zu heuchlerischem Gutmenschentum\"\n",
      "266,ru,\"Ğ¿Ğ¾ÑĞ°Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ°ÑˆĞ¸ Ğ¿ĞµĞ½ÑĞ¸Ğ¸, ÑƒÑ‰Ğ½Ğ°ĞµÑ‚ ÑÑƒĞºĞ°\",ĞŸĞ¾ÑĞ°Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ñ‹ Ğ½Ğ° Ğ½Ğ°ÑˆĞ¸ Ğ¿ĞµĞ½ÑĞ¸Ğ¸,\"Ğ½Ğ°ÑˆĞ¸ Ğ¿ĞµĞ½ÑĞ¸Ğ¸, ÑƒÑ‰Ğ½Ğ°ĞµÑ‚ ÑÑƒĞºĞ°\"\n",
      "267,de,Die Hetzer der Ã–R Medien haben wieder Hochkonjunktur dieses ganze Schweinejournalisten Pack gehÃ¶rt entsorgt und dass schnellstmÃ¶glich. AFD,Die Kritiker der Ã–R Medien haben wieder Hochkonjunktur diese ganzen Journalisten gehÃ¶ren weg und dass schnellstmÃ¶glich. AFD,<extra_id_0> die Hetzer der Ã–R Medien haben wieder Hochkonjunktur dieses ganze Schweinejournalisten Pack gehÃ¶rt entsorgt und dass schnellstmÃ¶glich AFD\n",
      "268,zh,é‚£è¦çŸ¥é“è¿™äº›å¥³äººç”·äººå¡‘é€ çš„ï¼Œä¸è¿‡æ˜¯ç”·äººæ€•å…¶ä»–ç”·äººå˜æˆèˆ”ç‹—ç½¢äº†,é‚£è¦çŸ¥é“è¿™äº›å¥³äººç”·äººå¡‘é€ çš„ï¼Œä¸è¿‡æ˜¯ç”·äººæ€•å…¶ä»–ç”·äººå˜æˆå•æ–¹é¢ä»˜å‡ºçš„äººç½¢äº†,\"é‚£è¦çŸ¥é“è¿™äº›å¥³äººç”·äººå¡‘é€ çš„,ä¸è¿‡æ˜¯ç”·äººæ€•å…¶ä»–ç”·äººå˜æˆèˆ”ç‹—ç½¢äº†\"\n",
      "269,am,á‹¨á‰€áŠ• áŒ…á‰¥áˆµ áŠ áŠ•á‹° áŠ áŠ•á‰º áŠ á‹­áŠá‰¶á‰¹ áŠ“á‰¸á‹á£ áˆµá‹ á‹¨áˆšá‹­áˆ­á‹±á‰µ á£ áˆµá‹ á‹¨áˆšá‹«á‰ƒáŒ¥áˆ‰á‰µ áŠ¥áŠ“ á‹¨áˆšáŒ­ááˆ©á‰µ áˆ˜á‹°áˆ˜áˆ­ áŠ¥áŠ“ áˆ˜á‰€áŠáˆµ á‰°áˆ›áˆª á‰¡á‹³,á‹¨áˆ˜áˆáŠ«áˆ áŠ áˆµá‰µá‹°á‹°áˆ­ á‰½áŒáˆ­ á‹«áˆˆá‰£á‰¸á‹ áŠ¥áŠ•á‹° áŠ áŠ•á‰º áŠ á‹­áŠá‰¶á‰¹ áŠ“á‰¸á‹á£ áˆˆáˆ°á‹ áŠ á‹˜áŠ’á‰³ á‹¨áˆŒáˆ‹á‰¸á‹,á‹¨á‰€áŠ• áŒ…á‰¥áˆµ áŠ áŠ•á‹° áŠ áŠ•á‰º áŠ á‹­áŠá‰¶á‰¹ áŠ“á‰¸á‹á£ áˆµá‹ á‹¨áˆšá‹«á‰ƒáŒ¥áˆ‰á‰µ áŠ¥áŠ“ á‹¨áˆšáŒ­ááˆ©á‰µ áˆ˜á‹°áˆ˜áˆ­ áŠ¥áŠ“ áˆ˜á‰€áŠáˆµ á‰°áˆ›áˆª á‰¡á‹³\n",
      "270,en,\"then all of a sudden i see her , shes now got the big phony tits and everything .\",\"All of a sudden i see her, she is all grown up.\",\"<extra_id_0> then all of a sudden i see her , shes now got the big tits and everything .\"\n",
      "271,am,áŠ áŠ•á‰° áˆ›áˆ˜áŠ• áŠ«áˆáˆáˆˆáŒáŠ­ áˆáŠ•áˆ á‰¢áˆ‰áˆ… áˆ›áˆ˜áŠ• áŠ á‰µáˆáˆáŒáˆá¢ áŠ¥áˆ…á‰µáˆ… á‰°á‹°áŠáˆªá‹«áˆˆáˆ áŠ«áˆˆá‰½áˆ… áŠ áŠ•á‰° áˆ›áŠ• áŠáˆ… áŠ á‹­ á‹áˆ¸á‰µáˆ½ áŠá‹,áŠ áŠ•á‰° áˆ›áˆ˜áŠ• áŠ«áˆáˆáˆˆáŒáŠ­ áˆáŠ•áˆ á‰¢áˆ‰áˆ… áˆ›áˆ˜áŠ• áŠ á‰µáˆáˆáŒáˆá¢ áŠ¥áˆ…á‰µáˆ… áŒ¥á‰ƒá‰µ á‹°áˆ­áˆ¶á‰¥áŠ›áˆ áŠ«áˆˆá‰½áˆ… áŠ áŠ•á‰° áŠ¥áŠ•á‹´á‰µ áŠá‹ á‹áˆ¸á‰µáˆ½ áŠá‹,áˆáŠ•áˆ á‰¢áˆ‰áˆ… áˆ›áˆ˜áŠ• áŠ á‰µáˆáˆáŒáˆá¢ áŠ¥áˆ…á‰µáˆ… á‰°á‹°áŠáˆªá‹«áˆˆáˆ áŠ«áˆˆá‰½áˆ… áŠ áŠ•á‰° áˆ›áŠ• áŠáˆ… áŠ á‹­ á‹áˆ¸á‰µáˆ½ áŠá‹\n",
      "272,es,\"Una paguita para ese hombre, que se ve con ganas de integrarse y aceptar nuestras reglas/costumbres.\",\"SerÃ­a apropiado proporcionar alguna ayuda econÃ³mica a ese hombre, parece estar dispuesto a integrarse y aceptar nuestras reglas y costumbres.\",<extra_id_0> que se ve con ganas de integrarse y aceptar nuestras reglas/costumbres\n",
      "273,am,Dani áˆˆáŒá‹œá‹ á‰ áˆ…á‹­á‹ˆá‰µ áŠ áˆˆáŠ• áˆˆáˆ›áˆˆá‰µ áŠ¨á‰°á‹°á‰ á‰€á‰ á‰µ áˆáˆ½áŒ áá‰¶á‹áŠ• áˆáŠ³áˆ::,áŠ áˆˆáŠ• á‹¨áˆšáˆ áˆ˜áˆáŠ¥áŠ­á‰µ áˆˆáˆ›áˆµá‰°áˆ‹áˆˆá á‰³áˆµá‰¦ á‹­áˆ˜áˆµáˆ‹áˆá¤ áá‰¶ á‰°áˆáŠ³áˆá¡á¡,áˆ…á‹­á‹ˆá‰µ áŠ áˆˆáŠ• áˆˆáˆ›áˆˆá‰µ áŠ¨á‰°á‹°á‰ á‰€á‰ á‰µ áˆáˆ½áŒ áá‰¶á‹áŠ• áˆáŠ³áˆ:\n",
      "274,uk,\"ĞĞ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñƒ Ğ·Ñ€Ğ¾Ğ±Ğ¸Ğ²ÑˆĞ¸ Ñ…ÑƒĞ¹Ğ½Ñ, Ñ‚Ğ¸ Ñ€Ğ°Ğ·-Ñƒ-Ñ€Ğ°Ğ· Ğ´ÑƒĞ¼Ğ°Ñ”Ñˆ Ğ½Ğ°Ğ´ Ñ‡ĞµÑ€Ğ³Ğ¾Ğ²Ğ¾Ñ.\",\"ĞĞ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñƒ Ğ·Ñ€Ğ¾Ğ±Ğ¸Ğ²ÑˆĞ¸ Ñ„iĞ³Ğ½Ñ, Ñ‚Ğ¸ Ñ€Ğ°Ğ·-Ñƒ-Ñ€Ğ°Ğ· Ğ´ÑƒĞ¼Ğ°Ñ”Ñˆ Ğ½Ğ°Ğ´ Ñ‡ĞµÑ€Ğ³Ğ¾Ğ²Ğ¾Ñ.\",\"<extra_id_0> Ğ·Ñ€Ğ¾Ğ±Ğ¸Ğ²ÑˆĞ¸ Ñ…ÑƒĞ¹Ğ½Ñ, Ñ‚Ğ¸ Ñ€Ğ°Ğ·-Ñƒ-Ñ€Ğ°Ğ· Ğ´ÑƒĞ¼Ğ°Ñ”Ñˆ Ğ½Ğ°Ğ´ Ñ‡ĞµÑ€Ğ³Ğ¾Ğ²Ğ¾Ñ\"\n",
      "275,es,\"Gobierno de mierda\"\"\",Gobierno ineficiente,<extra_id_0> Gobierno de mierda\n",
      "276,ar,ÙˆÙÙŠ Ø³ÙŠØ§Ù‚Ù Ø¢Ø®Ø± Ø§Ù„ØµØ¹Ø§ÙŠØ¯Ø© Ø§Ø­Ù„Ù‰ ÙˆØ§Ø­Ø³Ù† Ù†Ø§Ø³ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…,ÙˆÙÙŠ Ø³ÙŠØ§Ù‚Ù Ø¢Ø®Ø± Ø§Ù„Ù…ØµØ±ÙŠÙŠÙ† Ø§Ø­Ù„Ù‰ ÙˆØ§Ø­Ø³Ù† Ù†Ø§Ø³ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…,<extra_id_0> Ø³ÙŠØ§Ù‚Ù Ø¢Ø®Ø± Ø§Ù„ØµØ¹Ø§ÙŠØ¯Ø© Ø§Ø­Ù„Ù‰ ÙˆØ§Ø­Ø³Ù† Ù†Ø§Ø³ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…\n",
      "277,am,áŠ®á‰°á‰³áˆ áŠ áˆáŠ• áŠ á‰° áˆ°á‹‰ áŠáˆ… áŒ­á‰…áˆ‹á‰µáˆ… áˆ†á‹µáˆ… áŠá‹‰ áŠ¤áˆ­áˆšá‹«áˆµáŠ• áˆˆáˆ˜á‰°á‰¸á‰µ áˆáˆ«áˆ á‹¨áˆˆáˆ…áˆ á‹¨áŠ á‰ á‰  áŒˆáˆ‹á‹‰ á‹¨á‹‰áˆ¸á‰µ áˆ›áˆµáˆ¨áŒƒ áˆ›áŠ• á‹­áˆ°áˆ›á‹‹áˆ,áŠ áŠ•á‰° áˆ«áˆµ á‹ˆá‹³á‹µ áŠáˆ…á¤ áŠ¤áˆ­áˆá‹«áˆµáŠ• á‹¨áˆšá‹«áˆµá‰°á‰½áˆ… áŠáŒˆáˆ­ á‹¨áˆˆáˆá¤ á‹¨áŠ á‰ á‰  áŒˆáˆ‹á‹ á‹«áˆá‰°áˆ¨áŒ‹áŒˆáŒ  áˆ›áˆµáˆ¨áŒƒ áˆ˜áŠ• á‹­á‰€á‰ áˆˆá‹‹áˆ,áŒ­á‰…áˆ‹á‰µáˆ… áˆ†á‹µáˆ… áŠá‹‰ áŠ¤áˆ­áˆšá‹«áˆµáŠ• áˆˆáˆ˜á‰°á‰¸á‰µ áˆáˆ«áˆ á‹¨áˆˆáˆ…áˆ á‹¨áŠ á‰ á‰  áŒˆáˆ‹á‹‰ á‹¨á‹‰áˆ¸á‰µ áˆ›áˆµáˆ¨áŒƒ áˆ›áŠ• á‹­áˆ°áˆ›á‹‹áˆ\n",
      "278,ru,ÑÑ‚Ñƒ ÑĞ²Ğ¾Ğ»Ğ¾Ñ‡ÑŒ Ğ½Ğ°Ğ´Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ‚ÑŒ!!!!!,ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğµ Ğ½Ğ°ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ!!!!!,<extra_id_0> Ğ½Ğ°Ğ´Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ‚ÑŒ!!!!!\n",
      "279,hi,à¤ªà¤¿à¤›à¤µà¤¾à¤¡à¤¼à¤¾ à¤šà¤¾à¤Ÿà¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤•à¥‹ <number> à¤²à¤¾à¤– à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ à¤­à¤¡à¤¼à¤µà¤¾ |,à¤šà¤¾à¤ªà¤²à¥‚à¤¸à¥€ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤•à¥‹ <number> à¤²à¤¾à¤– à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ à¤¯à¤¹ à¤­à¥à¤°à¤·à¥à¤Ÿ à¤†à¤¦à¤®à¥€ |,à¤¬à¤¿à¤›à¤µà¤¾à¤¡à¤¼à¤¾ à¤šà¤¾à¤Ÿà¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤•à¥‹ <number> à¤²à¤¾à¤– à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ à¤­à¤¡à¤¼à¤µà¤¾ |\n",
      "280,es,\"A su vez que no es lo mismo ser cristiano que catolico , pero tu eres CATETA.\",A su vez que no es lo mismo ser cristiano que catolico.,\"<extra_id_0> A su vez que no es lo mismo ser cristiano que catolico , pero tu eres CATETA.\"\n",
      "281,uk,ĞšÑ€Ğ°Ñ‰Ğµ Ñ‚Ğ°Ğº Ğ½Ñ–Ğ¶ Ñ‚Ğ²Ñ–Ñ‚Ğ¸Ñ‚Ğ¸ Ğ´Ğ¾Ñ…ÑƒÑ Ğ²ÑÑŒÑĞºĞ¾Ğ³Ğ¾ ÑˆĞ»Ğ°ĞºÑƒ,ĞšÑ€Ğ°Ñ‰Ğµ Ñ‚Ğ°Ğº Ğ½Ñ–Ğ¶ Ñ‚Ğ²Ñ–Ñ‚Ğ¸Ñ‚Ğ¸ Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾ Ğ²ÑÑĞºĞ¾Ñ— Ğ½ĞµĞ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾Ñ— Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ—,<extra_id_0> Ñ‚Ğ°Ğº Ğ½Ñ–Ğ¶ Ñ‚Ğ²Ñ–Ñ‚Ğ¸Ñ‚Ğ¸ Ğ´Ğ¾Ñ…ÑƒÑ Ğ²ÑÑŒÑĞºĞ¾Ğ³Ğ¾ ÑˆĞ»Ğ°ĞºÑƒ\n",
      "282,hi,à¤¹à¤® à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¤¾ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆ,à¤¹à¤® à¤¡à¤°à¤ªà¥‹à¤• à¤¹à¥ˆà¤‚ à¤¹à¤® à¤•à¥à¤¯à¤¾ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆ,à¤¹à¤® à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¤¾ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆ\n",
      "283,en,dont give us that crap about out of towners coming here and dropping bucketsey either .,dont give us that story about out of towners coming here and dropping bucketsey either,<extra_id_0> dont give us that crap about out of towners coming here and dropping bucketsey either.\n",
      "284,ar,Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¥Ù„Ø§ Ø¨ØªÙˆØ§ÙÙ‚ Ø¢Ø®Ø± Ø¹Ù„Ù‰ Ø±Ø£ÙŠ Ù…Ø§Øª Ù‚Ø¨Ù„ Ø£Ù† ÙŠÙˆÙ„Ø¯,ÙˆØ§Ù„Ø§Ø³ØªØ¹Ø§Ø¶Ø© Ø¹Ù† Ø°Ù„Ùƒ Ø¨ØªÙˆØ§ÙÙ‚ Ø¢Ø®Ø± ÙÙŠ Ø§Ù„Ø¢Ø±Ø§Ø¡.,<extra_id_0> Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¥Ù„Ø§ Ø¨ØªÙˆØ§ÙÙ‚ Ø¢Ø®Ø± Ø¹Ù„Ù‰ Ø±Ø£ÙŠ Ù…Ø§Øª Ù‚Ø¨Ù„ Ø£Ù†ÙˆÙ„Ø¯\n",
      "285,uk,\"Ğ¥ĞµĞ¿Ğ¿Ñ– Ğ±ÑŒĞ¾Ğ·Ğ´ĞµĞ¹, Ñ‰Ğ¾ÑÑŒ Ğ½Ñ–Ñ…ÑƒÑ Ğ½Ğµ Ñ…ĞµĞ¿Ğ¿Ñ–.\",\"Ğ¥ĞµĞ¿Ğ¿Ñ– Ğ±ÑŒĞ¾Ğ·Ğ´ĞµĞ¹, Ñ‰Ğ¾ÑÑŒ Ğ·Ğ¾Ğ²ÑÑ–Ğ¼ Ğ½Ğµ Ñ…ĞµĞ¿Ğ¿Ñ–.\",\"Ğ¾Ğ½ĞµĞ·, Ñ‰Ğ¾ÑÑŒ Ğ½Ñ–Ñ…ÑƒÑ Ğ½Ğµ Ñ…ĞµĞ¿Ğ¿Ñ–.\"\n",
      "286,en,\"and go on saddam execution talk page, before you stink up the place.\",\"And please go on Saddam execution talk page, before you do anything else.\",\"saddam execution talk page, before you stink up the place.\"\n",
      "287,zh,å—¯å—¯å—¯ï¼Œæ˜¯ä¸æ˜¯æ”¯æŒåŒæ€§æ‹å°±ä¸ç‹­éš˜äº†ï¼Ÿæ˜¯ä¸æ˜¯ä¸è§‰å¾—åŒæ€§æ‹æ¶å¿ƒå°±ä¸ç‹­éš˜äº†ï¼Ÿ,å—¯å—¯å—¯ï¼Œæ˜¯ä¸æ˜¯æ”¯æŒåŒæ€§æ‹å°±ä¸ç‹­éš˜äº†ï¼Ÿæ˜¯ä¸æ˜¯ä¸è§‰å¾—åŒæ€§æ‹è®©äººä¸èˆ’æœå°±ä¸ç‹­éš˜äº†ï¼Ÿ,å…³äºåŒæ€§æ‹å°±ä¸ç‹­éš˜äº†?æ˜¯ä¸æ˜¯ä¸è§‰å¾—åŒæ€§æ‹æ¶å¿ƒå°±ä¸ç‹­éš˜äº†?æ˜¯ä¸æ˜¯ä¸è§‰å¾—åŒæ€§æ‹æ¶å¿ƒå°±ä¸ç‹­éš˜äº†?æ˜¯ä¸æ˜¯ä¸è§‰å¾—åŒæ€§æ‹æ¶å¿ƒå°±ä¸ç‹­éš˜äº†?\n",
      "288,zh,é•¿å¾—å’Œä¸ªè€é¼ ä¼¼çš„,é•¿å¾—ä¸ç¬¦åˆæˆ‘çš„å®¡ç¾,é•¿å¾—å’Œä¸ªè€é¼ ä¼¼çš„\n",
      "289,es,imagina y todo fue por entrar con una maleta en un museo. Putos nazis,imagina y todo fue por entrar con una maleta en un museo.,y todo fue por entrar con una maleta en un museo\n",
      "290,en,\"careful , you just described the goonies , and that shit was classic .\",\"careful , you just described the goonies , and that was classic\",\"<extra_id_0> you just described the goonies , and that shit was classic .\"\n",
      "291,uk,\"ĞÑ‡ĞµĞ²Ğ¸Ğ´Ğ½Ğ¾, Ğ±Ğ¾Ğ³ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ñ‚ÑƒĞ²Ğ°Ğ², Ñ‰Ğ¾Ğ± Ğ¿Ğ¸Ğ·Ğ´Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ, Ğ±Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ.\",\"ĞÑ‡ĞµĞ²Ğ¸Ğ´Ğ½Ğ¾, Ğ±Ğ¾Ğ³ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ñ‚ÑƒĞ²Ğ°Ğ², Ñ‰Ğ¾Ğ± Ğ¿Ğ¾ĞºĞ°Ñ€Ğ°Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ, Ğ±Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ.\",\"<extra_id_0>, Ğ±Ğ¾Ğ³ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ñ‚ÑƒĞ²Ğ°Ğ², Ñ‰Ğ¾Ğ± Ğ¿Ğ¸Ğ·Ğ´Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ, Ğ±Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ\"\n",
      "292,de,Verhaftungen von Vergewaltigern als Erfolg zu verkaufen ist das Aufmerksamkeitswinseln einer armseligen BehÃ¶rde. #Polizei #KÃ¶ln #Rapefugees,Verhaftungen von Vergewaltigern als Erfolg zu verkaufen ist das Aufmerksamkeitswinseln einer armseligen BehÃ¶rde. #Polizei #KÃ¶ln,<extra_id_0> Verhaftungen von Vergewaltigern als Erfolg zu verkaufen ist das Aufmerksamkeitswinseln einer armseligen BehÃ¶rde #Polizei #KÃ¶ln #Rapefugees\n",
      "293,zh,å¥³æ€§å°±æ˜¯çˆ±æ¯”ç”·æ€§åŒæ ‡ï¼Œæˆ‘è§‰å¾—è¿™å¥è¯ç¡®å®æ²¡é—®é¢˜,å¥³æ€§å°±æ˜¯çˆ±æ¯”ç”·æ€§ä»¥åŒé‡æ ‡å‡†çœ‹å¾…äº‹ç‰©ï¼Œæˆ‘ä¸ªäººè§‰å¾—è¿™å¥è¯ç¡®å®æ²¡é—®é¢˜,\"<extra_id_0>å°±æ˜¯çˆ±æ¯”ç”·æ€§åŒæ ‡,æˆ‘è§‰å¾—è¿™å¥è¯ç¡®å®æ²¡é—®é¢˜\"\n",
      "294,ru,\"Ğ¼ĞµĞ½Ñ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ… - Ğ½Ñƒ Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ…,ĞºĞ°Ñ€Ğ¼Ğ¾Ğ´Ñ€Ğ¾Ñ‡ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ²ÑÑĞ´Ñƒ\",\"Ğ¼ĞµĞ½Ñ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ… - Ğ½Ñƒ Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ…, Ğ´Ğ¾ĞºĞ¾Ğ¿Ğ°Ğ»Ğ¸ÑÑŒ Ğ¿Ğ¾Ğ²ÑÑĞ´Ñƒ\",\"Ğ½Ñƒ Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ…,ĞºĞ°Ñ€Ğ¼Ğ¾Ğ´Ñ€Ğ¾Ñ‡ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ²ÑÑĞ´Ñƒ\"\n",
      "295,am,Ethiopia: áŠ á‹­ á‹¨áŒˆáˆá‰± áŠáŒˆáˆ­á¢ á‰ á‰ƒ áˆ°áˆ‹áˆáŠ• á‰ áŒ á‰¥áˆ˜áŠ•áŒƒ áˆá‰³áˆ˜áŒ£áˆáŠ• áŠá‹‹á¢ áŠ•áŒáŒáˆ­áŠ“ á‹µáˆ­á‹µáˆ­ áŠ¥áŠ•á‹°áŒ­áˆ«á‰… á‰ áˆ…áˆáˆáˆ… áŠ¥á‹«á‰£áˆ¨áˆ¨ áŠá‹‰ áˆ˜á‰½áˆá¢ á‰ á‹ˆá‰³á‹°áˆ­ á‰¥á‹›á‰µ á‰¢áˆ†áŠ• á‹°áˆ­áŒ áŠ¥áˆµáŠ¨á‹›áˆ¬ á‹­,Ethiopia:  á‰ á‰ƒ áˆ°áˆ‹áˆáŠ• á‰ áŒ á‰¥áˆ˜áŠ•áŒƒ áˆá‰³áˆ˜áŒ£áˆáŠ• áŠ á‰µá‰½áˆáˆá¢ áŠ•áŒáŒáˆ­áŠ“ á‹µáˆ­á‹µáˆ­ á‰¥á‰» áŠá‹ áˆ°áˆˆáˆáŠ• á‹¨áˆšá‹«áˆµáŒˆáŠá¢ á‰ á‹ˆá‰³á‹°áˆ­ á‰¥á‹›á‰µ á‰¢áˆ†áŠ• á‹°áˆ­áŒ áŠ¥áˆµáŠ¨á‹›áˆ¬ á‹­áŠ–áˆ­ áŠá‰ áˆ­,Ethiopia: áŠ á‹­ á‹¨áŒˆáˆá‰± áŠáŒˆáˆ­á¢ á‰ á‰ƒ áˆ°áˆ‹áˆáŠ• á‰ áŒ á‰¥áˆ˜áŠ•áŒƒ áˆá‰³áˆ˜áŒ£áˆáŠ• áŠá‹‹á¢ á‰ á‰ƒ áˆ°áˆ‹áˆáŠ• á‰ áŒ á‰¥áˆ˜áŠ•áŒƒ áˆá‰³áˆ˜áŒ£áˆáŠ• áŠá‹‹á¢ á‰ á‰ƒ áˆ°áˆ‹áˆáŠ• á‰ áŒ á‰¥áˆ˜áŠ•áŒƒ áˆá‰³áˆ˜áŒ£áˆáŠ• áŠá‹‹á¢ á‰ á‰ƒ áˆ°áˆ‹áˆáŠ• á‰ áŒ á‰¥áˆ˜áŠ•áŒƒ áˆá‰³áˆ˜áŒ£áˆáŠ• áŠá‹‹á¢\n",
      "296,hi,à¤‰à¤«à¥à¤«!   à¤¬à¤¡à¤¼à¥€ à¤µà¤¿à¤¡à¤‚à¤¬à¤¨à¤¾ à¤¹à¥ˆ à¤­à¥‹à¤¸à¤¡à¤¼à¥€à¤•à¥‡...! ,à¤‰à¤«à¥à¤«! à¤¬à¤¡à¤¼à¥€ à¤µà¤¿à¤¡à¤‚à¤¬à¤¨à¤¾ à¤¹à¥ˆ...!,à¤¬à¤¿à¤¡à¤‚à¤¬à¤¨à¤¾ à¤¹à¥ˆ à¤­à¥‹à¤¸à¤¡à¤¼à¥€...!\n",
      "297,am,áˆ‚á‹µáŠ“ áˆµáŒˆá‹µáˆˆá‰µ áˆˆá‹š á‹áˆ» áˆ°á‰£áŠª áˆ˜áˆª áˆ˜á‰£áˆ áˆ«áˆ± áˆ›á‹­áŒˆá‰£á‹ á‰ áˆ½á‰°áŠ› áˆ«áˆµ á‹ˆá‹³á‹µ áˆ˜áŒ¥á áˆ°á‹á¢ á‹¨áˆ± á‹áˆ¾á‰½ áŠ á‰¥áˆ«á‰¹á‰µ á‰µá‰€á‰ áˆ«áˆ‹á‰¹ á‹ˆáˆ«á‹¶á‰½,áˆ„á‹°áˆ… á‰ áŠ¥áˆáŠá‰µáˆ… áŠ¥áŒ… áˆá‰µáŠáˆ³á‹ á‰µá‰½áˆ‹áˆˆáˆ…á¡á¡ áˆˆáŠ¥áŠ” áŒáŠ• áŒ¥áˆ© áˆ˜áˆª áŠá‹ á‰¥á‹¬ áŠ áˆ‹áˆµá‰¥áˆá¡á¡,<extra_id_0> á‹¨áˆ± á‹áˆ¾á‰½ áŠ á‰¥áˆ«á‰¹á‰µ á‰µá‰€á‰ áˆ«áˆ‹á‰¹ á‹ˆáˆ«á‹¶á‰½\n",
      "298,es,\"AsÃ­ actÃºa la propaganda de la izquierda, como cualquier rancia dictadura\",\"AsÃ­ actÃºa la propaganda de la izquierda, como cualquier dictadura\",\"<extra_id_0> actÃºa la propaganda de la derecha, como cualquier rancia dictadura\"\n",
      "299,ru,\"Ğ¸ Ğ½Ğ°Ğ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ² ÑĞ´Ğ°ĞºÑƒÑ Ñ…ĞµÑ€Ğ½Ñ, ĞµÑ‰Ñ‘ Ğ¸ Ğ³Ğ¾Ñ€Ğ´Ñ‹Ğ¹ Ñ…Ğ¾Ğ´Ğ¸Ñ‚ , ĞºĞ°Ğº Ğ±Ğ°Ğ¼Ğ±ÑƒĞº.\",\"Ğ˜ Ğ½Ğ°Ğ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ² Ñ‚Ğ°ĞºÑƒÑ ĞµÑ€ÑƒĞ½Ğ´Ñƒ, ĞµÑ‰Ñ‘ Ğ¸ Ğ³Ğ¾Ñ€Ğ´Ñ‹Ğ¹ Ñ…Ğ¾Ğ´Ğ¸Ñ‚\",\"<extra_id_0>, Ğ½Ğ°Ğ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ² ÑĞ´Ğ°ĞºÑƒÑ Ñ…ĞµÑ€Ğ½Ñ, ĞµÑ‰Ñ‘ Ğ¸ Ğ³Ğ¾Ñ€Ğ´Ñ‹Ğ¹ Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ¼Ğ±ÑƒĞº.\"\n",
      "300,ru,\"Ğ¿ÑĞ¸Ğ½Ğ° ĞµĞ±ÑƒÑ‡Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ÑĞºĞ°Ğ¶ÑƒÑ‚, Ñ‚Ğ¾ Ğ¸ Ğ¿Ğ¸Ğ·Ğ´Ğ¸Ñ‚ Ğ² ÑÑ„Ğ¸Ñ€Ğµ Ğ½Ğ° ÑĞ²Ğ¾Ñ‘Ğ¼ ĞµĞ±ÑƒÑ‡ĞµĞ¼ Ñ‚ĞµĞ»ĞµĞºĞ°Ğ½Ğ°Ğ»Ğµ\",Ğ§Ñ‚Ğ¾ ÑĞºĞ°Ğ¶ÑƒÑ‚ Ñ‚Ğ¾ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ Ğ² ÑĞ²Ğ¾ĞµĞ¼ ÑÑ„Ğ¸Ñ€Ğµ,\"<extra_id_0>Ñ‡Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ÑĞºĞ°Ğ¶ÑƒÑ‚, Ñ‚Ğ¾ Ğ¸ Ğ¿Ğ¸Ğ·Ğ´Ğ¸Ñ‚ Ğ² ÑÑ„Ğ¸Ñ€Ğµ Ğ½Ğ° ÑĞ²Ğ¾Ñ‘Ğ¼ ĞµĞ±ÑƒÑ‡ĞµĞ¼ Ñ‚ĞµĞ»ĞµĞºĞ°Ğ½Ğ°Ğ»Ğµ\"\n",
      "301,ru,\"Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€ĞºĞ¸, Ğ·Ğ°Ğ²Ğ¸Ğ´ÑƒĞ¹Ñ‚Ğµ Ğ¼Ğ¾Ğ»Ñ‡Ğ°, Ğ° Ñ‚Ğ¾ Ğ¿Ğ¸ÑˆĞµÑ‚Ğµ Ğ²ÑÑĞºÑƒÑ Ğ³Ğ°Ğ´Ğ¾ÑÑ‚ÑŒ\",\"Ğ—Ğ°Ğ²Ğ¸Ğ´ÑƒĞ¹Ñ‚Ğµ Ğ¼Ğ¾Ğ»Ñ‡Ğ°, Ğ° Ñ‚Ğ¾ Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ½ĞµĞ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğµ Ğ²ĞµÑ‰Ğ¸\",\"<extra_id_0>, Ğ·Ğ°Ğ²Ğ¸Ğ´ÑƒĞ¹Ñ‚Ğµ Ğ¼Ğ¾Ğ»Ñ‡Ğ°, Ğ° Ñ‚Ğ¾ Ğ¿Ğ¸ÑˆĞµÑ‚Ğµ Ğ²ÑÑĞºÑƒÑ Ğ³Ğ°Ğ´Ğ¾ÑÑ‚ÑŒ\"\n",
      "302,ar,Ø£Ø¬Ù„ØŒ Ù„ÙƒÙ† Ù‡Ø°Ø§ Ø§Ù„Ø¶Ø±Ø§Ø· Ø¬ÙˆÙ† Ø£Ø°ÙƒÙ‰ Ù…Ù†Ùƒ Ø¨ÙƒØ«ÙŠØ±,Ø£Ø¬Ù„ØŒ Ù„ÙƒÙ† Ø¬ÙˆÙ† Ø£Ø°ÙƒÙ‰ Ù…Ù†Ùƒ Ø¨ÙƒØ«ÙŠØ±,Ù„ÙƒÙ† Ù‡Ø°Ø§ Ø§Ù„Ø¶Ø±Ø§Ø· Ø¬ÙˆÙ† Ø£Ø°ÙƒÙ‰ Ù…Ù†Ùƒ Ø¨ÙƒØ«ÙŠØ±\n",
      "303,es,nonono este 2020 se anda pasando de verga,No no no estÃ© 2020 se anda pasando,<extra_id_0> nonono este anda pasando verga\n",
      "304,am,áˆ˜áŠ¨áˆ‹áŠ¨á‹« á‰°áŠ¨á‰¦ áˆ›áŠ• áŠá‰ áˆ­ á‹¨á‹°áˆ¨áˆ°áˆ‹á‰µ áŠ áˆáŠ•áˆµ áŠ¨áŠá‰µ áˆ†áŠ– áŠ¥á‹¨áˆá‰°áŠ“ áŠ¥á‹¨á‰°ááˆˆáˆ˜ á‹«áˆˆá‹ áˆ›áŠá‹ áŠ¨áˆ€á‹²á‹ áŠ áˆˆá‰€á‰ƒáˆ… áŠ¥á‰¥á‹­ áŠ áˆ…áˆ˜á‹µ áˆµáˆ™áŠ• á‰ áŠ á‹°á‰£á‰£á‹­ áŒ¥áˆ­á‰¶ áŠ áˆáˆµáŒáŠ–á‰µ áŠá‰ áˆ­,áˆ˜áŠ¨áˆ‹áŠ¨á‹« á‰°áŠ¨á‰¦ áˆ›áŠ• áŠá‰ áˆ­ á‹¨á‹°áˆ¨áˆ°áˆ‹á‰µ áŠ áˆáŠ•áˆµ áŠ¨áŠá‰µ áˆ†áŠ– áŠ¥á‹¨áˆá‰°áŠ“ áŠ¥á‹¨á‰°ááˆˆáˆ˜ á‹«áˆˆá‹ áˆ›áŠá‹ á‰ á‹ˆá‰…á‰± á‹¨á‰ áˆ‹á‹­ áŠ áˆ˜áˆ«áˆ®á‰½áˆ á‹­áˆ…áŠ• áŠ áˆ˜áˆ°áŒáŠá‹‹áˆ,áˆ°áˆ™áŠ• á‰ áŠ á‹°á‰£á‰£á‹­ áŒ¥áˆ­á‰¶ áŠ áˆáˆµáŒáŠ–á‰µ áŠá‰ áˆ­\n",
      "305,am,áˆˆáˆáŠ• áˆ„á‹°áˆ½ áŠ á‰µáˆáˆáŒáˆ áŠ áŠ•á‰º? á‹¨360 áŠ áˆ­á‰ áŠá‰½ áŠ á‰£áˆ áŠ á‹­á‹°áˆˆáˆ½áˆ,áˆˆáˆáŠ• áˆ„á‹°áˆ½ áŠ á‰µáˆáˆáŒŠáˆ áŠ áŠ•á‰º? á‹¨360 á‹¨áˆµáˆ« á‰£áˆá‹°áˆ¨á‰£ áŠ á‹­á‹°áˆˆáˆ½,áˆ„á‹°áˆ½ áŠ á‰µáˆáˆáŒáˆ áŠ áŠ•á‰º? á‹¨360 áŠ áˆ­á‰ áŠá‰½ áŠ á‰£áˆ áŠ á‹­á‹°áˆˆáˆ½áˆ\n",
      "306,am,áŠ á‰¥á‹­ áŠ áˆ…áˆ˜á‹µ á‰ áˆ³áˆáŠ• á‹áˆ»á‹ á‰°áˆ˜áˆµáŒˆáŠ• áŒ¥áˆ©áŠáˆ… á‰ áŠ©áˆ áˆáˆµáŠªáŠ• á‹¨áŒáŠ•á‹°áˆ­ áŒˆá‰ áˆ¬á‹á‰½áŠ• á‰ áˆµáˆœá‰µ áŠ¥á‹¨áŒˆá‹ áŠ¥áŠ“ á‰ áŠƒá‹­áˆ áŠ¥á‹«áˆµáŒˆá‹°á‹° : á‹˜áˆ˜áŠ“á‹Š áŠ¥áŠ“ áŠ¨á‰£á‹µ,áŠ á‰¥á‹­ áŠ áˆ…áˆ˜á‹µ á‰ á‰°áˆ˜áˆµáŒˆáŠ• áŒ¥áˆ©áŠáˆ… á‰ áŠ©áˆ á‹¨áŒáŠ•á‹°áˆ­ áŒˆá‰ áˆ¬á‹á‰½ á‰°áŒ½áŠ– áŠ¥á‹­á‹°áˆ¨áŒˆá‰ á‰¸á‹ áŠá‹,á‹ˆáˆ»á‹ á‰°áˆ˜áˆµáŒˆáŠ• áŒ¥áˆ©áŠáˆ… á‰ áŠ©áˆ áˆáˆµáŠªáŠ• á‹¨áŒáŠ•á‹°áˆ­ áŒˆá‰ áˆ¬á‹á‰½áŠ• á‰ áˆµáˆœá‰µ áŠ¥á‹¨áŒˆá‹ áŠ¥áŠ“ á‰ áŠƒá‹­áˆ áŠ¥á‹«áˆµáŒˆá‹°á‹° : á‹˜áˆ˜áŠ“á‹Š áŠ¥áŠ“ áŠ¨á‰£á‰¥\n",
      "307,de,Da macht sich die Bildung aus der Bild mal wieder bemerkbar. So ein Sau dummer Kommentar,Da macht sich die Bildung aus der Bild mal wieder bemerkbar. Ein sehr unÃ¼berlegter Beitrag,<extra_id_0> macht sich die Bildung aus der Bild wieder bemerkbar\n",
      "308,am,áˆ›áˆáˆªá‹« áŠáˆ… áŠ¨áˆáŒá‰£áˆ«á‰¸ áˆµáˆˆáˆšáŠ“áŒˆáˆ­ áŠá‹‹ áŠ¥áŠ•á‹³áŠ•á‰° áŠ á‹­áŠá‰µ áˆ°á‹á‰½áŠ• áˆ˜á…áˆ€á‰áˆ áˆµáˆˆáˆšáˆ 4.,áˆáŒá‰£áˆ«á‰¸á‹ áˆµáˆˆáˆšáŠ“áŒˆáˆ­ áŠá‹‹ áˆµáˆˆ áŠ¥áŠ“áŠ•á‰° á‹°áŒáˆ áˆ˜á…áˆ€á‰áˆ áˆµáˆˆáˆšáˆ 4.,áˆ°á‹á‰½ áˆ˜áˆáˆªá‹« áŠá‹‹ áŠ¥áŠ•á‹³áŠ•á‰° áŠ á‹­áŠá‰µ áŠáŒˆáˆ®á‰½ áˆµáˆˆáˆšáˆ\n",
      "309,uk,- Ğ¢Ğ¸ Ğ±ÑƒÑ…Ğ°Ñ”Ñˆ Ğ²Ğ¾Ğ´ÑÑ€Ñƒ Ñ– Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ”Ñˆ Ğ½Ğ° Ğ²ÑĞµ Ğ±Ğ¾Ğ»Ñ‚!,'- Ğ¢Ğ¸ Ğ¿''Ñ”Ñˆ Ğ³Ğ¾Ñ€Ñ–Ğ»ĞºÑƒ Ñ– Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ñ”Ñˆ Ğ½Ğ° Ğ²ÑĞµ Ñ€ĞµĞ°Ğ³ÑƒĞ²Ğ°Ñ‚Ğ¸',<extra_id_0> Ñ‚Ğ¸ Ğ±ÑƒÑ…Ğ°Ñ”Ñˆ Ğ²Ğ¾Ğ´ÑÑ€Ñƒ Ñ– Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ”Ñˆ Ğ½Ğ° Ğ²ÑĞµ Ğ±Ğ¾Ğ»Ñ‚!\n",
      "310,am,áŠ áŠ•á‰° áŒ“áˆ® áˆ˜áˆ†áŠ• áŠ áˆˆá‰ á‰µá¢ áŠ áŠ› á‹¨áˆ°á‹ áŠ áŠ•áˆáˆáŒáˆ á‹¨áˆ«áˆ³á‰½áŠ•áŠ•áˆ áŠ áŠ•áˆ°áŒ¥áˆ,áŠ¨áŠ áŠ•á‰° á‰ áŠ©áˆ áˆŠáˆ†áŠ• á‹­á‰½áˆ‹áˆá¢ áŠ áŠ› á‹¨áˆ°á‹ áŠ áŠ•áˆáˆáŒáˆ á‹¨áˆ«áˆ³á‰½áŠ•áŠ•áˆ áŠ áŠ•áˆ°áŒ¥áˆ,á‹ˆá‹« á‹¨áˆ°á‹ áŠ áŠ•áˆáˆáŒáˆ á‹¨áˆ«áˆ³á‰½áŠ•áŠ•áˆ áŠ áŠ•áˆ°áŒ¥áˆ\n",
      "311,ru,\"Ğ°ĞºÑ‚Ñ‘Ñ€ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ Ğ½Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğ¹,Ğ½Ğ¾ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº - Ñ…ÑƒĞ¹Ğ»Ğ¾ ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ.\",\"ĞĞºÑ‚ĞµÑ€ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ Ğ½Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğ¹,Ğ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ… ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº\",\"<extra_id_0> Ğ°ĞºÑ‚Ñ‘Ñ€ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ Ğ½ĞµĞ¿Ğ»Ğ¾Ñ…Ğ¾,Ğ½Ğ¾ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº - Ñ…ÑƒĞ¹Ğ»Ğ¾ ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ.\"\n",
      "312,en,\"I'm not going to reply to your ignorants comments, it's a zero!\",\"I'm not going to reply to your comments, it means nothing to me!\",\"<extra_id_0> I'm not going to reply to your comments, it's a zero!\"\n",
      "313,es,\"Pero que lo haga el gobierno y saquÃ© pasta, vamos gilipollas, que os podÃ©is forrar\",\"Que el gobierno lo gestione y obtenga beneficios, es decir, que aprovechen la oportunidad.\",\"<extra_id_0> que lo haga el gobierno y saquÃ© pasta, vamos gilipollas\"\n",
      "314,es,\"El homÃ³fobo #obispo de #Cordoba recomienda rezar mÃ¡s en vacaciones [y follar menos, claro].  vÃ­a\",El  #obispo de #Cordoba recomienda rezar mÃ¡s en vacaciones .  vÃ­a,\"<extra_id_0>, claro]. vÃ­a\"\n",
      "315,de,Die.linken kÃ¶nnen tun was sie wollen dieses pack,Die.linken kÃ¶nnen tun was sie wollen,linken kÃ¶nnen tun was sie wollen dieses pack\n",
      "316,ru,\"Ñ‚ĞµÑ€Ñ€Ğ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ² Ğ¸ÑÑ‚Ñ€ĞµĞ±Ğ»ÑÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ° ĞºĞ¾Ñ€Ñ€ÑƒĞ¿Ñ†Ğ¸Ñ Ğ½Ñƒ Ğ±Ğ»ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ°Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°!!! Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾, Ğ¿Ğ¸Ğ·Ğ´ĞµÑ†!!! ,,Ğ¼ÑƒĞ´Ğ°ĞºĞ¸ ,,\",\"Ğ¢ĞµÑ€Ñ€Ğ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ² Ğ¸ÑÑ‚Ñ€ĞµĞ±Ğ»ÑÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ° ĞºĞ¾Ñ€Ñ€ÑƒĞ¿Ñ†Ğ¸Ñ Ğ½Ñƒ Ñ‚Ğ°ĞºĞ°Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°!!!\",\"<extra_id_0> Ñ‚Ğ°ĞºĞ°Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°!!! Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾, Ğ¿Ğ¸Ğ·Ğ´ĞµÑ†!!! ,,Ğ¼ÑƒĞ´Ğ°ĞºĞ¸ ,,\"\n",
      "317,hi,à¤ªà¤¨à¥à¤¨à¥€ à¤«à¥à¤•à¥à¤•à¤¾ à¤¹à¥ˆ à¤•à¥à¤¯à¤¾ à¤¤à¤¬ à¤¸à¥‡ à¤•à¥à¤› à¤­à¥€ à¤¬à¥‹à¤²à¤¾ à¤œà¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤­à¥‹à¤¸à¤¡à¤¼à¥€ à¤•à¥‡ ,à¤•à¥à¤¯à¤¾ à¤–à¤¾à¤•à¤° à¤†à¤¯à¤¾ à¤¹à¥ˆ à¤¤à¤¬ à¤¸à¥‡ à¤•à¥à¤› à¤­à¥€ à¤¬à¥‹à¤²à¤¾ à¤œà¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆ,à¤ªà¤¨à¥à¤¨à¥€ à¤«à¥à¤•à¥à¤•à¤¾ à¤¹à¥ˆ à¤•à¥à¤¯à¤¾ à¤¤à¤¬ à¤¸à¥‡ à¤•à¥à¤› à¤­à¥€ à¤¬à¥‹à¤²à¤¾ à¤œà¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤­à¥‹à¤¸à¤¡à¤¼à¥€ à¤•à¥‡\n",
      "318,es,O sea que ahora la mafia tiene viajes de ida y vuelta,AsÃ­ que ahora la mafia ofrece viajes de ida y vuelta.,<extra_id_0> que ahora la mafia tiene viajes de ida y vuelta\n",
      "319,en,glad to see they were allowed to fuck up the santa barbara coast .,Glad to see they were alllowed to destroy the santa Barbara coast,<extra_id_0> glad to see they were allowed to fuck up the santa barbara coast .\n",
      "320,de,\"Genderisten mit ihren eigenen Waffen zu schlagen ist einfach und spaÃŸig. Sagt einfach, Ihr identifiziert Euch als schwarze lesbischer Transe\",\"Genderisten mit ihren eigenen Waffen zu schlagen ist einfach und spaÃŸig. Sagt einfach, Ihr identifiziert Euch als schwarze lesbischer Transperson\",\"<extra_id_0> ist einfach und spaÃŸig, Sagt einfach, Ihr identifiziert Euch als schwarze lesbischer Transe\"\n",
      "321,zh,ä¸­å›½ç”·äººå®¶æš´äº†é‚£å°±æ˜¯å› ä¸ºxdzæŠŠè‡ªå·±çš„è´Ÿèƒ½é‡å‘æ³„åœ¨é…å¶é‚£??,ä¸­å›½ç”·äººå®¶æš´äº†é‚£å°±æ˜¯å› ä¸ºæœ‰äº›ç”·æ€§æŠŠè‡ªå·±çš„è´Ÿèƒ½é‡å‘æ³„åœ¨é…å¶é‚£??,ä¸­å›½ç”·äººå®¶æš´äº†é‚£å°±æ˜¯å› ä¸ºxdzæŠŠè‡ªå·±çš„è´Ÿèƒ½é‡å‘æ³„åœ¨é…å¶é‚£??\n",
      "322,ar,Ù„ÙŠØ³ Ù‡Ø¬ÙˆÙ…Ù‹Ø§ Ø´Ø®ØµÙŠÙ‹Ø§ØŒ Ø£Ù†Øª Ù…Ø¹ØªÙˆÙ‡ Ø­Ø³Ø§Ø³.,Ù„ÙŠØ³ Ù‡Ø¬ÙˆÙ…Ù‹Ø§ Ø´Ø®ØµÙŠÙ‹Ø§ØŒ Ø£Ù†Øª Ø´Ø®Øµ Ø­Ø³Ø§Ø³.,<extra_id_0> Ù„ÙŠØ³ Ù‡Ø¬ÙˆÙ…Ù‹Ø§ Ø´Ø®ØµÙ‹Ø§ØŒ Ø£Ù†Øª Ù…Ø¹ØªÙˆÙ‡ Ø­Ø³Ø§Ø³.\n",
      "323,es,\"Hable como HernÃ¡ndez, ya sabemos que podes hablar mierda en ese canal, NARCO Â¿Seguridad?\",\"Hable como Hernandez, ya sabemos que podes hablar mal de ese canal. Â¿Seguridad?\",Rable como HernÃ¡ndez ya sabemos que pode hablar mierda en ese canal NARCO Â¿Seguridad?\n",
      "324,de,\"Da wird mal wieder stark Ãœbertrieben, es werden paar hundert wenn es hoch kommt teilnehmen!. NatÃ¼rlich wird die LÃ¼genpresse was anderes sagen\",\"Da wird mal wieder stark Ãœbertrieben, es werden paar hundert wenn es hoch kommt teilnehmen!. NatÃ¼rlich wird die Presse was anderes sagen\",\"<extra_id_0> wird mal wieder stark Ãœbertrieben, es werden paar hundert wenn es hoch kommt teilnehmen!.\"\n",
      "325,de,.@USER_0 Ich warte auf dem Moment das diese dummdeutschen das begreifen und diese VolksverrÃ¤terin zur Verantwortung zieht!!!,.@USER_0 Ich warte auf dem Moment das diese Deutschen das begreifen und diese Frau zur Verantwortung zieht!!!,.@USER_0 Ich warte auf dem Moment das diese dummdeutschen das begreifen und diese VolksverrÃ¤terin zur Verantwortung zieht!!!\n",
      "326,es,ya esta el subnormal de luis enrique jodiendome apuestas sin poner a suarez,ya esta luis enrique arruinandome apuestas sin poner a suarez,<extra_id_0> ya esta el subnormal de enrique jodiendome apuestas sin poner a suarez\n",
      "327,ar,ÙˆØ¬Ù…Ù‡ÙˆØ±Ù†Ø§ Ù…Ø¨Ø³ÙˆØ· ÙŠÙ‚ÙˆÙ„Ùƒ Ø§Ù„Ø¨Ø±ÙŠÙƒØ§Ù† Ù…Ùˆ Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ù‡ ÙŠØ³Ø¬Ù„ Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ğŸ˜­,ÙˆØ¬Ù…Ù‡ÙˆØ±Ù†Ø§ Ù…Ø¨Ø³ÙˆØ· ÙŠÙ‚ÙˆÙ„Ùƒ Ø§Ù„Ø¨Ø±ÙŠÙƒØ§Ù† Ù…Ùˆ Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ù‡ ÙŠØ³Ø¬Ù„ ğŸ˜­,<extra_id_0>Ùƒ Ø§Ù„Ø¨Ø±ÙŠÙƒØ§Ù† Ù…Ùˆ Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ù‡ ÙŠØ³Ø¬Ù„ Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ğŸ˜­\n",
      "328,en,You people are posting the WRONG results so its MY problem jackass,You people are posting the wrong results so its my problem.,<extra_id_0> you people are posting WRONG results\n",
      "329,hi,à¤•à¥‡à¤œà¤°à¥€à¤µà¤¾à¤² à¤¹à¤°à¤¾à¤®à¥€ à¤¸à¥à¤…à¤° à¤­à¤¡à¤¼à¤µà¤¾ à¤¹à¥ˆ | à¤à¤¸à¤¾ à¤®à¥‡à¤°à¥‡ à¤®à¥‹à¤¹à¤²à¥à¤²à¥‡ à¤µà¤¾à¤²à¥‡ à¤²à¥‹à¤— à¤•à¤¹ à¤°à¤¹à¥‡ à¤¥à¥‡ ||,à¤•à¥‡à¤œà¤°à¥€à¤µà¤¾à¤² à¤…à¤šà¥à¤›à¤¾ à¤†à¤¦à¤®à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ | à¤à¤¸à¤¾ à¤®à¥‡à¤°à¥‡ à¤®à¥‹à¤¹à¤²à¥à¤²à¥‡ à¤µà¤¾à¤²à¥‡ à¤²à¥‹à¤— à¤•à¤¹ à¤°à¤¹à¥‡ à¤¥à¥‡ ||,à¤¯à¤¹ à¤®à¥‡à¤°à¥‡ à¤®à¥‹à¤¹à¤²à¥à¤²à¥‡ à¤µà¤¾à¤²à¥‡ à¤²à¥‹à¤— à¤•à¤¹ à¤°à¤¹à¥‡ à¤¥à¥‡ ||\n",
      "330,hi,à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¥‡ à¤¹à¤°à¤¾à¤®à¤¿ à¤¤à¥à¤® à¤ªà¤¤à¥à¤°à¤•à¤¾à¤° à¤¹à¥‹,à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¥‡ à¤¬à¥à¤°à¥‡ à¤¤à¥à¤® à¤ªà¤¤à¥à¤°à¤•à¤¾à¤° à¤¹à¥‹,à¤¤à¥à¤® à¤ªà¤¤à¥à¤°à¤•à¤¾à¤° à¤¹à¥‹\n",
      "331,de,\"Bei #Halbneger meldet sich Till #Schweiger grosskotzig zu Wort, bei Mord an #Mila Valentin hÃ¶rt man nichts von diesen Heuchler.\",\"Bei bestimmten Themen meldet sich Till #Schweiger laut zu Wort, bei Mord an #Mila Valentin hÃ¶rt man nichts von ihm.\",Interviews bei #Halbneger meldet sich Till #Schweiger grosskotzig zu Wort bei Mord an Mila Valentin hÃ¶rt man nichts von diesen Menschen.\n",
      "332,es,\"Pero pedazo de subnormal, prefieres a Charlotte antes que Lana.\",\"Pero, prefieres a Charlotte antes que Lana.\",\"<extra_id_0> pedazo de subnormal, prefieres a Charlotte antes que Lana.\"\n",
      "333,zh,ç²¿ç±¹å‡Œè¿Ÿå°±å®Œäº‹,æˆ‘ä¸å–œæ¬¢çš„å›½å†…å¥³æ€§ä»˜å‡ºä»£ä»·å°±å®Œäº‹,ç²¿ç±¹å‡Œè¿Ÿå°±å®Œäº‹\n",
      "334,ar,Ù…Ø¹ Ø§Ù„Ø§Ø³Ù Ø­ØªÙ‰ ÙƒØªÙ„Ù†Ø§ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠÙ‡ Ù…Ø³ØªØ±Ø®ØµÙ‡ Ø¯Ù…Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠÙŠÙ† Ø­ØªÙ‰ Ù…Ø¹ Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ÙŠÙŠÙ† Ù…Ø³Ø§ÙˆÙ…Ø§Øª Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ…,Ù…Ø¹ Ø§Ù„Ø§Ø³Ù Ø­ØªÙ‰ ÙƒØªÙ„Ù†Ø§ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠÙ‡ Ù…Ø³ØªØ±Ø®ØµÙ‡ Ø¯Ù…Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠÙŠÙ† Ø­ØªÙ‰ Ù…Ø¹ Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ÙŠÙŠÙ† Ù…Ø³Ø§ÙˆÙ…Ø§Øª,Ø¨Ø­ØªÙ‰ ÙƒØªÙ„Ù†Ø§ Ù…Ø³ØªØ±Ø®ØµÙ‡ Ø¯Ù…Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠÙŠÙ† Ø­ØªÙ‰ Ù…Ø¹ Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ÙŠÙŠÙ† Ù…Ø³Ø§ÙˆÙ…Ø§Øª\n",
      "335,ru,\"Ğ²Ğ¾Ñ‚ ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹. Ğ²Ñ‹ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ¼ĞµÑÑ‚Ğµ Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€ĞºĞ¸,\",Ğ²Ğ¾Ñ‚ ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹. Ğ²Ñ‹ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ¼ĞµÑÑ‚Ğµ Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹,\"<extra_id_0> ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹. Ğ²Ñ‹ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ¼ĞµÑÑ‚Ğµ Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€ĞºĞ¸,\"\n",
      "336,ar,ÙƒÙ„ÙŠ Ø®Ø±Ø§ Ø´Ø±Ø§Ù…ÙŠØ· Ù…Ø§ Ø¨ÙŠØ³ØªØ±Ø¯ÙˆØ§ Ø´ÙŠ Ø­ØªÙ‰ Ø´Ø±ÙÙ‡Ù…,Ø§Ø³ÙƒØªÙŠ Ù…Ø§ Ø¨ÙŠØ³ØªØ±Ø¯ÙˆØ§ Ø´ÙŠ Ø­ØªÙ‰ Ø´Ø±ÙÙ‡Ù…,Ø´ÙØ§Ù…ÙŠØ· Ù…Ø§ Ø¨ÙŠØ³ØªØ±Ø¯ÙˆØ§ Ø´ÙŠ Ø­ØªÙ‰ Ø´Ø±ÙÙ‡Ù…\n",
      "337,zh,ä¸ä¼šçœŸçš„æœ‰äººå–œæ¬¢å¨˜ç‚®ç”·æ˜æ˜Ÿå§ï¼Œå–œæ¬¢è¿™äº›ç”·æ˜æ˜Ÿçš„ä¸éƒ½æ˜¯å¥³å­©å­å—,ä¸ä¼šçœŸçš„æœ‰äººå–œæ¬¢å¤–è¡¨æˆ–è€…è¡Œä¸ºæœ‰äº›å¥³æ€§åŒ–çš„ç”·æ˜æ˜Ÿå§ï¼Œå–œæ¬¢è¿™äº›ç”·æ˜æ˜Ÿçš„ä¸éƒ½æ˜¯å¥³å­©å­å—,\"ä¸ä¼šçœŸçš„æœ‰äººå–œæ¬¢å¨˜ç‚®ç”·æ˜æ˜Ÿå§,å–œæ¬¢è¿™äº›ç”·æ˜æ˜Ÿçš„ä¸éƒ½æ˜¯å¥³å­©å­å—\"\n",
      "338,hi,à¤®à¤¾à¤¦à¤°à¤œà¤¾à¤¤ à¤°à¤¾à¤œà¤¨à¥‡à¤¤à¤¾à¤“à¤‚ à¤”à¤° à¤­à¤¡à¤¼à¤µà¤¾ à¤ªà¤¤à¥à¤°à¤•à¤¾à¤°à¥‹à¤‚ à¤•à¥‡ à¤®à¥à¤‚à¤¹ à¤¸à¥‡ à¤‰à¤«à¥à¤« à¤¤à¤• à¤¨à¤¹à¥€à¤‚ à¤¨à¤¿à¤•à¤²à¤¤à¤¾,à¤°à¤¾à¤œà¤¨à¥‡à¤¤à¤¾à¤“à¤‚ à¤”à¤° à¤ªà¤¤à¥à¤°à¤•à¤¾à¤°à¥‹à¤‚ à¤•à¥‡ à¤®à¥à¤‚à¤¹ à¤¸à¥‡ à¤‰à¤«à¥à¤« à¤¤à¤• à¤¨à¤¹à¥€à¤‚ à¤¨à¤¿à¤•à¤²à¤¤à¤¾,à¤ªà¤¤à¥à¤°à¤•à¤¾à¤°à¥‹à¤‚ à¤•à¥‡ à¤®à¥à¤‚à¤¹ à¤¸à¥‡ à¤‰à¤«à¥à¤« à¤¤à¤• à¤¨à¤¹à¥€à¤‚ à¤¨à¤¿à¤•à¤²à¤¤à¤¾\n",
      "339,hi,à¤šà¤²à¤¾ à¤¹à¥ˆ à¤²à¥‹à¤¡à¤¾ à¤à¤•à¥à¤¸à¤ªà¤°à¥à¤Ÿ à¤¬à¤¨à¤¨à¥‡...,à¤šà¤²à¤¾ à¤¹à¥ˆ à¤à¤•à¥à¤¸à¤ªà¤°à¥à¤Ÿ à¤¬à¤¨à¤¨à¥‡...,à¤¬à¤¿à¤¨à¤¾ à¤¹à¥ˆ à¤²à¥‹à¤¡à¤¾ à¤à¤•à¥à¤¸à¤ª à¤¬à¤¨à¤¨à¥‡\n",
      "340,en,Women with masculine spirits are trash.,Women with masculine spirits are not my type.,<extra_id_0> women with masculine spirit are trash\n",
      "341,de,Auf die BewÃ¤hrung schei.......ss ich!,Die BewÃ¤hrung ist mir egal!,<extra_id_0> Auf die BewÃ¤hrung schei.......ss ich!\n",
      "342,zh,è¿˜æœ‰å¥½å¤šæ˜¯ç©çš„ã€‚æŠŠäººå¼„æ€€å­•äº†ï¼Œæä¸Šè£¤å­å°±è·‘è·¯ã€‚è¿™ç§å°±å«â€œååƒåœ¾â€,è¿˜æœ‰å¥½å¤šæ˜¯ç©çš„ã€‚æŠŠäººå¼„æ€€å­•äº†ï¼Œå°±ä¸è´Ÿè´£åœ°è·‘è·¯ã€‚è¿™ç§å°±å«â€œåäººå¤±è´¥è€…â€,\"<extra_id_0>å¥½å¤šæ˜¯ç©çš„ã€‚æŠŠäººå¼„æ€€å­•äº†,æä¸Šè£¤å­å°±è·‘è·¯ã€‚è¿™ç§å°±å«â€œååƒåœ¾â€\"\n",
      "343,zh,æœ‰å¾ˆå¤šç§åŸå› ï¼Œé»‘æ²³å—ã€é»‘åŒ—æ–¹çš„éƒ½æ˜¯å—æ»¡,æœ‰å¾ˆå¤šç§åŸå› ï¼Œæˆ‘è®¤ä¸ºé»‘æ²³å—ã€é»‘åŒ—æ–¹çš„éƒ½æ˜¯å—æ–¹äºº,\"æœ‰å¾ˆå¤šç§åŸå› ,é»‘æ²³å—ã€é»‘åŒ—æ–¹çš„éƒ½æ˜¯å—æ»¡\"\n",
      "344,hi,à¤¤à¥à¤®à¤•à¥‹ à¤•à¥à¤› à¤¬à¥‹à¤²à¤¨à¥‡ à¤•à¥€ à¤œà¤¼à¤°à¥‚à¤°à¤¤ à¤¨à¤¹à¥€ à¤¤à¥à¤® à¤¦à¤° à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤°à¤‚à¤¡à¥€ à¤•à¥€ à¤—à¤¾à¤‚à¤¡ à¤•à¥€ à¤¤à¤°à¤¹ à¤¬à¥‹à¤²à¤¤à¥‡ à¤¹à¥‹,à¤¤à¥à¤®à¤•à¥‹ à¤•à¥à¤› à¤¬à¥‹à¤²à¤¨à¥‡ à¤•à¥€ à¤œà¤°à¥‚à¤°à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¤à¥à¤® à¤•à¥à¤› à¤­à¥€ à¤¬à¥‡à¤•à¤¾à¤° à¤•à¥€ à¤¬à¤¾à¤¤ à¤¬à¥‹à¤²à¤¤à¥‡ à¤¹à¥‹ ,<extra_id_0> à¤¦à¤° à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤°à¤‚à¤¡à¥€ à¤•à¥€ à¤—à¤¾à¤‚à¤¡ à¤•à¥€ à¤¤à¤°à¤¹ à¤¬à¥‹à¤²à¤¤à¥‡ à¤¹à¥‹\n",
      "345,zh,å»é»‘éæ´²å°±è¡Œï¼Œé‚£é‡Œæœ‰é…‹é•¿,å»éæ´²å°±è¡Œï¼Œé‚£é‡Œæœ‰é…‹é•¿,\"å»é»‘éæ´²å°±è¡Œ,é‚£é‡Œæœ‰é…‹é•¿\"\n",
      "346,ar,Ù‡Ùˆ ÙƒØ¯Ø§ Ø¹Ø±Ù ÙŠØ±Ø¨ÙŠ ÙØ¹Ù„Ø§ Ø´Ø±Ø§Ù…ÙŠØ· Ø§Ø®Ø± Ø²Ù…Ù†,Ù‡Ùˆ ÙƒØ¯Ø§ Ø¹Ø±Ù ÙŠØ±Ø¨ÙŠ ÙØ¹Ù„Ø§ ÙØ§Ø´Ù„ÙŠÙ† Ø§Ø®Ø± Ø²Ù…Ù†,<extra_id_0> Ù‡Ùˆ ÙƒØ¯Ø§ Ø¹Ø±Ù ÙŠØ±Ø¨ÙŠ ÙØ¹Ù„Ø§ Ø´Ø±Ø§Ù…ÙŠØ· Ø¢Ø®Ø± Ø²Ù…Ù†\n",
      "347,es,\"Es su cultura y hay que respetarla. Un abrazo desde Ronda, MÃ¡laga.\",\"Es importante respetar su cultura. Un saludo desde Ronda, MÃ¡laga.\",\"<extra_id_0>, MÃ¡laga.\"\n",
      "348,ru,\"Ğ¡Ñ…ÑƒÑ ÑÑ‚Ğ¾ Ğ² ÑÑÑÑÑÑ€Ğµ Ğ±Ñ‹Ğ» ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼, Ğ´ĞµĞ±Ğ¸Ğ»ÑƒÑˆĞºĞ°? Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ¼Ğ¾Ñ€\",Ğ¡ Ñ‡ĞµĞ³Ğ¾ Ğ±Ñ‹ ÑÑ‚Ğ¾ Ğ² ÑÑÑÑÑÑ€Ğµ Ğ±Ñ‹Ğ» ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼? Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ¼Ğ¾Ñ€,\"<extra_id_0> ÑÑ‚Ğ¾ Ğ² ÑÑÑÑÑÑ€Ğµ Ğ±Ñ‹Ğ» ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼, Ğ´ĞµĞ±Ğ¸Ğ»ÑƒÑˆĞºĞ°? Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ¼Ğ¾Ñ€\"\n",
      "349,hi,à¤“à¤ à¤•à¤¾à¤¨à¤¿à¤¯à¥‡ à¤‡à¤¤à¤¨à¥‡ à¤œà¤²à¥à¤¦à¥€ à¤®à¥‡à¤‚ à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¥à¤¯à¥‹à¤‚ à¤•à¤° à¤¦à¥‡à¤¤à¥‡ à¤¹à¥‹? ,à¤“à¤ à¤‡à¤¤à¤¨à¥‡ à¤œà¤²à¥à¤¦à¥€ à¤®à¥‡à¤‚ à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¥à¤¯à¥‹à¤‚ à¤•à¤° à¤¦à¥‡à¤¤à¥‡ à¤¹à¥‹?,ç»ªà¤¨à¥‡ à¤œà¤²à¥à¤¦à¥€ à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¥à¤¯à¥‹à¤‚ à¤¦à¥‡à¤¤à¥‡ à¤¹à¥‹\n",
      "350,uk,\"Ğ† Ğ±Ğ»ÑÑ‚ÑŒ, Ğ½Ğ°Ğ²ĞºĞ¾Ğ»Ğ¾ ÑĞºÑ–ÑÑŒ Ğ´Ñ–Ñ‚Ğ¸ Ñ– Ğ²Ğ¾Ğ½Ğ¸ Ñ…Ğ¾Ñ‡Ñƒ Ğ±ÑƒÑ…Ğ°Ñ‚Ğ¸ Ñ– Ğ²Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¾ Ñ†Ğµ Ğ¿Ğ¸ÑˆÑƒÑ‚ÑŒ.\",ĞĞ°Ğ²ĞºĞ¾Ğ»Ğ¾ ÑĞºÑ–ÑÑŒ Ğ´Ñ–Ñ‚Ğ¸ Ñ– Ğ²Ğ¾Ğ½Ğ¸ Ñ…Ğ¾Ñ‡Ñƒ Ğ½Ğ°Ğ¿Ğ¸Ğ²Ğ°Ñ‚Ğ¸ÑÑ Ñ– Ğ²Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¾ Ñ†Ğµ Ğ¿Ğ¸ÑˆÑƒÑ‚ÑŒ.,<extra_id_0> Ğ²Ğ¾Ğ½Ğ¸ Ñ…Ğ¾Ñ‡ÑƒÑ‚ÑŒ Ğ±ÑƒÑ…Ğ°Ñ‚Ğ¸ Ñ– Ğ²Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¾ Ñ†Ğµ Ğ¿Ğ¸ÑˆÑƒÑ‚ÑŒ\n",
      "351,hi,\"à¤¸à¥‹à¤œà¤¾  à¤­à¥ˆà¤¨à¤šà¥‹à¤¦, à¤¹à¤° à¤•à¥‹à¤ˆ à¤¨à¤¯à¤¾ à¤¨à¤¯à¤¾ à¤šà¥à¤Ÿà¤•à¥à¤²à¤¾ à¤•à¤°à¤¤à¤¾ à¤°à¥‡à¤¹à¤¤à¤¾ à¤¹à¥‡\",\"à¤¸à¥‹à¤œà¤¾, à¤¹à¤° à¤•à¥‹à¤ˆ à¤¨à¤¯à¤¾ à¤¨à¤¯à¤¾ à¤šà¥à¤Ÿà¤•à¥à¤²à¤¾ à¤•à¤°à¤¤à¤¾ à¤°à¥‡à¤¹à¤¤à¤¾ à¤¹à¥‡\",\"à¤¨à¤¬à¥‹à¤¦, à¤¹à¤° à¤•à¥‹à¤ˆ à¤¨à¤¯à¤¾ à¤šà¥à¤Ÿà¤•à¥à¤²à¤¾ à¤•à¤°à¤¤à¤¾ à¤°à¥‡à¤¹à¤¤à¤¾ à¤¹à¥‡\"\n",
      "352,am,áŠ áŠ•á‰º áˆáŠá‹ áŠ¥áŠ•á‹²áˆ… á‰…áŒ¥áˆ áŠ áˆáˆ½ á‹¨áˆ°á‹ á‹µáˆáŒ½ áˆˆáˆ›áˆáŠ• á‰¥áˆˆáˆ½ áŠá‹ á‹°á‹°á‰¥ áˆ‚áŒ…á‹­ áŠ¥á‹› á‹¨áˆ…á‹ˆáˆ€á‰µ á‰¡á‰½áˆ‹á¢,áŠ áŠ•á‰º á‰ á‹šáˆ… á‹°áˆ¨áŒƒ áˆá‰µá‰ áˆ³áŒª áŠ á‹­áŒˆá‰£áˆ áˆ°á‹áˆ á‹µáˆááŠ• á‰ áŠáƒáŠá‰µ áˆ›áˆ°áˆ›á‰± á‰°áŒˆá‰¢ áŠá‹ á¡á¡,áˆáŠá‹ áŠ¥áŠ•á‹²áˆ… á‰…áŒ¥áˆ áŠ áˆáˆ½ á‹¨áˆ°á‹ á‹µáˆáŒ½ áˆˆáˆ›áˆáŠ• á‰¥áˆˆáˆ½ áŠá‹ á‹°á‹°á‰¥ áˆ‚áŒ…á‹­ áŠ¥á‹› á‹¨áˆ…á‹ˆáˆ€á‰µ á‰¡á‰½áˆ‹á¢\n",
      "353,en,\"holy shit yeah , forgot about that one .\",Just forget about that one,\"yeah , forgot about that one\"\n",
      "354,uk,Ğ¯ Ğ¼Ğ¾Ğ¶Ñƒ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ—Ğ±Ğ°Ğ½ÑƒÑ‚Ñ– Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¡Ğ¢Ğ‘.,Ğ¯ Ğ¼Ğ¾Ğ¶Ñƒ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ´Ğ¸Ğ²Ğ½Ñ– Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¡Ğ¢Ğ‘.,<extra_id_0> Ğ¼Ğ¾Ğ¶Ñƒ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ—Ğ±Ğ°Ğ½ÑƒÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¡Ğ¢Ğ‘.\n",
      "355,hi,à¤…à¤¬ à¤¤à¥‹ à¤­à¥‡à¤¨à¤šà¥‹à¤¦ à¤¬à¥‡à¤‡à¤œà¥à¤œà¤¤à¥€ à¤•à¥‹ à¤­à¥€ à¤¶à¤°à¥à¤® à¤† à¤°à¤¹à¥€ à¤¹à¥‹ à¤—à¤ˆ |,à¤…à¤¬ à¤¤à¥‹ à¤¬à¥‡à¤‡à¤œà¥à¤œà¤¤à¥€ à¤•à¥‹ à¤­à¥€ à¤¶à¤°à¥à¤® à¤† à¤°à¤¹à¥€ à¤¹à¥‹ à¤—à¤ˆ |,<extra_id_0> à¤¤à¥‹ à¤­à¥‡à¤¨à¤šà¥‹à¤¦ à¤¬à¥‡à¤‡à¤œà¥à¤œà¤¤à¥€ à¤•à¥‹ à¤­à¥€ à¤¶à¤°à¥à¤® à¤† à¤°à¤¹à¥€ à¤¹à¥‹ à¤—à¤ˆ |\n",
      "356,uk,\"'ĞĞ»Ğµ Ğ½Ğµ Ğ¼Ğ¾Ğ¶Ñƒ, Ğ±Ğ¾ Ñ‚Ğ¾ Ğ±Ğ»ÑÑ‚ÑŒ ÑÑ–Ğ¼''Ñ.'\",\"'ĞĞ»Ğµ Ğ½Ğµ Ğ¼Ğ¾Ğ¶Ñƒ, Ğ±Ğ¾ Ñ‚Ğ¾ ÑÑ–Ğ¼''Ñ.'\",\"''ĞĞ»Ğµ Ğ½Ğµ Ğ¼Ğ¾Ğ¶Ñƒ, Ğ±Ğ¾ Ñ‚Ğ¾ Ğ±Ğ»ÑÑ‚ÑŒ ÑÑ–Ğ¼''Ñ.'\"\n",
      "357,ar,Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ø§Ù„Ø£ØºØ¨ÙŠØ§Ø¡ Ù…Ø«Ù„ Ø¨ÙŠÙ„ÙˆØ³ÙŠ,Ø§Ù„Ù†Ø§Ø³ Ù…Ø«Ù„ Ø¨ÙŠÙ„ÙˆØ³ÙŠ Ù„ÙŠØ³ÙˆØ§ Ø¬ÙŠØ¯ÙŠÙ†,Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ø§Ù„Ø£ØºØ¨ÙŠØ§Ø¡ Ù…Ø«Ù„ Ø¨ÙŠÙ„ÙˆØ³ÙŠ\n",
      "358,zh,é‚£äº›äº©ç‹—ï¼Œçº¯çº¯ç•œç”Ÿï¼Œè¿™ä¸ªè¯ä½œä¸ºPUAç”·æ€§çš„ä¸œè¥¿ï¼Œè§£é‡Šæƒåªèƒ½åœ¨å°ä»™å¥³æ‰‹ä¸Šã€‚,é‚£äº›å¥³æ€§ï¼ŒçœŸè®©æˆ‘åæ„Ÿï¼Œè¿™ä¸ªè¯ä½œä¸ºå¯¹ç”·æ€§ç²¾ç¥æ§åˆ¶çš„ä¸œè¥¿ï¼Œè§£é‡Šæƒåªèƒ½åœ¨è‡ªæˆ‘æ„Ÿè§‰è¿‡äºè‰¯å¥½çš„å¥³æ€§æ‰‹ä¸Šã€‚,\"é‚£äº›äº©ç‹—,çº¯çº¯ç•œç”Ÿ,è¿™ä¸ªè¯ä½œä¸ºPUAç”·æ€§çš„ä¸œè¥¿,è§£é‡Šæƒåªèƒ½åœ¨å°ä»™å¥³æ‰‹ä¸Šã€‚\"\n",
      "359,en,And who is the motherfucker who nominated it for deletion?,And who is the person who nominated it for deletion?,<extra_id_0> who is a mother who nominated it for you\n",
      "360,uk,Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ñ€Ğ¾Ğ±Ğ»Ñ Ñ…ÑƒĞ¹Ğ¾Ğ²Ğ¸Ğ¹ Ğ° Ñ—Ğ¹ Ğ½Ñ€Ğ°,Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ñ€Ğ¾Ğ±Ğ»Ñ Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¸Ğ¹ Ğ° Ñ—Ğ¹ Ğ½Ñ€Ğ°,<extra_id_0> Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ñ€Ğ¾Ğ±Ğ»Ñ Ğ° Ñ—Ğ¹ Ğ½Ñ€Ğ°\n",
      "361,en,because its fucking reposted so much .,Because it has been reposted very often.,<extra_id_0> because its fucking reposted so much\n",
      "362,zh,ä¸å¬è€å¦ˆå®è¯æ•´å®¹ï¼Œä¿¡è·¯äººåŒ–å¦†ï¼Œå¹•åˆƒæ™ºå•†å¯ä»¥çš„,ä¸å¬è€å¦ˆå®è¯æ•´å®¹ï¼Œä¿¡è·¯äººåŒ–å¦†ï¼Œè¿™ä½è®©æˆ‘ä¸å–œæ¬¢çš„å¥³æ€§æ™ºå•†å¯ä»¥çš„,\"ä¸å¬è€å¦ˆå®è¯æ•´å®¹,ä¿¡è·¯äººåŒ–å¦†,å¹•åˆƒæ™ºå•†å¯ä»¥çš„\"\n",
      "363,zh,å’Œé‚£ç¾¤åŸºä½¬ä¸€æ ·ï¼Œå®‰èƒ½è¾©å®ƒæ˜¯é›„é›Œ,å’Œéƒ¨åˆ†ç”·åŒæ€§æ‹ä¸€æ ·ï¼Œå¾ˆéš¾è¾¨åˆ«æ€§åˆ«,\"å’Œé‚£ç¾¤åŸºä½¬ä¸€æ ·,å®‰èƒ½è¾©å®ƒæ˜¯é›„é›Œ\"\n",
      "364,ru,Ğ¸ Ğ½ĞµÑ…ÑƒÑ Ğ·Ğ´ĞµÑÑŒ Ğ¾Ñ€Ğ°Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¼ Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ°Ğ´Ğ»ĞµĞ¶Ğ°Ğ»Ğ¾,Ğ˜ Ğ½ĞµÑ‡ĞµĞ³Ğ¾ Ğ·Ğ´ĞµÑÑŒ Ğ¾Ñ€Ğ°Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¼ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ°Ğ´Ğ»ĞµĞ¶Ğ°Ğ»Ğ¾,Ğ¸ Ğ½ĞµÑ…ÑƒÑ Ğ·Ğ´ĞµÑÑŒ Ğ¾Ñ€Ğ°Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¼ Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ°Ğ´Ğ»ĞµĞ¶Ğ°Ğ»Ğ¾\n",
      "365,hi,à¤µà¤¹à¤¾à¤‚ à¤•à¤¾ à¤®à¥à¤–à¥à¤¯à¤®à¤‚à¤¤à¥à¤°à¥€ à¤­à¤¡à¤µà¤¾ à¤¹à¥ˆ à¥¤,à¤µà¤¹à¤¾à¤‚ à¤•à¤¾ à¤®à¥à¤–à¥à¤¯à¤®à¤‚à¤¤à¥à¤°à¥€ à¤…à¤šà¥à¤›à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ |,à¤µà¤¹à¤¾à¤‚ à¤•à¤¾ à¤®à¥à¤–à¥à¤¯à¤®à¤‚à¤¤à¥à¤°à¥€ à¤­à¤¡à¤µà¤¾ à¤¹à¥ˆ\n",
      "366,uk,\"ĞĞ°Ñ…ÑƒĞ¹ Ñ‚Ğ¸ Ğ¿Ğ°Ğ»Ğ¸Ñˆ, Ğ½Ñƒ Ğ±Ğ»Ñ\",\"Ğ½Ğ°Ñ‰Ğ¾ Ñ‚Ğ¸ Ğ¿Ğ°Ğ»Ğ¸Ñˆ, Ğ½Ñƒ Ğ±Ğ»Ğ¸Ğ½\",\"Ğ¾Ğ½ĞµĞ· Ñ‚Ğ¸ Ğ¿Ğ°Ğ»Ğ¸Ñˆ,Ğ½Ñƒ Ğ±Ğ»Ñ\"\n",
      "367,de,\"Ein schmieriger Sozi,ein Faschist der extremen Linken,ein Antideutsches Subjekt!\",\"Ein Sozialist,ein AnhÃ¤nger der extremen Linken,ein Antideutsches Subjekt!\",\"schmieriger Sozi,ein Faschist der extremen Linken,ein Antideutsches Subjekt!\"\n",
      "368,es,Ese buen hombre por lo menos pagarÃ¡ 7 u 8 pensiones. Es el mÃ¡s valiente sin duda.,Ese hombre generoso seguramente estÃ© contribuyendo con al menos 7 u 8 pensiones. Es admirable su valentÃ­a.,<extra_id_0> ese buen hombre por lo menos pagarÃ¡ 7 u 8 pensiones. Es el mÃ¡s valiente sin duda.\n",
      "369,es,\"Totalmente de acuerdo con la diputada, a esos cÃ¡fres asquerosos que no los vayan a dejar devolver a BogotÃ¡.\",Estoy totalmente de acuerdo con el diputado que no les permitirÃ¡ regresar a BogotÃ¡.,<extra_id_0> a esos cÃ¡fres asquerosos que no los vayan a dejar devolver a BogotÃ¡.\n",
      "370,zh,åˆ°æ—¶å€™å¥½çœ‹çš„å¦¹å¦¹ä¸€ä¸ªä¸å‰©ï¼Œå‰©ä¸‹çš„å…¨æ˜¯é‡å‹å¦å…‹,åˆ°æ—¶å€™å¥½çœ‹çš„å¦¹å¦¹ä¸€ä¸ªä¸å‰©ï¼Œå‰©ä¸‹çš„å…¨æ˜¯ä½“é‡è¾ƒé‡çš„å¥³æ€§,\"åˆ°æ—¶å€™å¥½çœ‹çš„å¦¹å¦¹ä¸€ä¸ªä¸å‰©,å‰©ä¸‹çš„å…¨æ˜¯é‡å‹å¦å…‹\"\n",
      "371,hi,\"à¤¤à¥‚ à¤¨à¤¹à¥€ à¤¸à¥à¤§à¤°à¥‡à¤—à¤¾ à¤¤à¥‚ à¤•à¤®à¥€à¤¨à¤¾ à¤•à¤¾ à¤•à¤®à¥€à¤¨à¤¾ à¤¹à¥€ à¤°à¤¹à¥‡à¤—à¤¾ , à¤šà¥‚à¤¤à¤¿à¤¯à¤¾à¥¤\",à¤¤à¥‚ à¤¨à¤¹à¥€à¤‚ à¤¸à¥à¤§à¤°à¥‡à¤—à¤¾ à¤µà¥ˆà¤¸à¤¾ à¤•à¤¾ à¤µà¥ˆà¤¸à¤¾ à¤¹à¥€ à¤°à¤¹ à¤œà¤¾à¤à¤—à¤¾ |,\"<extra_id_0> à¤¤à¥‚ à¤¨à¤¹à¥€ à¤¸à¥à¤§à¤°à¥‡à¤—à¤¾ à¤¤à¥‚ à¤•à¤®à¥€à¤¨à¤¾ à¤•à¤¾ à¤•à¤®à¥€à¤¨à¤¾ à¤¹à¥€ à¤°à¤¹à¥‡à¤—à¤¾, à¤šà¥‚à¤¤à¤¿à¤¯à¤¾à¥¤\"\n",
      "372,en,\"I didnt attack you, you retard DO NOT POST ON MY PAGE\",\"I didnt attack you, so, do not post on my page.\",\"<extra_id_0> didnt attack you, you retard DO NOT POST ON MY Page\"\n",
      "373,hi,à¤…à¤°à¥‡ à¤ªà¤¾à¤—à¤² à¤œà¥‚à¤ à¤¾ à¤°à¥‹à¤œ à¤²à¤¾à¤ˆà¤Ÿ à¤¬à¤‚à¤¦ à¤°à¤–à¤¤à¤¾ à¤¹à¥‡à¤‚ à¤­à¤¡à¥à¤µà¤¾,à¤…à¤°à¥‡ à¤œà¥‚à¤ à¤¾ à¤°à¥‹à¤œ à¤²à¤¾à¤ˆà¤Ÿ à¤¬à¤‚à¤¦ à¤°à¤–à¤¤à¤¾ à¤¹à¥‡à¤‚,à¤°à¥‹à¤œ à¤²à¤¾à¤ˆà¤Ÿ à¤¬à¤‚à¤¦ à¤°à¤–à¤¤à¤¾ à¤¹à¥‡à¤‚ à¤­à¤¡à¥à¤µà¤¾\n",
      "374,ru,\"ÑÑ‚Ğ¾ Ğ¿Ğ¸Ğ¿ĞµÑ†! Ğ³Ñ€ĞµÑ… ĞºĞ°ĞºĞ¾Ğ¹! Ğ³Ğ¾Ñ€ĞµÑ‚ÑŒ Ğ¸Ğ¼ Ğ² Ğ°Ğ´Ñƒ , Ğ²Ğ°Ğ½Ğ´Ğ°Ğ»Ñ‹!\",Ğ­Ñ‚Ğ¾ Ğ¿Ğ¸Ğ¿ĞµÑ†! Ğ“Ñ€ĞµÑ… ĞºĞ°ĞºĞ¾Ğ¹!,\"<extra_id_0> ÑÑ‚Ğ¾ Ğ¿Ğ¸Ğ¿ĞµÑ† ĞºĞ°ĞºĞ¾Ğ¹! Ğ³Ğ¾Ñ€ĞµÑ‚ÑŒ Ğ¸Ğ¼ Ğ² Ğ°Ğ´Ñƒ, Ğ²Ğ°Ğ½Ğ´Ğ°Ğ»Ñ‹!\"\n",
      "375,en,and ya can 't draw fucking cartoons .,and you can't draw cartoons,<extra_id_0> and ya can 't draw cartoons .\n",
      "376,ru,\"Ğ´Ğ°Ğ¶Ğµ Ğ²Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğµ Ğ½ĞµÑƒĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ´ĞµĞ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ÑƒÑ€Ñ‹.\",\"Ğ´Ğ°Ğ¶Ğµ Ğ²Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğµ Ğ½ĞµÑƒĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ´ĞµĞ²ÑƒÑˆĞºĞ¸\",<extra_id_0> Ğ´Ğ°Ğ¶Ğµ Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ»ÑĞ´Ğ¸.\n",
      "377,es,#YoSoyAnonymousUE basta de absurda represiÃ³n victimista por parte del Estado Fascista,#YoSoyAnonymousUE basta de absurda represiÃ³n victimista por parte del Estado,ç»ª Soy AnonymousUE basta de absurd represiÃ³n victimista por parte del Estadosta\n",
      "378,en,Bin Laden and his 19 so-called hijackers were patsies.,Bin Laden and his 19 so-called hijackers were not the smartest people.,<extra_id_0> Bin Laden and his 19 so-called hijacker were patsies\n",
      "379,es,Si fuese verdad se lo estarÃ­a pagando el gobierno y les darÃ­an una paga para que no tengan problemas econÃ³micos.,\"Si eso fuera cierto, el gobierno les proporcionarÃ­a ayuda econÃ³mica para evitar problemas financieros.\",<extra_id_0> les darÃ­an una paga para que no tengan problema econÃ³micos\n",
      "380,uk,Ğ¦ĞµĞ¹ Ğ±Ğ»ÑĞ´ÑÑŒĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ñ–Ğ¾Ğ½ĞµÑ€ - Ğ³Ğ¾Ğ¼Ğ¾Ñ„Ğ¾Ğ±!,Ğ¦ĞµĞ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ñ–Ğ¾Ğ½ĞµÑ€ - Ğ³Ğ¾Ğ¼Ğ¾Ñ„Ğ¾Ğ±!,<extra_id_0> Ğ¦ĞµĞ¹ Ğ±Ğ»ÑĞ´ÑÑŒĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ñ–Ğ¾Ğ½ĞµÑ€ Ğ³Ğ¾Ğ¼Ğ¾Ñ„Ğ¾Ğ±!\n",
      "381,en,who the fuck is pushing diversity on you ? .,who is pushing diversity on you ? .,<extra_id_0> who the fuck is pushing diversity on you ?\n",
      "382,hi,à¤¯à¤¹à¤¾à¤ à¤­à¥‹à¤¸à¤¡à¥€à¤•à¥‡ à¤¹à¤® à¤šà¤¾à¤° à¤²à¥Œà¤‚à¤¡à¥‡ à¤¬à¥ˆà¤  à¤•à¥‡ à¤®à¥ˆà¤š à¤¦à¥‡à¤– à¤°à¤¹à¥‡,à¤¯à¤¹à¤¾à¤ à¤¹à¤® à¤šà¤¾à¤° à¤²à¥Œà¤‚à¤¡à¥‡ à¤¬à¥ˆà¤  à¤•à¥‡ à¤®à¥ˆà¤š à¤¦à¥‡à¤– à¤°à¤¹à¥‡,à¤¯à¤¹à¤¾à¤ à¤­à¥‹à¤¸à¤¡à¥€à¤•à¥‡ à¤¹à¤® à¤šà¤¾à¤° à¤²à¥Œà¤‚à¤¡à¥‡ à¤¬à¥ˆà¤  à¤®à¥ˆà¤š à¤¦à¥‡à¤– à¤°à¤¹à¥‡\n",
      "383,uk,ĞÑƒ Ñ– Ğ¹Ñ‚Ğ¸ Ğ´Ğ¾ ÑÑ€Ğ°ĞºĞ¸,ĞÑƒ Ñ– Ğ¹Ñ‚Ğ¸  Ğ´Ğ°Ğ»ĞµĞºĞ¾,<extra_id_0> ĞÑƒ Ñ– Ğ¹Ñ‚Ğ¸ Ğ´Ğ¾ ÑÑ€Ğ°ĞºĞ¸\n",
      "384,ru,Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ½ ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ´Ğ¾Ñ…ÑƒÑ!!! Ğ¸ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğµ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²ÑÑ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ€Ğ°ÑÑ‚Ğ¾Ñ‰Ğ¸Ğ»Ğ¸,\"Ğ¢Ğ°ĞºĞ¸Ñ…, ĞºĞ°Ğº Ğ¾Ğ½ ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾! Ğ˜ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğµ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ğ²ÑÑ ÑÑ‚Ñ€Ğ°Ğ½Ñƒ Ñ€Ğ°ÑÑ‚Ğ°Ñ‰Ğ¸Ğ»Ğ¸.\",Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğµ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²ÑÑ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ€Ğ°ÑÑ‚Ğ¾Ñ‰Ğ¸Ğ»Ğ¸\n",
      "385,uk,\"Ğ‘ÑƒĞ²Ğ°Ñ”, Ñ‡Ğ¸Ñ‚Ğ°Ñ”Ñˆ ÑˆĞ¾ÑÑŒ Ñ– Ğ´ÑƒĞ¼Ğ°Ñ”Ñˆ Ğ¿Ñ€Ğ¾ ÑĞµĞ±Ğµ \"\"ĞÑƒ Ñ‰Ğ¾ Ğ·Ğ° Ñ…ÑƒĞ¹Ğ½Ñ?\",\"Ğ‘ÑƒĞ²Ğ°Ñ”, Ñ‡Ğ¸Ñ‚Ğ°Ñ”Ñˆ ÑˆĞ¾ÑÑŒ Ñ– Ğ´ÑƒĞ¼Ğ°Ñ”Ñˆ Ğ¿Ñ€Ğ¾ ÑĞµĞ±Ğµ \"\"ĞÑƒ Ñ‰Ğ¾ Ğ·Ğ° Ğ½Ñ–ÑĞµĞ½Ñ–Ñ‚Ğ½Ğ¸Ñ†Ñ?\",\"<extra_id_0>, Ñ‡Ğ¸Ñ‚Ğ°Ñ”Ñˆ ÑˆĞ¾ÑÑŒ Ñ– Ğ´ÑƒĞ¼Ğ°Ñ”Ñˆ Ğ¿Ñ€Ğ¾ ÑĞµĞ±Ğµ \"\"ĞÑƒ Ñ‰Ğ¾ Ğ·Ğ° Ñ…ÑƒĞ¹Ğ½Ñ?\"\n",
      "386,ar,Ù‡Ø°Ø§ Ø®Ø§Ø±Ø¬ÙŠ ØªÙƒÙÙŠØ±ÙŠ. ÙˆÙŠØ£ØªÙˆÙ† Ø¨Ø¢Ø«Ø§Ø± Ù…ÙƒØ°ÙˆØ¨Ù‡ ÙˆÙŠØªØ±ÙƒÙˆÙ† Ø§Ù„Ø§Ø­Ø§Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø§Ø­ Ø§Ù„ØªÙŠ ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ø¨Ù„â€¦,Ù‡Ø°Ø§ ÙŠÙƒÙØ± Ø§Ù„Ù†Ø§Ø³ . ÙˆÙŠØ£ØªÙˆÙ† Ø¨Ø¢Ø«Ø§Ø± Ù…ÙƒØ°ÙˆØ¨Ù‡ ÙˆÙŠØªØ±ÙƒÙˆÙ† Ø§Ù„Ø§Ø­Ø§Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø§Ø­ Ø§Ù„ØªÙŠ ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ø¨Ù„â€¦,<extra_id_0>ÙŠØ£ØªÙˆÙ† Ø¨Ø¢Ø«Ø§Ø± Ù…ÙƒØ°ÙˆØ¨Ù‡ ÙˆÙŠØªØ±ÙƒÙˆÙ† Ø§Ù„Ø§Ø­Ø§Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø§Ø­ Ø§Ù„ØªÙŠ ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ø¨Ù„...\n",
      "387,uk,\"- ĞœĞµĞ½Ñ– Ğ¿Ğ¾Ñ…ÑƒĞ¹, Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾ Ğ¼ĞµĞ½Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ÑŒ.\",\"- ĞœĞµĞ½Ñ– Ğ²ÑĞµ Ñ€iĞ²Ğ½Ğ¾, Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾ Ğ¼ĞµĞ½Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ÑŒ.\",\"- ĞœĞµĞ½Ñ– Ğ¿Ğ¾Ñ…ÑƒĞ¹, Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾ Ğ¼ĞµĞ½Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ÑŒ.\"\n",
      "388,ru,Ğ´Ğ° Ğ¸ Ğ¿ÑƒÑÑ‚ÑŒ Ñ…ÑƒÑÑ€Ğ¸Ñ‚ Ñ‚ÑƒĞ´Ğ° . Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€Ğ¾Ğº,Ğ´Ğ° Ğ¸ Ğ¿ÑƒÑÑ‚ÑŒ Ğ¸Ğ´ĞµÑ‚ Ñ‚ÑƒĞ´Ğ°,Ğ¸ Ğ¿ÑƒÑÑ‚ÑŒ Ñ…ÑƒÑÑ€Ğ¸Ñ‚ Ñ‚ÑƒĞ´Ğ° . Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€Ğ¾Ğº\n",
      "389,uk,Ğ†Ğ´Ñ–Ñ‚ÑŒ Ğ½Ğ°Ñ…ÑƒĞ¹ Ğ·Ñ– ÑĞ²Ğ¾Ñ—Ğ¼Ğ¸ Ğ±Ğ»ÑĞ´ÑÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸.,Ğ’Ñ–Ğ´Ñ‡ĞµĞ¿Ñ–Ñ‚ÑŒÑÑ Ğ·Ñ– ÑĞ²Ğ¾Ñ—Ğ¼Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸.,Ñ–Ğ´Ñ–Ñ‚ÑŒ Ğ½Ğ°Ñ…ÑƒĞ¹ Ğ·Ñ– ÑĞ²Ğ¾Ñ—Ğ¼Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸.\n",
      "390,ar,ÙŠØ§ Ø¥Ù„Ù‡ÙŠØŒ Ù„Ù‚Ø¯ Ø°Ù‡Ø¨Øª Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†ØŒ Ø¥Ù†Ù‡ Ø£Ù…Ø± Ø³Ø®ÙŠÙ !,Ø£ÙˆÙ…Ø¬ØŒ Ø£Ù†Øª Ø­ØªÙ‰ Ø§Ù„Ø¢Ù† Ø°Ù‡Ø¨Øª Ø£Ù†Ù‡Ø§ Ù„ÙŠØ³Øª Ø¬ÙŠØ¯Ø©!,<extra_id_0> Ù„Ù‚Ø¯ Ø°Ù‡Ø¨Øª Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†ØŒ Ø¥Ù†Ù‡ Ø£Ù…Ø± Ø³Ø®ÙŠÙ\n",
      "391,ar,Ø¥Ù†Ù‡Ù… ÙŠØ­Ø§ÙˆÙ„ÙˆÙ† Ø£Ù† ÙŠØ¬Ø¹Ù„ÙˆÙƒ ØªØªØºÙˆØ· ÙÙŠ Ø¨Ù†Ø·Ø§Ù„Ùƒ ÙƒÙ„ Ù…Ø±Ø© ØªØ³Ù…Ø¹ Ø£Ø³Ù…Ù‡Ù…,Ø§Ù†Ù‡Ù… ÙŠØ­Ø§ÙˆÙ„ÙˆÙ† Ø§Ø®Ø§ÙØªÙƒ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© ØªØ³Ù…Ø¹ Ø§Ø³Ù…Ù‡Ù…,<extra_id_0> ÙŠØ­Ø§ÙˆÙ„ÙˆÙ† Ø£Ù† ÙŠØ¬Ø¹Ù„ÙˆÙƒ ØªØªØºÙˆØ· ÙÙŠ Ø¨Ù†Ø·Ø§Ù„Ùƒ ÙƒÙ„ Ù…Ø±Ø© ØªØ³Ù…Ø¹ Ø£Ø³Ù…Ù‡Ù…\n",
      "392,am,áŠ¥áŠ”áŠ• áŒ¨áˆáˆ® áˆ°á‹‰ áˆ½áˆ˜áˆáˆµáŠ• áˆ›á‹¨á‰µ áŠ á‹­áˆáˆáŒáˆ áŒáŠ• áˆˆá‹“á‹­áŠ“á‰½áŠ• á‹¨á‰ á‹›á‹ áŠ¥áˆ± áŠá‹,áŠ¥áŠ”áŠ• áŒ¨áˆáˆ® áˆ°á‹‰ áˆ½áˆ˜áˆáˆµáŠ• áˆ›á‹¨á‰µ áŠ á‹­áˆáˆáŒáˆ áŒáŠ• á‰ á‰¥á‹›á‰µ á‹¨áˆšá‰³á‹¨á‹ á‹°áŒáˆ áŠ¥áˆ± áŠá‹,<extra_id_0>áŠ”áŠ• áŒ¨áˆáˆ® áˆ°á‹‰ áˆ½áˆ˜áˆáˆµáŠ• áˆ›á‹¨á‰µ áŠ á‹­áˆáˆáŒáˆ áŒáŠ• áˆˆá‹“á‹­áŠ“á‰½áŠ• á‹¨á‰ á‹›á‹ áŠ¥áˆ± áŠá‹\n",
      "393,es,Que ganas de que metan al peluo en la carcel por subnormalğŸ˜†ğŸ˜†ğŸ˜†,Espero que lo lleven a la cÃ¡rcel por sus acciones inapropiadas.,ganas de que metan al peluo en la carcel por subnormalğŸ˜†ğŸ˜†ğŸ˜†\n",
      "394,am,á‰ áŠ áŠ•á‹´ áŠ«á‹µáˆ½áŠ? áˆˆáŠáŒˆáˆ© á‹¨áˆ˜áŒ€áˆ˜áˆªá‹«áˆ½ áŠ á‹­á‹°áˆˆáˆ ??,á‰ áŠ áŠ•á‹´ á‹µáŒ‹ááˆ½áŠ• áŠ áŠáˆ³áˆ½? áˆˆáŠáŒˆáˆ© á‹¨áˆ˜áŒ€áˆ˜áˆªá‹«áˆ½ áŠ á‹­á‹°áˆˆáˆ ??,<extra_id_0> áˆˆáŠáŒˆáˆ© á‹¨áˆ˜áŒ€áˆ˜áˆªá‹«áˆ½ áŠ á‹­á‹°áˆˆáˆ ??\n",
      "395,hi,à¤­à¥‹à¤¸à¤¡à¥€à¤•à¥‡ à¤¯à¥‡ à¤…à¤«à¤µà¤¾à¤¹ à¤¨à¤¹à¥€ à¤¸à¤¤à¥à¤¯ à¤¹à¥ˆà¥¤ à¤¸à¤¾à¤°à¥‡ à¤•à¤¾ à¤¹à¤¿à¤¸à¥à¤¸à¤¾ à¤–à¤¾ à¤°à¤¹à¥‡ à¤¹à¥‹ à¤¤à¥à¤® à¤²à¥‹à¤—,à¤¯à¥‡ à¤…à¤«à¤µà¤¾à¤¹ à¤¨à¤¹à¥€ à¤¸à¤¤à¥à¤¯ à¤¹à¥ˆà¥¤ à¤¸à¤¾à¤°à¥‡ à¤•à¤¾ à¤¹à¤¿à¤¸à¥à¤¸à¤¾ à¤–à¤¾ à¤°à¤¹à¥‡ à¤¹à¥‹ à¤¤à¥à¤® à¤²à¥‹à¤—,<extra_id_0> à¤¯à¥‡ à¤…à¤«à¤µà¤¾à¤¹ à¤¨à¤¹à¥€à¤‚ à¤¸à¤¤à¥à¤¯ à¤¹à¥ˆà¥¤ à¤¸à¤¾à¤°à¥‡ à¤•à¤¾ à¤¹à¤¿à¤¸à¥à¤¸à¤¾ à¤–à¤¾ à¤°à¤¹à¥‡ à¤¹à¥‹ à¤¤à¥à¤® à¤²à¥‹à¤—\n",
      "396,zh,çŒªç²¾éƒ½æ˜¯è¿™æ ·çš„ï¼Œåƒä¸åˆ°æ€§åˆ«ç¦åˆ©åªèƒ½èµ°è¿™æ¡è·¯,æœ‰äº›è‡ªæˆ‘æ„Ÿè§‰è‰¯å¥½çš„è‚¥èƒ–å¥³æ€§éƒ½æ˜¯è¿™æ ·çš„ï¼Œåƒä¸åˆ°æ€§åˆ«ç¦åˆ©åªèƒ½èµ°è¿™æ¡è·¯,\"çŒªç²¾éƒ½æ˜¯è¿™æ ·çš„,åƒä¸åˆ°æ€§åˆ«ç¦åˆ©åªèƒ½èµ°è¿™æ¡è·¯\"\n",
      "397,zh,æ˜¯è¿™ä¸ªddç¬¬äºŒä»£å¼•èµ·å­™å§ååŒï¼Ÿ,æ˜¯è¿™ä¸ªddç¬¬äºŒä»£å¼•èµ·å­™å§åå¯¹æ€§å°‘æ•°ç¾¤ä½“ï¼Ÿ,æ˜¯è¿™ä¸ªddç¬¬äºŒä»£å¼•èµ·å­™å§ååŒ?\n",
      "398,de,Warum muss ein AFD Vogel dabei sein?,Warum muss einer von der AFD dabei sein?,<extra_id_0> warum muss man einen Vogel beibei sein.\n",
      "399,en,this deadly behavior is why abusive police in the 60s were referred to as pigs .,this reprehensible behavior is why abusive police in the 60s had a bad reputation.,<extra_id_0> this deadly behavior is why abusive police in the 60s were referred to as pigs.\n",
      "400,am,áˆµáˆˆáˆ›á‹­áŒˆá‰£á‹ áˆˆáˆ›á‹­áˆ¨á‹³ áŠ á‹­áˆáˆ®á‹ á‹µááŠ• áˆˆáˆ†áŠ áˆ°á‹ á‹­áˆ„ áˆáˆ‹ áŠ á‹«áˆµáˆáˆáŒáˆ áˆ°áˆ‰á‹¬ áˆ€áá‰°á‹­ áˆ˜á‰°á‹ á‰¥á‰» ??,áˆˆáˆ›á‹­áŒˆá‰£á‹ áˆˆáˆ›á‹­áˆ¨á‹³ áˆ°á‹ á‹­áˆ„ áˆáˆ‹ áŠ á‹«áˆµáˆáˆáŒáˆ áˆ°áˆ‰á‹¬ áˆ€áá‰°á‹­ áˆ˜á‰°á‹ á‰¥á‰» ??,áˆ°áˆ‰á‹¬ áˆ€áá‰°á‹­ áˆ˜á‰°á‹ á‰¥á‰» ??\n",
      "401,de,\"@USER_13 @USER_15 @USER_9 @USER_3 @USER_4 @USER_7 @USER_1 @USER_14 @USER_6 @USER_12 @USER_5 @USER_10 @USER_11 Ne, das gibts nicht, ist der Vollpfosten immer noch da???\",\"@USER_13 @USER_15 @USER_9 @USER_3 @USER_4 @USER_7 @USER_1 @USER_14 @USER_6 @USER_12 @USER_5 @USER_10 @USER_11 Ne, das gibts nicht, ist der Typ immer noch da???\",'/USER_13 @USER_13 @USER_13 @USER_15 @USER_9 @USER_1 @USER_1 @USER_1 @USER_6 @USER_5 @USER_6 @USER_6 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_6 @USER_5 @USER_5 @USER_6 @USER_6 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_6 @USER_5 @USER_6 @USER_6 @USER_6 @USER_5 @USER_6 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_6 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_5 @USER_6 @USER_6 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @USER_1 @\n",
      "402,ar,ÙÙ‚Ø· Ø§Ù„ÙƒÙ†Ø§Ø¦Ø³ ØªØ¹Ø·ÙŠ Ø­Ù…Ø§Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ¨Ø¯Ùˆ.,ÙÙ‚Ø· Ø§Ù„ÙƒÙ†Ø§Ø¦Ø³ ØªÙ‡ØªÙ… Ø¨Ù‡ ÙŠØ¨Ø¯Ùˆ,<extra_id_0> Ø§Ù„ÙƒÙ†Ø§Ø¦Ø³ ØªØ¹Ø·ÙŠ Ø­Ù…Ø§Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ¨Ø¯Ùˆ\n",
      "403,uk,ĞšĞ¸Ğ´Ğ°Ñ” Ñ„Ğ¾Ñ‚ĞºÑƒ Ğ² Ğ¿Ğ»Ğ°Ñ‚Ñ‚Ñ– Ñ– ĞºĞ°Ğ¶Ğµ - Ğ´Ğ¸Ğ²Ğ¸ ÑĞºĞ° Ğ² Ğ¼ĞµĞ½Ğµ Ñ‚ÑƒÑ‚ Ğ¶Ğ¾Ğ¿Ğ°!,'ĞšĞ¸Ğ´Ğ°Ñ” Ñ„Ğ¾Ñ‚ĞºÑƒ Ğ² Ğ¿Ğ»Ğ°Ñ‚Ñ‚Ñ– Ñ– ĞºĞ°Ğ¶Ğµ - Ğ´Ğ¸Ğ²Ğ¸ ÑĞºĞ° Ğ² Ğ¼ĞµĞ½Ğµ Ñ‚ÑƒÑ‚ Ğ¿''ÑÑ‚Ğ° Ñ‚Ğ¾Ñ‡ĞºĞ°!',<extra_id_0>Ğ´Ğ°Ñ” Ñ„Ğ¾Ñ‚ĞºÑƒ Ğ² Ğ¿Ğ»Ğ°Ñ‚Ñ‚Ñ– Ñ– ĞºĞ°Ğ¶Ğµ - Ğ´Ğ¸Ğ²Ğ¸ ÑĞº Ğ² Ğ¼ĞµĞ½Ğµ Ñ‚ÑƒÑ‚ Ğ¶Ğ¾Ğ¿Ğ°!\n",
      "404,am,áŠ¥áŠ“ áŠ áˆœáˆªáŠ« á‹¨áŠ á‹œáŒ‹ áŠ“á‰µ á‹ˆá‹­áˆ áŠá‰ áˆ­á‰½ áŒˆáŠ“áˆˆáŒˆáŠ“ á‰  á‹¶áˆ‹áˆ­ áˆ²áŠ¨áˆáˆáˆ… áŠ áˆœáˆªáŠ« á‹˜áˆ˜á‹µáˆ… áˆ˜áˆ°áˆˆá‰½áˆ…,áŠ¥áŠ“ áŠ áˆœáˆªáŠ« á‹¨áŠ á‹œáŒ‹ áŠ“á‰µ á‹ˆá‹­áˆ áŠá‰ áˆ­á‰½ á¤ áŠ­áá‹« á‰ á‹¶áˆ‹áˆ­ áˆµáˆˆá‰°áˆáŒ¸áˆ˜áˆáˆ… áŠ áˆœáˆªáŠ«áŠ• á‰ á‹šáˆ… á‹°áˆ¨áŒƒ áˆ›á‰…áˆ¨á‰¥,á‹ˆá‹­áˆ áŠá‰ áˆ­á‰½ á‰ á‹¶áˆ‹áˆ­ áˆ²áŠ¨áˆáˆáˆ… áŠ áˆœáˆªáŠ« á‹˜áˆ˜á‹µáˆ… áˆ˜áˆ°áˆˆá‰½áˆ…\n",
      "405,hi,à¤¸à¤¾à¤²à¤¾ à¤¦à¤²à¥à¤²à¤¾ à¤µà¥‹à¤Ÿ à¤•à¥‡ à¤•à¥à¤› à¤­à¥€ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ |,à¤µà¤¹ à¤µà¥‹à¤Ÿ à¤•à¥‡ à¤•à¥à¤› à¤­à¥€ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ |,<extra_id_0> à¤¸à¤¾à¤²à¤¾ à¤¦à¤²à¥à¤²à¤¾ à¤µà¥‹à¤Ÿ à¤•à¥à¤› à¤­à¥€ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ\n",
      "406,zh,å¯¹å¯¹å¯¹ï¼Œéƒ½æ˜¯å°ä»™å¥³ï¼Œå°ä»™å¥³æ²¡æœ‰é”™ï¼Œéƒ½æ˜¯ä¸­å›½ç”·äººçš„é”™ï¼Œä¸­å›½ç”·äººåº”è¯¥ç­ç»ï¼,å¯¹å¯¹å¯¹ï¼Œéƒ½æ˜¯è‡ªæˆ‘æ„Ÿè§‰è¿‡äºè‰¯å¥½çš„å¥³æ€§ï¼Œè‡ªæˆ‘æ„Ÿè§‰è¿‡äºè‰¯å¥½çš„å¥³æ€§æ²¡æœ‰é”™ï¼Œéƒ½æ˜¯ä¸­å›½ç”·äººçš„é”™ï¼Œä¸­å›½ç”·äººåº”è¯¥ç­ç»ï¼,\"<extra_id_0>å¯¹å¯¹,éƒ½æ˜¯ä¸­å›½ç”·äºº,å°ä»™å¥³æ²¡æœ‰é”™,éƒ½æ˜¯ä¸­å›½ç”·äººçš„é”™,ä¸­å›½ç”·äººåº”è¯¥ç­ç»!\"\n",
      "407,de,\"Schreibt besser mal Ã¼ber die Asylanten Verbrechen ,dass wird bei euch totgeschwiegen!\",\"Schreibt besser mal Ã¼ber die Asylbewerber Verbrechen ,dass wird bei euch totgeschwiegen!\",\"besser mal Ã¼ber die Asylanten Verbrechen ,dass wird bei euch totgeschwiegen!\"\n",
      "408,es,Porque usted es un hombre de bien y con valores. Esos otros son alimaÃ±as y 0,Porque usted es un hombre de bien y con valores. Esos otros no lo son,<extra_id_0> usted es un hombre de bien y con valores. Esos otros son alimaÃ±as y 0\n",
      "409,en,she fucking lied to them about what she was doing .,she  lied to them about what she was doing .,fucking lied to them about what she was doing\n",
      "410,en,Queen Mary Mrs Windsor's granny was a bitch.,Queen Mary Windsor's grandmother was not a pleasant person.,Windsor's granny was a bitch.\n",
      "411,ar,Ø¹Ù†Ø¬Ø¯ Ù‡ÙŠØ¯Ø§ Ø´Ø§Ø±ÙˆÙ† Ù‡ÙŠØ¯Ø§ Ø¬Ù…Ø§Ø¹Ø© Ø´Ø§Ø±ÙˆÙ† Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ ÙŠØ§Ø±Ø¨ ØªØ®Ø³Ø± Ø£ÙˆÙ„Ø§Ø¯Ùƒ,Ø¹Ù†Ø¬Ø¯ Ù‡ÙŠØ¯Ø§ Ø´Ø§Ø±ÙˆÙ† Ù‡ÙŠØ¯Ø§ Ø¬Ù…Ø§Ø¹Ø© Ø´Ø§Ø±ÙˆÙ† ÙŠØ§Ø±Ø¨ ØªØ®Ø³Ø± Ø£ÙˆÙ„Ø§Ø¯Ùƒ,Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ù…Ø¬Ø±Ù… ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ ÙŠØ§\n",
      "412,ru,\"Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ñ ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ·Ğ° Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ²Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµÑĞ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒ,ĞºĞ°Ğº 1918-1922Ğ³,ÑĞ¸Ğ±Ğ¸Ñ€ÑŒ,Ğ°Ğ»Ñ‚Ğ°Ğ¹,Ğ¸ Ğ´Ñ€.Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ğ²ÑƒÑ‚ Ğ±ĞµĞ· Ğ¼Ğ¾ÑĞºĞ°Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ñ†ĞµĞ²,Ğ° Ğ²Ğ¾Ñ‚ Ğ¾Ğ½Ğ¸?\",\"Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ñ ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ·Ğ° Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ²Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµÑĞ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒ,ĞºĞ°Ğº 1918-1922Ğ³,ÑĞ¸Ğ±Ğ¸Ñ€ÑŒ,Ğ°Ğ»Ñ‚Ğ°Ğ¹,Ğ¸ Ğ´Ñ€.Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ğ²ÑƒÑ‚ Ğ±ĞµĞ· Ğ¼Ğ¾ÑĞºĞ²Ğ¸Ñ‡ĞµĞ¹,Ğ° Ğ²Ğ¾Ñ‚ Ğ¾Ğ½Ğ¸?\",\"Ñ ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ·Ğ° Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ²Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµÑĞ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒ,ĞºĞ°Ğº 1918-1922Ğ³,ÑĞ¸Ğ±Ğ¸Ñ€ÑŒ,Ğ°Ğ»Ñ‚Ğ°Ğ¹,Ğ¸ Ğ´Ñ€.Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ğ²ÑƒÑ‚ Ğ±ĞµĞ· Ğ¼Ğ¾ÑĞºĞ°Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ñ†ĞµĞ²,Ğ° Ğ²Ğ¾Ñ‚ Ğ¾Ğ½Ğ¸?\"\n",
      "413,de,\"Die Laien-Ministerin kann zwar nicht mehr \"\"werfen\"\", aber sie erzeugt bei. mir Geilheit: \"\"den Umschalter zu nutzen.\"\"\",\"Die Laien-Ministerin kann zwar nicht mehr \"\"werfen\"\", aber sie erzeugt bei. mir Interesse: \"\"den Umschalter zu nutzen.\"\"\",\"<extra_id_0> sie kann zwar nicht mehr \"\"werfen\"\", aber sie erzeugt bei mir Geilheit: \"\"den Umschalter zu nutzen.\"\"\"\n",
      "414,de,\"Frau Kerestan hat frisch und gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint.\",Frau Kerestan hat frisch und gut argumentiert. Herr Weinberg schneint nur rÃ¼ckwÃ¤rtsgewandt zu blicken.,\"<extra_id_0> Frau Kerestan hat frisch und gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Frau Kerestan hat frisch und gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Frau Kerestan hat gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Frau Kerestan hat gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Frau Kerestan hat gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Frau Kerestan hat gut argumentiert. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken scheint. Herr Weinberg ist ein Betonkopf, der nur rÃ¼ckwÃ¤rtsgewandt zu blicken\"\n",
      "415,hi,à¤®à¥ˆà¤š à¤«à¤¿à¤¨à¤¿à¤¶ à¤•à¤° à¤¦à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤    à¤—à¤¾à¤‚à¤¡ à¤ªà¥‡ à¤¹à¤¾à¤¥ à¤°à¤– à¤•à¤° à¤¬à¥ˆà¤  à¤¨à¤¹à¥€à¤‚ à¤œà¤¾à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤ ,à¤®à¥ˆà¤š à¤«à¤¿à¤¨à¤¿à¤¶ à¤•à¤° à¤¦à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤  à¤œà¥à¤¯à¤¾à¤¦à¤¾ à¤¦à¥‡à¤° à¤¨à¤¹à¥€à¤‚ à¤•à¤°à¤¨à¥€ à¤šà¤¾à¤¹à¤¿à¤ ,à¤¬à¤¿à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤ à¤—à¤¾à¤‚à¤¡ à¤ªà¥‡ à¤¹à¤¾à¤¥ à¤°à¤– à¤•à¤° à¤¬à¥ˆà¤  à¤¨à¤¹à¥€à¤‚ à¤œà¤¾à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤\n",
      "416,uk,\"Ğ¢Ğ¸ ÑÑƒĞºĞ°Ğ½, Ğ±ÑƒÑ…Ğ°Ñ”Ñˆ Ğ±ĞµĞ· Ğ½Ğ°Ñ Ğ· \",'Ñ‚Ğ¸ Ğ¿''Ñ”Ñˆ Ğ±ĞµĞ· Ğ½Ğ°Ñ',\"',' Ğ¢Ğ¸ ÑÑƒĞºĞ°Ğ½ Ğ±ÑƒÑ…Ğ°Ñ”Ñˆ\"\n",
      "417,ru,ĞµÑĞ»Ğ¸ ÑÑ‚Ğ° Ñ‚ÑƒÑ…Ğ»Ğ¾Ğ´Ñ‹Ñ€Ğ°Ñ Ğ¾Ğ²ĞµÑ‡ĞºĞ° ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ¾Ğ±Ğ´Ğ¾Ğ»Ğ±Ğ°Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼ Ğ½Ğ¸Ñ…ÑƒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ Ñ‚Ğ¾Ñ€Ñ‡ĞµĞº Ğ¾Ğ±Ğ¾ÑÑĞ°Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ ÑÑ‚Ğ¾ ĞµĞ³Ğ¾ Ğ½ĞµĞ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¿Ğ°Ñ€Ğ°ÑˆĞ¸,\"Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚, Ñ‚Ğ¾ ÑÑ‚Ğ¾ ĞµĞ³Ğ¾ Ğ½Ğµ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸\",ĞµÑĞ»Ğ¸ ÑÑ‚Ğ° Ñ‚ÑƒÑ…Ğ»Ğ¾Ğ´Ñ‹Ñ€Ğ°Ñ Ğ¾Ğ²ĞµÑ‡ĞºĞ° ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ¾Ğ±Ğ´Ğ¾Ğ»Ğ±Ğ°Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼ Ğ½Ğ¸Ñ…ÑƒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ Ñ‚Ğ¾Ñ€Ñ‡ĞµĞº Ğ¾Ğ±Ğ¾ÑÑĞ°Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ ÑÑ‚Ğ¾ ĞµĞ³Ğ¾ Ğ½ĞµĞ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¿Ğ°Ñ€Ğ°ÑˆĞ¸\n",
      "418,am,á‰¥áˆŒáŠ• á‹­áˆ„ áˆ°á‹ á‹¨áŠ áˆ›áˆ« áááˆ áŒ áˆ‹á‰µ áŠ¥áŠ•á‹°áˆ†áŠ á‰¥á‹™ áˆ˜áˆ¨áŒƒ áŠ áˆˆáŠá¢ He is just useless á‹¨áˆá‰µá‹ˆáŒ…á‹ áŠ¨áˆ†áŠ á‹­á‰…áˆ­á‰³á¢ á•áˆ®áŒáˆ°áˆ­ áŠ áˆµáˆ«á‰µ á‹ˆáˆá‹°á‹¨áˆµáŠ• á‹«áˆµá‹«á‹˜ áŠ¥áˆ± áŠ,á‰¥áˆŒáŠ• á‹­áˆ„ áˆ°á‹ áˆˆáŠ áˆ›áˆ« áˆ…á‹á‰¥ á‰¥á‹™ á‹¨áˆšáŒ á‰…áˆ áˆ€áˆ³á‰¥ áŠ¥áŠ•áˆŒáˆˆá‹ á‰ áˆ˜áˆ¨áŒƒ áŒ­áˆáˆ­ áˆ˜áŠ“áŒˆáˆ­ áŠ¥á‰½áˆ‹áˆˆáˆá¡ áŠ¨á‹­á‰…áˆ­á‰³ áŒ‹áˆ­,á•áˆ®áŒáˆ°áˆ­ áŠ áˆµáˆ«á‰µ á‹ˆáˆá‹°á‹¨áˆµáŠ• á‹«áˆµá‹«á‹˜ áŠ¥áˆ± áŠáŠá¢ He is just useless á‹¨áˆá‰µá‹ˆáŒ…á‹ áŠ¨áˆ†áŠ á‹­á‰…áˆ­á‰³á¢\n",
      "419,uk,\"Ğ›ÑƒÑ‡Ñ‡Ñ” Ñ Ğ·Ğ´Ğ¾Ñ…Ğ½Ñƒ Ñ”Ğ±ÑƒÑ‡Ñ–Ğ¼ Ğ½Ğ¾ÑƒĞ½ĞµĞ¹Ğ¼Ğ¾Ğ¼, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ°ÑĞ»ÑĞ²Ğ»ÑÑÑŒ Ñ– Ğ¼ĞµĞ½Ğµ ÑÑ‚Ğ°Ğ½ÑƒÑ‚ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ Ğ³Ñ–Ğ²Ğ½Ğ¾Ñ‚Ğ²\",\"Ğ›ÑƒÑ‡Ñ‡Ñ” Ñ Ğ±ÑƒĞ² Ğ½Ğ¾ÑƒĞ½ĞµĞ¹Ğ¼Ğ¾Ğ¼, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ°ÑĞ»ÑĞ²Ğ»ÑÑÑŒ Ñ– Ğ¼ĞµĞ½Ğµ ÑÑ‚Ğ°Ğ½ÑƒÑ‚ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ²\",\"<extra_id_0> Ñ Ğ·Ğ´Ğ¾Ñ…Ğ½Ñƒ Ñ”Ğ±ÑƒÑ‡Ñ–Ğ¼ Ğ½Ğ¾ÑƒĞ½ĞµĞ¹Ğ¼Ğ¾Ğ¼, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ°ÑĞ»ÑĞ²Ğ»ÑÑÑŒ Ñ– Ğ¼ĞµĞ½Ğµ ÑÑ‚Ğ°Ğ½ÑƒÑ‚ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ Ğ³Ñ–Ğ²Ğ½Ğ¾Ñ‚Ğ²\"\n",
      "420,ar,ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ù‡ ÙŠØ²Ø¯Ù‡Ø± Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙˆÙƒØ«ÙŠØ±Ø§ Ù…Ø§ ÙŠØ¬Ø¹Ù„ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… ØªØ¨Ø¯Ùˆ Ù…Ø«Ù„ Ø§Ù„Ø­Ù…Ù‚Ù‰.,ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ù‡ ÙŠØ²Ø¯Ù‡Ø± Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠØ¬Ø¹Ù„ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… ØªØ¨Ø¯Ùˆ ØºÙŠØ± Ø­ÙƒÙŠÙ…Ø©.,<extra_id_0> Ø£Ù†Ù‡ ÙŠØ²Ø¯Ù‡Ø± Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙˆÙƒØ«ÙŠØ±Ø§ Ù…Ø§ ØªØ¬Ø¹Ù„ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… ØªØ¨Ø¯Ùˆ Ù…Ø«Ù„ Ø§Ù„Ø­Ù…Ù‚Ù‰\n",
      "421,es,\"En catalÃ¡n se escribe Espanya. Para dÃ¡rselas usted de ser periodista, no tieene usted puta idea de escribir.\",\"En catalÃ¡n se escribe Espanya. Para dÃ¡rselas usted de ser periodista, no tieene usted  idea de escribir.\",\"<extra_id_0> se escribe EspaÃ±a. Para darselas usted de ser periodista, no tieene usted puta idea de escribir.\"\n",
      "422,uk,\"Ğ¡ÑƒĞºĞ°, Ñ Ğ½Ñ–Ñ…ÑƒÑ Ğ½Ğµ Ğ²ÑÑ‚Ğ¸Ğ³Ğ½Ñƒ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°ÑÑ‚Ğ¸ Ñ†Ñ– Ğ¹Ğ¾Ğ±Ğ°Ğ½Ñ– Ñ‚ĞµĞºÑÑ‚Ğ¸ Ğ·Ğ° 2 Ğ´Ğ½Ñ–.\",Ñ Ğ½Ñ–Ñ‡Ğ¾Ğ³Ğ¾ Ğ½Ğµ Ğ²ÑÑ‚Ğ¸Ğ³Ğ½Ñƒ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°ÑÑ‚Ğ¸ Ñ†Ñ– Ñ‚ĞµĞºÑÑ‚Ğ¸ Ğ·Ğ° 2 Ğ´Ğ½Ñ–.,<extra_id_0> Ñ Ğ½Ñ–Ñ…ÑƒÑ Ğ½Ğµ Ğ²ÑÑ‚Ğ¸Ğ³Ğ½Ñƒ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°ÑÑ‚Ğ¸ Ñ†Ñ– Ñ‚ĞµĞºÑÑ‚Ğ¸ Ğ·Ğ° 2 Ğ´Ğ½Ñ–.\n",
      "423,ru,\"Ñ…Ğ¾Ñ‚ÑŒ Ğ¸ Ğ´ĞµĞ±Ğ¸Ğ»ĞºĞ°, Ğ½Ğ¾ Ğ¾Ğ¾Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ°Ñ Ğ´ĞµĞ²ÑƒÑˆĞºĞ°!\",\"Ñ…Ğ¾Ñ‚ÑŒ Ğ¸ Ğ½Ğµ ÑƒĞ¼Ğ½Ğ°Ñ, Ğ½Ğ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ°Ñ Ğ´ĞµĞ²ÑƒÑˆĞºĞ°\",\"ç»ª Ğ¸ Ğ´ĞµĞ±Ğ¸Ğ»ĞºĞ°, Ğ½Ğ¾ Ğ¾Ğ¾Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ°Ñ Ğ´ĞµĞ²ÑƒÑˆĞºĞ°!\"\n",
      "424,de,#Rosenmontag in Mainz und NordafrikadÃ¼sseldorf abgesagt. Hat aber nichts mit #rapefugees zu tun.,#Rosenmontag in Mainz und NordafrikadÃ¼sseldorf abgesagt. Hat aber nichts mit GeflÃ¼chteten zu tun.,Interviews in Mainz und Mainz abgesagt Hat aber nichts mit Frauen zu tun\n",
      "425,ar,ÙƒØ³ Ø§Ù…ÙƒÙ… Ù…Ø§ÙÙŠÙ‡ Ø§Ø­Ø¯ ÙŠØ¬Ø§ÙˆØ¨Ù†ÙŠ,Ù…Ø§ÙÙŠÙ‡ Ø§Ø­Ø¯ ÙŠØ¬Ø§ÙˆØ¨Ù†ÙŠ,Ø±Ø¬Ù„ Ø§Ù…ÙƒÙ… Ù…Ø§ÙÙŠÙ‡ Ø§Ø­Ø¯Ø§ÙˆØ¨Ù†ÙŠ\n",
      "426,de,Ich schÃ¤me mich in solchem Verbrecherstaat mit deutschfeindlichen Ganoven-Richtern und korrupten Politikern zu leben!,Ich schÃ¤me mich in solchem Staat mit ungerechten Richtern und bestechlichen Politikern zu leben!,<extra_id_0> ich schÃ¤me mich in solchen Verbrecherstaat mit deutschfeindlichen Ganoven-Richtern und korrupten Politikern zu leben!\n",
      "427,hi,à¤…à¤°à¥‡ à¤¦à¥‡à¤–à¥‹ à¤¦à¥‡à¤–à¥‹ à¤•à¥Œà¤¨ à¤­à¥‹à¤‚à¤•à¥€ à¤“ à¤¹à¥‡à¤²à¥‹ à¤¤à¥‚ à¤¤à¥‡à¤°à¥€ à¤‰à¤‚à¤—à¤²à¤¿à¤¯à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¨à¤¹à¥€à¤‚ à¤•à¤¹à¥€ à¤”à¤° à¤¹à¥€ à¤šà¤²à¤¾,à¤¤à¥‚ à¤¤à¥‡à¤°à¥€ à¤‰à¤‚à¤—à¤²à¤¿à¤¯à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¨à¤¹à¥€à¤‚ à¤•à¤¹à¥€ à¤”à¤° à¤¹à¥€ à¤šà¤²à¤¾,à¤¤à¥‚ à¤¤à¥‡à¤°à¥€ à¤‰à¤‚à¤—à¤²à¤¿à¤¯à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¨à¤¹à¥€à¤‚ à¤•à¤¹à¥€ à¤”à¤° à¤¹à¥€ à¤šà¤²à¤¾\n",
      "428,hi,à¤¤à¥‚ à¤œà¥‡à¤à¤¨à¤¯à¥‚ à¤®à¥‡à¤‚ à¤à¤¾à¤‚à¤  à¤¨à¤¹à¥€à¤‚ à¤‰à¤–à¤¾à¤¡à¤¼à¤¤à¤¾ à¤­à¥‹à¤¸à¤¡à¥€à¤•à¥‡,à¤¤à¥‚ à¤œà¥‡à¤à¤¨à¤¯à¥‚ à¤®à¥‡à¤‚ à¤•à¥à¤› à¤¨à¤¹à¥€à¤‚ à¤•à¤° à¤ªà¤¾à¤¤à¤¾ à¤¸à¤®à¤à¥‡,ç»ª à¤œà¥‡à¤à¤¨à¤¯à¥‚ à¤®à¥‡à¤‚ à¤à¤¾à¤‚à¤  à¤¨à¤¹à¥€à¤‚ à¤‰à¤–à¤¾à¤¡à¤¼à¤¤à¤¾ à¤­à¥‹à¤¸à¤¡à¥€à¤•à¥‡\n",
      "429,en,Once again I'm SO glad that Wikipedia is not biased or filled with retarded children.,\"Once again, I'm so glad that Wikipedia is not biased or filled children.\",<extra_id_0> again I'm SO glad that Wikipedia is not biased or filled with retarded children.\n",
      "430,am,á‰ áŒá‹µ áŠ áˆ›áˆ«áŠ• áˆ˜áˆáˆ«á‰µ áŠ á‹­á‰»áˆ áŠ¥áŒƒá‰½áˆáŠ• X áŠ áŒ£áˆáˆ«á‰½áˆ á€áŒ‰áˆ«á‰½áˆáŠ• á‰°áˆ‹áŒ­á‰³á‰½áˆ áŠ¥áŠ•á‹°áŒ…áˆ 3 áŠ áˆ˜á‰µ áˆµá‰³áˆ‹á‹áŠ‘ áŒ€áŒáŠ“á‹ áŠ áˆ›áˆ« á‰ á‰€áŠ“á‰¶á‰½ áŠááŒ¡áŠ• á‹­á‹ áˆáˆ‰áŠ• áŠ¥á‹«áŒ‹á‹¨,á‰ áŒá‹µ áŠ áˆ›áˆ«áŠ• áˆ˜áˆáˆ«á‰µ áŠ á‹­á‰»áˆ áŠ¥áŒƒá‰½áˆáŠ• X áŠ áŒ£áˆáˆ«á‰½áˆ á€áŒ‰áˆ«á‰½áˆáŠ• á‰°áˆ‹áŒ­á‰³á‰½áˆ áŠ á‹­á‹°áˆáˆ áŠáŒˆáˆ©,<extra_id_0> á‰ áŒá‹µ áŠ áˆ›áˆ«áŠ• áˆ˜áˆáˆ«á‰µ áŠ á‹­á‰»áˆ áŠ¥áŒƒá‰½áˆáŠ• X áŠ áŒ£áˆáˆ«á‰½áˆ á€áŒ‰áˆ«á‰½áˆáŠ• á‰°áˆ‹áŒ­á‰³á‰½áˆ áŠ¥áŠ•á‹°áŒ…áˆ 3 áŠ áˆ˜á‰µ áˆµá‰³áˆ‹á‹áŠ‘\n",
      "431,am,á‰ á†á‰³á‹Š á‰µáŠ•áŠ®áˆ³ enjoy áŠ¨áˆšá‹«á‹°áˆ­áŒ‰á‰µ áŠ áŠ•á‹± áŠáˆ… áŠ¥áŠ•á‹´? áˆáŠ•áˆ á‹¨áˆšá‹«á‰ áˆ³áŒ­ áŠáŒˆáˆ­ áŠ áˆáˆ°áˆ©áˆ,á‰ á†á‰³á‹Š á‰µáŠ•áŠ®áˆ³ enjoy áŠ¥áŠ•á‹°áˆšá‹«á‹°áˆ­áŒ áˆ°á‹ á‰ á‹šáˆ… áŒ‰á‹³á‹­ áˆ˜á‰ áˆ³áŒ¨á‰µ áŠ á‹­áŒˆá‰£áˆ…áˆ áŠá‰ áˆ­,<extra_id_0> á‰ á†á‰³á‹Š á‰µáŠ•áŠ®áˆ³ enjoy áŠ¨áˆšá‹«á‹°áˆ­áŒ‰á‰µ áŠ áŠ•á‹± áŠáˆ… áŠ¥áŠ•á‹´? áˆáŠ•áˆ á‹¨áˆšá‹«á‰ áˆ‹áŒ­ áŠáŒˆáˆ­ áŠ áˆáˆ°áˆ©áˆ\n",
      "432,en,can t people just leave cuba alone so it can get it s feet under itself for number fuckin minutes ? .,Can people just leave Cuba to itself so it can get its feet under itself for number minutes?,<extra_id_0> can t people just leave cuba alone so it can get it s feet under itself for number fuckin minutes ?\n",
      "433,zh,ç¬¬ä¸€ä»£å¥³æ‹³æ˜¯å¢ƒå¤–åŠ¿åŠ›èµ„åŠ©ï¼Œå¾®åšé‚£ä¼šè¢«æŠ“çš„å…¨æ˜¯ç”·æ€§ï¼ŒçœŸè¦ä»¤æˆ‘ç¬‘æ­»,ç¬¬ä¸€ä»£æç«¯å¥³æƒä¸»ä¹‰è€…æ˜¯å¢ƒå¤–åŠ¿åŠ›èµ„åŠ©ï¼Œå¾®åšé‚£ä¼šè¢«æŠ“çš„å…¨æ˜¯ç”·æ€§ï¼ŒçœŸè¦ä»¤æˆ‘ç¬‘æ­»,\"ç¬¬ä¸€ä»£å¥³æ‹³æ˜¯å¢ƒå¤–åŠ¿åŠ›èµ„åŠ©,å¾®åšé‚£ä¼šè¢«æŠ“çš„å…¨æ˜¯ç”·æ€§,çœŸè¦ä»¤æˆ‘ç¬‘æ­»\"\n",
      "434,uk,\"Ğ’ÑĞµÑĞ²Ñ–Ñ‚ Ğ²Ñ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ½Ğµ Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ‚Ğ¸ Ñ…ÑƒĞ¹ Ğ½Ğ° Ñ‚Ğ¸Ñ…, Ñ…Ñ‚Ğ¾ Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ” Ğ¹Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¼ĞµĞ½Ğµ, Ğ° Ñ Ğ½Ñ–ÑĞº Ğ½Ğµ Ğ²Ğ¸Ğ²Ñ‡Ñƒ ÑƒÑ€Ğ¾Ğº...\",\"Ğ’ÑĞµÑĞ²Ñ–Ñ‚ Ğ²Ñ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ½Ğµ Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ‚Ğ¸ Ğ½Ğ° Ñ‚Ğ¸Ñ…, Ñ…Ñ‚Ğ¾ Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ” Ğ½Ğ° Ğ¼ĞµĞ½Ğµ, Ğ° Ñ Ğ½Ñ–ÑĞº Ğ½Ğµ Ğ²Ğ¸Ğ²Ñ‡Ñƒ ÑƒÑ€Ğ¾Ğº...\",\"<extra_id_0> Ğ²Ñ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ½Ğµ Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ‚Ğ¸ Ñ…ÑƒĞ¹ Ğ½Ğ° Ñ‚Ğ¸Ñ…, Ñ…Ñ‚Ğ¾ Ğ·Ğ°Ğ±Ğ¸Ğ²Ğ°Ñ” Ğ¹Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¼ĞµĞ½Ğµ...\"\n",
      "435,en,\"no , that s fucking bullshit .\",\"No, that is so wrong\",\", that s fucking bullshit .\"\n",
      "436,ru,ÑĞ¿ĞµÑ€Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ Ğ³Ğ´Ğµ Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ» Ğ³Ğ´Ğµ Ğ¿Ğ¸ÑĞ°Ğ» Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½ ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ñ‹Ğ¹ Ğ¸ÑˆĞ°ĞºĞ¸ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¸Ğ½,ÑĞ¿ĞµÑ€Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ Ğ³Ğ´Ğµ Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ» Ğ³Ğ´Ğµ Ğ¿Ğ¸ÑĞ°Ğ» Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¸Ğ½,Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½ ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ñ‹Ğ¹ Ğ¸ÑˆĞ°ĞºĞ¸ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ Ğ±Ğ»Ğ¸Ğ½\n",
      "437,es,\"Potencialmente SÃ¡nchez es el que mÃ¡s daÃ±o puede hacer, primer psicopata que llega a ser presidente en EspaÃ±a.\",\"Potencialmente SÃ¡nchez es el que mÃ¡s daÃ±o puede hacer, primer persona que llega a ser presidente en EspaÃ±a.\",\"<extra_id_0> es el que mÃ¡s daÃ±o puede hacer, primer psicopata que llega a ser presidente en EspaÃ±a.\"\n",
      "438,am,á‹›áˆ¬ á‹¨áŠ› áˆ°áˆáˆ­ áˆ˜á‰¥áˆ«á‰µ áˆƒá‹­áˆ á‰¥áˆ«á‰½ á‹¨áˆáˆ½á‰µ á‰°áˆ¨áŠ› áˆ°áˆ«á‰°áŠ›á‹ áˆ…áƒáŠ• áˆáŒáŠ• á‹­á‹ áŠá‹ áˆ˜áˆ°áˆˆáŠ áˆµáˆ« á‹¨áˆ„á‹°á‹ á£ áˆ˜á‰¥áˆ«á‰±áŠ• á‰¥áˆáŒ­ á‹µáˆ­áŒáˆ áŠ¥á‹«áˆ¨áŒˆá‹ áŠá‹á¢,á‹›áˆ¬ á‹¨áŠ› áˆ°áˆáˆ­ áˆ˜á‰¥áˆ«á‰µ  á‰¥áˆáŒ­ á‹µáˆ­áŒáˆ áˆ²áˆ áˆ…áƒáŠ• áˆáŒ… áŠ¥á‹«á‰ áˆ«áŠ“ áŠ¥á‹«áŒ á‹ á‹¨áˆšáŒ«á‹ˆá‰µá‰ á‰µ áŠá‹ á‹¨áˆšáˆ˜áˆµáˆˆá‹á¡á¡,<extra_id_0> á‹¨áŠ› áˆ°áˆáˆ­ áˆ˜á‰¥áˆ«á‰µ áˆƒá‹­áˆ á‰¥áˆ«á‰½ á‹¨áˆáˆ½á‰µ á‰°áˆ¨áŠ› áˆ°áˆ«á‰°áŠ›á‹ áˆ…áƒáŠ• áˆáŒáŠ• á‹­á‹ áŠá‹ áˆ˜áˆ°áˆˆáŠ áˆµáˆ« á‹¨áˆ„á‹°á‹ á£ áˆ˜á‰¥áˆ«á‰±áŠ• á‰¥áˆáŒ­ á‹µáˆ­áŒáˆ áŠ¥á‹«áˆ¨áŒˆá‹ áŠá‹á¢\n",
      "439,ar,ÙØ±ÙŠÙ‚ ØºØ¨ÙŠ Ø­ØªÙ‰ Ø§Ù„Ø¨Ù„Ù†ØªÙŠ Ø¬Ø§Ø¡ Ø¨Ø§Ù„ØºÙ„Ø·,ÙØ±ÙŠÙ‚ Ù…Ø¨ØªØ¯Ø¦ Ø­ØªÙ‰ Ø§Ù„Ø¨Ù„Ù†ØªÙŠ Ø¬Ø§Ø¡ Ø¨Ø§Ù„ØºÙ„Ø·,ÙØ±ÙŠÙ‚ ØºØ¨ÙŠ Ø­ØªÙ‰ Ø§Ù„Ø¨Ù„Ù†ØªÙŠ Ø¬Ø§Ø¡ Ø¨Ø§Ù„ØºÙ„Ø·\n",
      "440,ar,Ù„Ù† ÙŠØ´Ø§Ù‡Ø¯ÙˆØ§ Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø±Ø§Ø¡ Ø£Ø¨Ø¯Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.,Ù„Ù† Ù†Ø±ØºØ¨ ÙÙŠ Ù…Ø´Ø§Ù‡Ø¯Ø© Ø°Ù„Ùƒ Ù…Ø¬Ø¯Ø¯Ø§ Ø£Ø¨Ø¯Ø§,<extra_id_0> Ù„Ù† ÙŠØ´Ø§Ù‡Ø¯ÙˆØ§ Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø±Ø§Ø¡ Ø£Ø¨Ø¯Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰\n",
      "441,am,áŠ á‹²áˆµ áˆµá‹µá‰¥ áŠ áˆ˜áŠ•áŒª áŠ¥áŠ•áŒ‚ áŠ á‹²áˆµ áˆƒáˆ³á‰¥ áŠ áˆ˜áŠ•áŒª áŠ á‹­á‹°áˆˆáŠ­áˆ:: á‰ á‹°áˆ á‹¨áŒ¨á‰€á‹­áˆ… á‹áˆ¸á‰³áˆ áŠ áˆµáˆ˜áˆ³á‹­ áˆ°á‹ áŠáˆ…::,áŠ á‹²áˆµ á€á‹«á á‰ƒáˆ áŠ áˆ˜áŠ•áŒª áŠ¥áŠ•áŒ‚ áŠ á‹²áˆµ áˆƒáˆ³á‰¥ áŠ áˆ˜áŠ•áŒª áŠ á‹­á‹°áˆˆáŠ­áˆ:: á‰ áˆ°á‹ áŠááˆµ áŒ‰á‹³á‹­ áŠ¥áŒ…áˆ… á‹«áˆˆá‰ á‰µ áˆ°á‹ áŠáˆ…á¡á¡,<extra_id_0> á‰ á‹°áˆ áŒ¨á‰€á‹­áˆ… á‹áˆ¸á‰³áˆ áŠ áˆµáˆ˜áˆ³á‹­ áˆ°á‹ áŠáˆ…::\n",
      "442,es,ME JUEGAS PUTO RETO O COMO VA LA COSA PUTO RETRASADO,Me juegas al reto o cÃ³mo va la cosa,RETRO O COMO VA LA COSA PUTO RETRASADO\n",
      "443,ar,ÙŠØ§ Ø­Ù…Ø§Ø± Ù„Ù… Ù†ÙƒÙ† ÙŠÙˆÙ…Ø§ ØªØ­Øª Ø§Ù„Ø§Ø³ØªØ¹Ù…Ø§Ø± Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ ØŒ Ø¨Ù„ Ù‡Ø²Ù…Ù†Ø§Ù‡ ÙÙŠ Ù…Ø¹Ø±ÙƒØ© Ù‚Ø±Ø¨ ÙØ§Ø³â€¦,Ù„Ù… Ù†ÙƒÙ† ÙŠÙˆÙ…Ø§ ØªØ­Øª Ø§Ù„Ø§Ø³ØªØ¹Ù…Ø§Ø± Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ ØŒ Ø¨Ù„ Ù‡Ø²Ù…Ù†Ø§Ù‡ ÙÙŠ Ù…Ø¹Ø±ÙƒØ© Ù‚Ø±Ø¨ ÙØ§Ø³â€¦,<extra_id_0> Ù†ÙƒÙ† ÙŠÙˆÙ…Ø§ ØªØ­Øª Ø§Ù„Ø§Ø³ØªØ¹Ù…Ø§Ø± Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ ØŒ Ø¨Ù„ Ù‡Ø²Ù…Ù†Ø§Ù‡ ÙÙŠ Ù…Ø¹Ø±ÙƒØ© Ù‚Ø±Ø¨ ÙØ§Ø³...\n",
      "444,de,\"Hinweis: Wer wegen Nadalnys angeblicher Vergiftung nun Nordstream Gas ablehnt ,muss wegen dem tatsÃ¤chlich getÃ¶teten Kashoggi auf das Saudi Ã–l verzichten Ihr Heuchler. #staatsTV\",\"Hinweis: Wer wegen Nadalnys angeblicher Vergiftung nun Nordstream Gas ablehnt ,muss wegen dem tatsÃ¤chlich getÃ¶teten Kashoggi auf das Saudi Ã–l verzichten. #staatsTV\",\"Hinweis: Wer wegen Nadalnys angeblicher Vergiftung nun Nordstream Gas ablehnt,muss wegen dem tatsÃ¤chlich getÃ¶teten Kashoggi auf das Saudi Ã–l verzichten Ihr Heuchler #staatsTV\"\n",
      "445,uk,Ğ±Ğ»ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ¾ Ğ¶Ğ¸Ğ·Ğ° Ñ Ğ² ÑˆĞ¾Ñ†Ñ–,Ğ¾Ñ‚Ğ¾ Ğ¶Ğ¸Ğ·Ğ° Ñ Ğ² ÑˆĞ¾Ñ†Ñ–,Ğ¾Ñ‚Ğ¾ Ğ¶Ğ¸Ğ·Ğ° Ñ Ğ² ÑˆĞ¾Ñ†Ñ–\n",
      "446,ru,Ğ²Ğ¾ Ğ´ĞµĞ±Ğ¸Ğ» Ñ‚Ñ‹ Ğ¿Ğ°Ğ½-Ğ±Ğ°Ñ€Ğ°Ğ½))))))))))))))))))). ĞµĞ²Ñ€ĞµĞ¹ÑĞºĞ¾Ğµ Ñ‚Ğ² Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ» Ğ´Ğ°ÑƒĞ½???)))),ĞµĞ²Ñ€ĞµĞ¹ÑĞºĞ¾Ğµ Ñ‚Ğ² Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ»???,ĞµĞ²Ñ€ĞµĞ¹ÑĞºĞ¾Ğµ Ñ‚Ğ² Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ» Ğ´Ğ°ÑƒĞ½???))))\n",
      "447,de,Was erwarten Sie von Rothschild Sklaven die ihre Seele dem Satan verkauf haben!,Was erwarten Sie von Rothschild AnhÃ¤ngern die ihre Interessen anderen Ã¼berlassen!,<extra_id_0> die ihre Seele dem Satan verkauf haben! Was erwarten Sie von Rothschild Sklaven die ihre Seele dem Satan verkauf haben!\n",
      "448,uk,\"Ğ°Ğ»Ğµ ÑĞ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ Ñ‚Ğ¾ Ğ±ÑƒĞ»Ğ¾ Ğ½Ğµ Ğ²ÑĞµ Ğ¾Ğ´Ğ½Ğ¾, Ğ·Ğ½Ğ°Ñ‡ Ñ‚Ğ¸ Ğ½Ğ°Ğ¼Ğ°Ğ³Ğ°Ğ²ÑÑ Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚Ğ¸ Ñ…ÑƒĞ¹, Ğ° Ñ†Ğµ Ğ²Ğ¶Ğµ ÑˆĞ¾ÑÑŒ\",\"Ğ°Ğ»Ğµ ÑĞ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ Ñ‚Ğ¾ Ğ±ÑƒĞ»Ğ¾ Ğ½Ğµ Ğ²ÑĞµ Ğ¾Ğ´Ğ½Ğ¾, Ğ·Ğ½Ğ°Ñ‡ Ñ‚Ğ¸ Ğ½Ğ°Ğ¼Ğ°Ğ³Ğ°Ğ²ÑÑ Ñ‰Ğ¾ÑÑŒ Ñ€Ğ¾Ğ±Ğ¸Ñ‚Ğ¸, Ğ° Ñ†Ğµ Ğ²Ğ¶Ğµ ÑˆĞ¾ÑÑŒ\",\"<extra_id_0> Ñ‚Ğ¸ Ğ½Ğ°Ğ¼Ğ°Ğ³Ğ°Ğ²ÑÑ Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚Ğ¸ Ñ…ÑƒĞ¹, Ğ° Ñ†Ğµ Ğ²Ğ¶Ğµ ÑˆĞ¾ÑÑŒ\"\n",
      "449,en,\"HEY CRETIN, LOOK UP YOUR HISTORY!\",\"Hey, sir, look up your history.\",\"HEY CRETIN, LOOK UP YOUR HISTORY!\"\n",
      "450,hi,à¤¯à¥‡ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤®à¤ à¤•à¥‡ à¤¬à¤¾à¤¹à¤° à¤¹à¥ˆ à¤¤à¥à¤® à¤°à¤‚à¤¡à¥€ à¤°à¥‹à¤“ à¤‡à¤§à¤° à¤®à¤¤ à¤˜à¥à¤¸à¥‹à¥¤,à¤¯à¥‡ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤®à¤ à¤•à¥‡ à¤¬à¤¾à¤¹à¤° à¤¹à¥ˆ à¤¤à¥à¤® à¤¬à¥‡à¤•à¤¾à¤° à¤•à¥‡ à¤¬à¤¹à¤¾à¤¨à¥‡ à¤•à¤°à¥‹ à¤‡à¤§à¤° à¤®à¤¤ à¤˜à¥à¤¸à¥‹à¥¤,<extra_id_0> à¤¯à¥‡ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤®à¤ à¤•à¥‡ à¤¬à¤¾à¤¹à¤° à¤¹à¥ˆ à¤¤à¥à¤® à¤°à¤‚à¤¡à¥€ à¤°à¥‹à¤“ à¤‡à¤§à¤° à¤®à¤¤ à¤˜à¥à¤¸à¥‹à¥¤\n",
      "451,ar,Ø£Ù†Øª Ù…Ø®ØªÙ„ Ø¹Ù‚Ù„Ø¨Ø§ ØŸ,Ø£Ù„ÙŠØ³ Ù„Ø¯ÙŠÙƒ Ø¹Ù‚Ù„ØŸ,<extra_id_0> Ø£Ù†Øª Ù…Ø®ØªÙ„ Ø¹Ù‚Ù„Ø¨Ø§ ØŸ\n",
      "452,es,Pero el calentamiento global son COSAS DE HIPPIES Y PERROFLAUTAS,Pero el calentamiento global son COSAS DE HIPPIES,calentamiento global son COSAS DE HIPPIES Y PERROFLAUTAS\n",
      "453,en,maybe now no one will be rammed in the ass .,maybe now no one will be treated badly,<extra_id_0> now no one will be rammed in the ass .\n",
      "454,am,ááˆ­á‹± á‰ á‰°áˆ˜á‹°á‰ á‹ áŒŠá‹œ á‹­á‹°áˆ¨áŒ‹áˆ á¡á¡ á‹°áŠá‹á‹ áˆ˜áˆ†áŠ• á‰µá‹•áŒáˆµá‰µáˆ…áŠ• áŠ á‰áˆ,ááˆ­á‹± á‰ á‰°áˆ˜á‹°á‰ á‹ áŒŠá‹œ á‹­á‹°áˆ¨áŒ‹áˆ,ááˆ­á‹± á‰ á‰°áˆ˜á‹°á‰ á‹ áŒŠá‹œ á‹­á‹°áˆ¨áŒ‹áˆ á¡á¡ á‹°áŠá‹á‹ áˆ˜áˆ†áŠ• á‰µá‹•áŒáˆµá‰µáˆ…áŠ• áŠ á‰áˆ\n",
      "455,hi,à¤­à¥‹à¤¸à¤¡à¤¼à¥€à¤•à¥‡ à¤¯à¤¹à¤¾à¤‚ à¤†à¤•à¤° à¤¬à¤¾à¤¤ à¤•à¤° à¤¤à¥à¤à¥‡ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤‚à¤—à¥‡à¥¤ ,à¤¯à¤¹à¤¾à¤‚ à¤†à¤•à¤° à¤¬à¤¾à¤¤ à¤•à¤° à¤¤à¥à¤à¥‡ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤‚à¤—à¥‡à¥¤ ,à¤¯à¤¹à¤¾à¤‚ à¤†à¤•à¤° à¤¬à¤¾à¤¤ à¤•à¤° à¤¤à¥à¤à¥‡ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤‚à¤—à¥‡à¥¤\n",
      "456,zh,è¿è¿™ä¸ªæ˜¯è½¬å‘è´´éƒ½çœ‹ä¸å‡ºæ¥åªè¦æœ‰äººéª‚ä»–ä»–å°±è¯´äººå®¶æ˜¯åŸºä½¬å˜,è¿è¿™ä¸ªæ˜¯è½¬å‘è´´éƒ½çœ‹ä¸å‡ºæ¥åªè¦æœ‰äººéª‚ä»–ä»–å°±è¯´äººå®¶æ˜¯ç”·åŒæ€§æ‹å˜,è¿è¿™ä¸ªæ˜¯è½¬å‘è´´éƒ½çœ‹ä¸å‡ºæ¥åªè¦æœ‰äººéª‚ä»–ä»–å°±è¯´äººå®¶æ˜¯åŸºä½¬å˜\n",
      "457,uk,\"Ğ”ÑĞºÑƒÑ Ğ²ÑÑ–Ğ¼, Ñ…Ñ‚Ğ¾ Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ² ĞºÑƒĞ»Ğ°ĞºĞ¸, Ğ°Ğ»Ğµ ÑĞ¿Ñ–Ğ²Ğ±ĞµÑÑ–Ğ´Ğ° Ğ½Ñ” ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ Ñ– Ğ¼ĞµĞ½Ğµ Ğ¿Ğ¾ÑĞ»Ğ°Ğ»Ğ¸ Ğ½Ğ°Ñ…Ñ”Ñ€.\",\"Ğ”ÑĞºÑƒÑ Ğ²ÑÑ–Ğ¼, Ñ…Ñ‚Ğ¾ Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒĞ²Ğ°Ğ² Ğ¼ĞµĞ½Ğµ, Ğ°Ğ»Ğµ ÑĞ¿Ñ–Ğ²Ğ±ĞµÑÑ–Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¹ÑˆĞ»Ğ° Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¾\",\"<extra_id_0>, Ñ…Ñ‚Ğ¾ Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ² ĞºÑƒĞ»Ğ°ĞºĞ¸, Ğ°Ğ»Ğµ ÑĞ¿Ñ–Ğ²Ğ±ĞµÑÑ–Ğ´Ğ° ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ Ñ– Ğ¼ĞµĞ½Ğµ Ğ¿Ğ¾ÑĞ»Ğ°Ğ»Ğ¸ Ğ½Ğ°Ñ…Ñ”Ñ€.\"\n",
      "458,hi,à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤¾à¤¥ à¤¸à¤¾à¤°à¥‡ à¤µà¤¿à¤ªà¤•à¥à¤·à¥€ à¤­à¤¡à¤¼à¤µà¤¾ à¤¡à¤¾à¤‚à¤¸ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥‹à¤¤à¥‡à¥¤,à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤¾à¤¥ à¤¸à¤¾à¤°à¥‡ à¤µà¤¿à¤ªà¤•à¥à¤·à¥€ à¤¬à¥‡à¤•à¤¾à¤° à¤•à¥€ à¤¬à¤¾à¤¤à¥‡à¤‚ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥‹à¤¤à¥‡ |,à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤¾à¤¥ à¤¸à¤¾à¤°à¥‡ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿ à¤­à¤¡à¤¼à¤µà¤¾ à¤¡à¤¾à¤‚à¤¸ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥‹à¤¤à¥‡\n",
      "459,uk,Ğ‘Ğ°Ñ‚ÑÑˆĞºĞ° ÑĞºĞ°Ğ·Ğ°Ğ² ÑˆĞ¾ Ğ¼Ğ¸Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚-Ñ†Ğµ Ğ½Ğ°Ñˆ Ğ¿Ğ°ÑÑ‚Ğ¸Ñ€ Ğ´Ğ¾ Ğ¡Ğ¿Ğ°ÑÑ–Ğ½Ğ½Ñ Ğ° Ğ¼Ğ¸Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ñ–Ñ‚ĞµĞ½-Ñ†Ğµ ÑĞºĞ°ÑÑŒ Ñ…ÑƒĞ¹Ğ½Ñ Ğ· Ñ€ĞµĞ»ÑŒÑĞ°Ğ¼Ğ¸ ÑĞºĞ° Ğ²ĞµĞ´Ğµ Ñƒ Ğ¢ĞµÑ€ĞµĞ¼ĞºĞ¸,Ğ‘Ğ°Ñ‚ÑÑˆĞºĞ° ÑĞºĞ°Ğ·Ğ°Ğ² ÑˆĞ¾ Ğ¼Ğ¸Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚ Ñ†Ğµ Ğ½Ğ°Ñˆ Ğ¿Ğ°ÑÑ‚Ğ¸Ñ€ Ğ´Ğ¾ ÑĞ¿Ğ°ÑÑ–Ğ½Ğ½Ñ Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ñ–Ñ‚ĞµĞ½ Ñ†Ğµ ÑĞºĞ°ÑÑŒ ÑˆÑ‚ÑƒĞºĞ° Ğ· Ñ€ĞµĞ»ÑŒÑĞ°Ğ¼Ğ¸ ÑĞºĞ° Ğ²ĞµĞ´Ğµ Ñƒ Ñ‚ĞµÑ€ĞµĞ¼ĞºĞ¸,<extra_id_0> ÑĞºĞ°Ğ·Ğ°Ğ² ÑˆĞ¾ Ğ¼Ğ¸Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚-Ñ†Ğµ ÑĞºĞ°ÑÑŒ Ñ…ÑƒĞ¹Ğ½Ñ Ğ· Ñ€ĞµĞ»ÑŒÑĞ°Ğ¼Ğ¸ ÑĞºĞ° Ğ²ĞµĞ´Ğµ Ñƒ Ğ¢ĞµÑ€ĞµĞ¼ĞºĞ¸\n",
      "460,en,come on ya fuckin dummy get your right step right .,\"can you please get your steps, right?\",<extra_id_0> come on ya fuckin dummy get your right step right\n",
      "461,es,\"Otro dÃ­a de mierda, en Ã©ste mes de mierda, de Ã©ste aÃ±o de mierda\",\"Otro dÃ­a malo, en Ã©ste mes malo, de Ã©ste aÃ±o malo\",\"Otro dÃ­a de mierda, en Ã©ste mes de mierda, de Ã©ste aÃ±o de mierda\"\n",
      "462,ar,Ù…Ø§Ø´ÙŠ ÙŠØ§ Ø§Ù‚Ø±Ø¹ Ù…Ù‚Ø¨ÙˆÙ„Ù‡ Ù…Ù†Ùƒ,Ù…Ø§Ø´ÙŠ Ù…Ù‚Ø¨ÙˆÙ„Ù‡ Ù…Ù†Ùƒ,<extra_id_0> Ù…Ø§Ø´ÙŠ Ø§Ù‚Ø§Ø±Ø¨ Ù…Ù†Ùƒ\n",
      "463,en,harper needs to get the fuck out of office .,Harper needs to get out of office,<extra_id_0> needs to get the fuck out of office .\n",
      "464,ru,Ğ¼Ğ¾Ğ»Ğ¾Ğ´ĞµÑ†!!! Ğ¾Ğ±ĞµĞ·ÑŒÑĞ½Ğ°Ğ¼ Ğ¿Ğ¸Ğ·Ğ´ÑĞ»ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑĞ¸Ğ»Ğ°.Ğ° Ñ‚Ğ¾ Ñ‡Ñ‚Ğ¾ ÑÑƒĞ´ÑÑ‚ Ñ‚Ğ¾ ÑÑ‚Ğ¾ ĞµĞ±Ğ°Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ĞµÑ€Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ.,\"ĞœĞ¾Ğ»Ğ¾Ğ´ĞµÑ†!!! ĞĞ°ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑ‚Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹. Ğ Ñ‚Ğ¾ Ñ‡Ñ‚Ğ¾ ÑÑƒĞ´ÑÑ‚, Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ»ĞµÑ€Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ\",Ğ»ÑĞ½Ğ°Ğ¼ Ğ¿Ğ¸Ğ·Ğ´ÑĞ»ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑĞ¸Ğ»Ğ°.Ğ° Ñ‚Ğ¾ Ñ‡Ñ‚Ğ¾ ÑÑƒĞ´ÑÑ‚ Ñ‚Ğ¾ ÑÑ‚Ğ¾ ĞµĞ±Ğ°Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ĞµÑ€Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ.\n",
      "465,es,Que arrechera esta economÃ­a de mierda,Que pena esta economÃ­a tan dÃ©bil,<extra_id_0> que arrechera esta economÃ­a de mierda\n",
      "466,uk,\"Ğ¥Ğ¾Ñ‡ĞµÑˆ Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¸Ğ¼ @ ÑƒÑÑ–Ğ¼ Ğ½Ğ° Ñ‚ĞµĞ±Ğµ Ğ¿Ğ¾Ñ…ÑƒĞ¹ @ Ğ»ÑĞ´Ğ¸ Ğ´Ğ¸Ğ²ÑƒÑÑ‚ÑŒÑÑ \"\"Ğ ÑˆĞ¾ Ğ¶ Ñ†Ğµ Ñ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğ¹ Ğ·Ğ»Ğ¸Ğ¹?\"\".\",\"Ğ¥Ğ¾Ñ‡ĞµÑˆ Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¸Ğ¼ @ ÑƒÑÑ–Ğ¼ Ğ½Ğ° Ñ‚ĞµĞ±Ğµ Ğ±Ğ°Ğ¹Ğ´ÑƒĞ¶Ğµ@ Ğ»ÑĞ´Ğ¸ Ğ´Ğ¸Ğ²ÑƒÑÑ‚ÑŒÑÑ \"\"Ğ ÑˆĞ¾ Ğ¶ Ñ†Ğµ Ñ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğ¹ Ğ·Ğ»Ğ¸Ğ¹?\"\"\",\"Ñ…Ğ¾Ñ‡ĞµÑˆ Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¸Ğ¼ @ ÑƒÑÑ–Ğ¼ Ğ½Ğ° Ñ‚ĞµĞ±Ğµ Ğ¿Ğ¾Ñ…ÑƒĞ¹ @ Ğ»ÑĞ´Ğ¸ Ğ´Ğ¸Ğ²ÑƒÑÑ‚ÑŒÑÑ \"\"Ğ ÑˆĞ¾ Ğ¶ Ñ†Ğµ Ñ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğ¹ Ğ·Ğ»Ğ¸Ğ¹?\"\".\"\n",
      "467,hi,à¤…à¤—à¤° à¤¯à¥‡ à¤²à¤•à¥à¤·à¤£ à¤®à¤¹à¤¸à¥‚à¤¸ à¤¹à¥‹ à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚ à¤¤à¥‹ à¤¤à¥à¤°à¤‚à¤¤ à¤®à¥‹à¤¦à¥€_à¤­à¤•à¥à¤¤à¤¿ à¤›à¥‹à¤¡à¤¼ à¤¦à¥‹ à¤šà¥à¤¤à¤¿à¤¯à¥‹,à¤…à¤—à¤° à¤¯à¥‡ à¤²à¤•à¥à¤·à¤£ à¤®à¤¹à¤¸à¥‚à¤¸ à¤¹à¥‹ à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚ à¤¤à¥‹ à¤¤à¥à¤°à¤‚à¤¤ à¤®à¥‹à¤¦à¥€_à¤­à¤•à¥à¤¤à¤¿ à¤›à¥‹à¤¡à¤¼ à¤¦à¥‹,<extra_id_0> à¤…à¤—à¤° à¤¯à¥‡ à¤²à¤•à¥à¤·à¤£ à¤®à¤¹à¤¸à¥‚à¤¸ à¤¹à¥‹ à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚ à¤¤à¥‹ à¤¤à¥à¤°à¤‚à¤¤ à¤®à¥‹à¤¦à¥€_à¤­à¤•à¥à¤¤à¤¿ à¤›à¥‹à¤¡à¤¼ à¤¦à¥‹ à¤šà¥à¤¤à¤¿à¤¯à¥‹\n",
      "468,am,á‰¦á‰…á‰§á‰ƒ áŠ¥áŠ•á‹°áŠ áŠ•á‰° áŠ á‹­áŠá‰±áŠ• á‰³á‹µá‹®áˆµ á‰³áŠ•á‰± á‰ áˆ˜á‹µáˆ¨áŠ­ áŠ áˆµáŠáŒ¥áˆ¶ á‹­á‹˜áˆ­áˆ«áˆá¢ áŠ¥á‹šáˆ… áŠ¥áŠ•á‹´á‰µ áŠ¥áŠ•á‹°á‹°áˆ¨áˆµáŠ­ á‹¨áŠ«á‹µáŠ­ á‹áˆˆá‰³ á‰¢áˆµ áˆ˜áˆ†áŠ•áˆ…áŠ• á‹¨áˆšá‹«áˆ³á‹¨,áŠ áŠ•á‰°áŠ• á‰³á‹µá‹®áˆµ á‰³áŠ•á‰± á‰ áˆ˜á‹µáˆ¨áŠ­ á‹¨áˆ€áˆ³á‰¥ ááŒ­á‰µ á‰ á‹°áŠ•á‰¥ á‹«áˆ³áˆáŠ“áˆá¢ á‹›áˆ¬ áˆ‹á‹­ áŠ¥áŠ•á‹´á‰µ áŠ¥áŠ•á‹°á‹°áˆ¨áˆµáŠ­ áˆ³á‰³áˆáŠ• áˆ˜áˆáŒ£á‰µáˆ…áŠ•,á‰¦á‰…á‰§á‰ƒ áŠ¥áŠ•á‹°áŠ áŠ•á‰° áŠ á‹­áŠá‰±áŠ• á‰³á‹µá‹®áˆµ á‰³áŠ•á‰± á‰ áˆ˜á‹µáˆ¨áŠ­ áŠ áˆµáŠáŒ¥áˆ¶ á‹­á‹˜áˆ­áˆ«áˆá¢\n",
      "469,hi,à¤²à¤—à¤¤à¤¾ à¤¸à¤¾à¤°à¥‡ à¤¬à¥à¤¦à¥à¤§à¤œà¥€à¤µà¥€ à¤ªà¤ªà¥à¤ªà¥‚ à¤®à¥‚à¤¤à¥à¤° à¤ªà¥€ à¤•à¥‡ à¤¸à¥‹ à¤—à¤ ,à¤²à¤—à¤¤à¤¾ à¤¸à¤¾à¤°à¥‡ à¤¬à¥à¤¦à¥à¤§à¤œà¥€à¤µà¥€ à¤…à¤ªà¤¨à¥‡ à¤•à¥€ à¤¸à¥‹à¤šà¤¨à¥‡ à¤•à¥€ à¤¶à¤•à¥à¤¤à¤¿ à¤–à¥‹ à¤šà¥à¤•à¥‡ à¤¹à¥ˆ,à¤¬à¤¿à¤¨à¤¾ à¤ à¥€à¤• à¤¹à¥ˆ\n",
      "470,de,haha sucht man schon Ausreden fÃ¼r die vÃ¶llig Dumme Berichterstattung der letzten Wochen...?,haha sucht man schon Ausreden fÃ¼r die Berichterstattung der letzten Wochen...?,<extra_id_0> sucht man schon Ausreden fÃ¼r die vÃ¶llige Dumme Berichterstattung der letzten Wochen...?\n",
      "471,uk,\"Ğ”Ğ¸Ğ²Ğ»ÑÑÑŒ Ğ½Ğ° Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ³Ğ¾Ğ»Ğ¾ÑÑƒĞ²Ğ°Ğ½ÑŒ Ñ– Ğ±Ğ»Ñ, who the fuck Ñ‚Ğ°Ğº Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¾ Ğ¼ĞµĞ½Ğµ Ğ·Ğ½Ğ°Ñ”?\",\"Ğ”Ğ¸Ğ²Ğ»ÑÑÑŒ Ğ½Ğ° Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ³Ğ¾Ğ»Ğ¾ÑÑƒĞ²Ğ°Ğ½ÑŒ Ñ–, Ñ…Ñ‚Ğ¾ Ñ‚Ğ°Ğº Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¾ Ğ¼ĞµĞ½Ğµ Ğ·Ğ½Ğ°Ñ”?\",\"Ğ±Ğ»Ñ, who the fuck Ñ‚Ğ°Ğº Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¾ Ğ¼ĞµĞ½Ğµ Ğ·Ğ½Ğ°Ñ”?\"\n",
      "472,am,á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰° áŒáŠ• á‹°áˆá‰·á‰½áˆ áˆ½áˆ­áˆ­áˆ­ á‰µáˆ‹áˆ‹á‰½áˆ á‹­áŒˆáˆ­áˆ›áŠ• á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ áŠ¢á‰µá‹®áŒ²á‹«á‹Šá‹«áŠ• áŠ¥áˆ­á‰ƒáŠ“á‰¼á‹áŠ• á‰ á‰¥áˆ­á‹µ á‰ áˆ™á‰€á‰µ á‰ áˆ,á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ áŠ¢á‰µá‹®áŒ²á‹«á‹Šá‹«áŠ• áŠ¥á‹¨á‰°áŒ‰á‹± áŠ¥áŠ“áŠ•á‰° áŒáŠ• á‰ áˆ³á‹á‹µ á‹­áˆ…áŠ• á‰ áˆ›á‹«áˆ³á‹­ áˆ†áŠ”á‰³ áŠá‹ á‹¨á‰³á‹«á‰½áˆá‰µá¡á¡,<extra_id_0> á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰° áŒáŠ• á‹°áˆá‰·á‰½áˆ áˆ½áˆ­áˆ­áˆ­ á‰µáˆ‹áˆ‹á‰½áˆ á‹­áŒˆáˆ­áˆ›áŠ• á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰° áŒáŠ• á‹°áˆá‰·á‰½áˆ áˆ½áˆ­áˆ­áˆ­ á‰µáˆ‹áˆ‹á‰½áˆ á‹­áŒˆáˆ­áˆ›áŠ• á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰° áŒáŠ• á‹°áˆá‰·á‰½áˆ áˆ½áˆ­áˆ­áˆ­ á‰µáˆ‹áˆ‹á‰½áˆ á‹­áŒˆáˆ­áˆ›áŠ• á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰° áŒáŠ• á‹°áˆá‰·á‰½áˆ áˆ½áˆ­áˆ­áˆ­ á‰µáˆ‹áˆ‹á‰½áˆ á‹­áŒˆáˆ­áˆ›áŠ• á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰° áŒáŠ• á‹°áˆá‰·á‰½áˆ áˆ½áˆ­áˆ­áˆ­ á‰µáˆ‹áˆ‹á‰½áˆ á‹­áŒˆáˆ­áˆ›áŠ• á‰ áˆ˜á‰¶ áˆº áŠ¥áˆšá‰†áŒ áˆ­ á‹¨áŠ¢á‰µá‹®áŒ²á‹«áŠ• áˆµá‹°á‰°áŠá‰½ á‰ áˆµáˆ­ áŠ¥á‹¨á‰°áˆ°á‰ƒá‹© á‰ áˆ³á‹á‹µ áŠ¥áŠ“á‰°\n",
      "473,en,military can be very stupid too .,Military can make a mistake.,<extra_id_0> military can be very stupid too\n",
      "474,ru,Ğ·Ğ¸Ğ·Ñƒ Ğ¿ÑƒÑÑ‚ÑŒ Ğ¼Ğ¾Ğ½Ğ°ĞºĞ¾ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ñ€ÑĞµĞ»ÑŒ ÑĞ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ¾Ğ¼...Ğ¿Ğ¸Ğ·Ğ´ĞµÑˆÑŒ Ñ€ĞµĞ±ÑÑ‚..,Ğ·Ğ¸Ğ·Ñƒ Ğ¿ÑƒÑÑ‚ÑŒ Ğ¼Ğ¾Ğ½Ğ°ĞºĞ¾ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ñ€ÑĞµĞ»ÑŒ ÑĞ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ¾Ğ¼...Ğ²Ñ€Ğ°Ğ½ÑŒĞµ Ñ€ĞµĞ±ÑÑ‚..,\"Ğ»ÑĞ½Ğ¸ÑˆÑŒ Ğ¸ ÑĞ´ĞµĞ»Ğ°ĞµÑˆÑŒ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ»ÑŒĞ·Ñ.\"\n",
      "475,zh,å–èˆå¾—é…’ï¼Œåšæ™®ä¿¡ç”·ï¼Œæ²¡æ¯›ç—…,å–èˆå¾—é…’ï¼Œåšæ²¡æœ‰è‡ªçŸ¥ä¹‹æ˜çš„ç”·æ€§ï¼Œæ²¡æ¯›ç—…,\"å–èˆå¾—é…’,åšæ™®ä¿¡ç”·,æ²¡æ¯›ç—…\"\n",
      "476,am,áˆ˜á…áˆ€á áˆ›áˆˆá‰µ áŠ áŠ•á‰º á‰°áŒ‹á‹µáˆ˜áˆ½ áŠ¥áŠ•á‹³áˆ®áŒŒ áŠ¥áˆ­áŒá‰¥ áŠ áˆáŒ‹áˆ‹á‹­ á‹¨áˆá‰³áŒáˆ‹áŒ‚áŒ‚á‰ á‰µ áˆ›áˆˆá‰µ áˆ³á‹­áˆ†áŠ• áŠ¥áŠ•á‹° á‰µáŒ‰áˆ… áŠ•á‰¥ á‰ áŠ•á‰ƒá‰µ áŠ¥á‹ˆá‰€á‰µ á‹¨áˆšáˆ˜áŒ áŒ¥á‰ á‰µ áŠá‹á¢,áˆ˜á…áˆ€á áˆ›áˆˆá‰µ áŠ áŠ•á‰º áŠ¥áŠ•á‹° áŠáŒˆáˆ© áŒ‹á‹°áˆ á‰¥áˆˆáˆ½  á‹áˆá‰¥áˆ á‹¨áˆšáŠá‰ á‰¥ áˆ³á‹­áˆ†áŠ• áŠ¥áŠ•á‹° á‰µáŒ‰áˆ… áŠ•á‰¥ á‰ áŠ•á‰ƒá‰µ áŠ¥á‹ˆá‰€á‰µ á‹¨áˆšáˆ˜áŒ áŒ¥á‰ á‰µ áŠá‹á¢,<extra_id_0> áŠ áŠ•á‰º á‰°áŒ‹á‹µáˆ˜áˆ½ áŠ¥áŠ•á‹³áˆ®áŒŒ áŠ¥áˆ­áŒá‰¥ áŠ áˆáŒ‹áˆ‹á‹­ á‹¨áˆá‰³áŒáˆ‹áŒ‚áŒ‚á‰ á‰µ áˆ›áˆˆá‰µ áˆ³á‹­áˆ†áŠ• áŠ¥áŠ•á‹° á‰µáŒ‰áˆ… áŠ•á‰¥ á‰ áŠ•á‰ƒá‰µ áŠ¥á‹ˆá‰€á‰µ á‹¨áˆšáˆ˜áŒ áŒ¥á‰ á‰µ áŠá‹á¢\n",
      "477,zh,ä¹Ÿå°±æ˜¯æœ‰é’±çš„é—²è›‹æ‰éœ€è¦ã€‚,ä¹Ÿå°±æ˜¯æœ‰é’±çš„é—²äººæ‰éœ€è¦ã€‚,ä¹Ÿå°±æ˜¯æœ‰é’±çš„é—²è›‹æ‰éœ€è¦ã€‚\n",
      "478,am,áˆ˜áˆµáˆªá‹« á‰¤á‰µ áˆµáˆˆ áŠ”á‹® áˆŠá‰¥áˆ«áˆŠá‹áˆ áˆ°á‹­áŒ£áŠ•áŠá‰µ áˆµáŠ•á‰µ áŠ¥áŠ•á‹³áˆ‹áˆµá‰°áˆ›áˆ©áŠ• ??,áˆ˜áˆµáˆªá‹« á‰¤á‰µ áˆµáˆˆ áŠ”á‹® áˆŠá‰¥áˆ«áˆŠá‹áˆ áˆ˜áŒ¥ááŠá‰µ áˆµáŠ•á‰µ áŠ¥áŠ•á‹³áˆ‹áˆµá‰°áˆ›áˆ©áŠ• ??,áŠ”á‹® áˆŠá‰¥áˆ«áˆŠá‹áˆ áˆ°á‹­áŒ£áŠ•áŠá‰µ áˆµáŠ•á‰µ áŠ¥áŠ•á‹³áˆ‹áˆµá‰°áˆ›áˆ©áŠ• ??\n",
      "479,de,\"Das einzige was @USER_1 bereut bedauert, dass er erwischt wurde.. Dieser PÃ¤dophile gehÃ¶rt eindeutig entsorgt...samt seiner GrÃ¼nen Brut!!\",\"Das einzige was @USER_1 bereut bedauert, dass er erwischt wurde.. Dieser PÃ¤dophile gehÃ¶rt eindeutig abgewÃ¤hlt...samt seiner GrÃ¼nen Brut!!\",<extra_id_0> er erwischt wurde.. Dieser PÃ¤dophile gehÃ¶rt eindeutig entsorgt...samt seiner GrÃ¼nen Brut!!\n",
      "480,hi,\"à¤­à¥‹à¤¸à¤¡à¥€à¤•à¥‡, à¤à¤• à¤¶à¤¬à¥à¤¦ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¬à¥‹à¤²à¤¾ à¤—à¤¯à¤¾ à¤†à¤¤à¥à¤® à¤®à¥à¤—à¥à¤§ à¤¬à¥Œà¤¨à¥‡à¥¤\",à¤à¤• à¤¶à¤¬à¥à¤¦ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¬à¥‹à¤²à¤¾ à¤—à¤¯à¤¾ à¤†à¤¤à¥à¤® à¤®à¥à¤—à¥à¤§ à¤‡à¤‚à¤¸à¤¾à¤¨à¥¤,à¤¬à¤¿à¤¨à¤¾ à¤¶à¤¬à¥à¤¦ à¤­à¥€ à¤¬à¥‹à¤²à¤¾\n",
      "481,uk,\"Ğ·Ğ° Ñ‚Ğ¸Ğ¼ ÑĞ°Ğ¼Ğ¸Ğ¼ Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾ĞµÑ‚Ñ–Ğ² Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»Ñ– Ğ¼Ğ¸Ñ‚Ñ†Ñ–Ğ² - Ğ½Ğ°Ñ…ÑƒĞ¹ Ğ· Ğ¿Ğ»ÑĞ¶Ğ°, Ğ²Ğ¸Ğ³Ğ½Ğ°Ñ‚Ğ¸ Ğ·Ğ° Ğ¼ĞµĞ¶Ñ– Ğ”ĞµÑ€Ğ¶Ğ°Ğ²Ğ¸ ĞŸĞ»Ğ°Ñ‚Ğ¾Ğ½Ğ°)\",\"Ğ·Ğ° Ñ‚Ğ¸Ğ¼ ÑĞ°Ğ¼Ğ¸Ğ¼ Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾ĞµÑ‚Ñ–Ğ² Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ³Ğ°Ğ»Ñ– Ğ¼Ğ¸Ñ‚Ñ†Ñ–Ğ² - Ğ½Ğ°Ñ„iĞ³ Ğ· Ğ¿Ğ»ÑĞ¶Ğ°, Ğ²Ğ¸Ğ³Ğ½Ğ°Ñ‚Ğ¸ Ğ·Ğ° Ğ¼ĞµĞ¶Ñ– Ğ”ĞµÑ€Ğ¶Ğ°Ğ²Ğ¸ ĞŸĞ»Ğ°Ñ‚Ğ¾Ğ½Ğ°)\",\"- Ğ½Ğ°Ñ…ÑƒĞ¹ Ğ· Ğ¿Ğ»ÑĞ¶Ğ°, Ğ²Ğ¸Ğ³Ğ½Ğ°Ñ‚Ğ¸ Ğ·Ğ° Ğ¼ĞµĞ¶Ñ– Ğ”ĞµÑ€Ğ¶Ğ°Ğ²Ğ¸ ĞŸĞ»Ğ°Ñ‚Ğ¾Ğ½Ğ°)\"\n",
      "482,am,áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆá‰³áˆ³á‹áŠáŠ áŠ¥áŠ•á‹° áŠ áŠ•á‰° áŠ á‹­áŠá‰µ á‹°áŠá‹ áˆ˜áŒáˆˆáŒ« áˆ²áˆ°áŒ¥ áŠ áŠ•á‰° á‹µá‹³áˆ,áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆá‰³áˆ³á‹áŠáŠ áŠ¥áŠ•á‹° áŠ áŠ•á‰° áŠ á‹­áŠá‰µ áˆ°á‹  áˆ˜áŒáˆˆáŒ« áˆ²áˆ°áŒ¥,áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆá‰³áˆ³á‹áŠáŠ áŠ¥áŠ•á‹° áŠ áŠ•á‰° áŠ á‹­áŠá‰µ á‹°áŠá‹ áˆ˜áŒáˆˆáŒ« áˆ²áˆ°áŒ¥ áŠ áŠ•á‰° á‹µá‹³áˆ\n",
      "483,hi,à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤¹à¤°à¤¾à¤®à¥€ à¤•à¥€ à¤«à¤¼à¤¿à¤²à¥à¤® à¤¦à¥‡à¤–à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤…à¤•à¥à¤² à¤•à¥€ à¤œà¤°à¥‚à¤°à¤¤ à¤¨à¤¹à¥€à¤‚ à¤ªà¤¡à¤¼à¤¤à¥€ ....,à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤‰à¤¸à¤•à¥€ à¤«à¤¿à¤²à¥à¤® à¤¦à¥‡à¤–à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤…à¤•à¥à¤² à¤•à¥€ à¤œà¤°à¥‚à¤°à¤¤ à¤¨à¤¹à¥€à¤‚ à¤ªà¤¡à¤¼à¤¤à¥€ ....,<extra_id_0> à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤¹à¤°à¤¾à¤®à¥€ à¤•à¥€ à¤«à¤¼à¤¿à¤²à¥à¤® à¤¦à¥‡à¤–à¤¨à¥‡ à¤²à¤¿à¤ à¤…à¤•à¥à¤² à¤•à¥€ à¤œà¤°à¥‚à¤°à¤¤ à¤¨à¤¹à¥€à¤‚ à¤ªà¤¡à¤¼à¤¤à¥€ ....\n",
      "484,ar,Ù…Ø§ Ù‚ØµØ±Øª .. ØªØ·ÙˆØ± Ø§Ù„Ø§Ù…Ø± .. Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ…,Ù…Ø§ Ù‚ØµØ±Øª .. ØªØ·ÙˆØ± Ø§Ù„Ø§Ù…Ø± ..,<extra_id_0> Ù…Ø§ Ù‚ØµØ±Øª .. ØªØ·ÙˆØ± Ø§Ù„Ø§Ù…Ø± ..Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ…\n",
      "485,uk,ĞĞµ Ñ€Ğ¾Ğ·ÑƒĞ¼Ñ–Ñ Ñ‡Ğ¸Ğ¼ Ñ Ğ·Ğ°ÑĞ»ÑƒĞ¶Ğ¸Ğ»Ğ° Ñ†Ñ Ñ…ÑƒĞ¹Ğ¾Ğ²Ñƒ ĞºĞ°Ñ€Ğ¼Ñƒ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ğ¼ Ñ‡Ğ°ÑĞ¾Ğ¼,ĞĞµ Ñ€Ğ¾Ğ·ÑƒĞ¼iÑ Ñ‡Ğ¸Ğ¼ Ñ Ğ·Ğ°ÑĞ»ÑƒĞ¶Ğ¸Ğ»Ğ° Ñ†Ñ ĞºĞ°Ñ€Ğ¼Ñƒ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½iĞ¼ Ñ‡Ğ°ÑĞ¾Ğ¼,<extra_id_0> Ñ Ğ·Ğ°ÑĞ»ÑƒĞ¶Ğ¸Ğ»Ğ° Ñ†Ñ Ñ…ÑƒĞ¹Ğ¾Ğ²Ñƒ ĞºĞ°Ñ€Ğ¼Ñƒ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ğ¼ Ñ‡Ğ°ÑĞ¾Ğ¼\n",
      "486,en,\"collins , is an idiot , but we didnt need this article to establish that fact .\",Colin is not so intelligent but we didn't need this article to establish this fact.,\"<extra_id_0> is an idiot , but we didnt need this article to establish that fact .\"\n",
      "487,en,making and distributing cp is fucked .,Making and distributing cp is not good,making and distributing cp is fucked .\n",
      "488,am,áˆ›áŠáˆ… áŠ áŠ•á‰° áˆµá‹©áˆá£ áŠ áŠ•á‹µ áˆ°á‹ áŠ¨áˆá‰° áŠ á‰¥á‹­ áˆµáˆáŒ£áŠ• áŠ¥áˆˆá‰ƒáˆˆáˆ á‰¥áˆáˆ áˆµá‰µáˆ áŠ áˆáŠá‰ áˆ¨áˆ?? á‹¨áŒ…áˆ›á‹áŠ• áŠ¥áˆá‰‚á‰µ áŠ áˆáˆ°áˆ›áˆ…áˆ?? á‹¨áˆ†áŠ•áŠ­ á‰±áˆá‰±áˆ‹ áŠáˆ…,á‹¨áˆ°á‹ áŠááˆµ á‹¨áˆšáŒ á‹ áŠ¨áˆ†áŠ áŒ /áˆšáˆ© áˆµáˆáŒ£áŠ• áŠ¥áˆˆá‰ƒáˆˆáˆ á‰¥áˆáˆ á‰¥áˆˆáˆ… áŠá‰ áˆ­ á¤ á‹¨áŒ…áˆ›á‹áˆµ á‹¨áˆ°á‹ áˆ•á‹­á‹ˆá‰µ áˆ˜áŒ¥á‹á‰µ áˆáŠ• áˆŠá‰£áˆ áŠá‹ ?,áˆ°á‹ áŠ¨áˆá‰° áŠ á‰¥á‹­ áˆµáˆáŒ£áŠ• áŠ¥áˆˆá‰ƒáˆˆáˆ á‰¥áˆáˆ áˆµá‰µáˆ áŠ áˆáŠá‰ áˆ¨áˆ?? á‹¨áŒ…áˆ›á‹áŠ• áŠ¥áˆá‰‚á‰µ áŠ áˆáˆ°áˆ›áˆ…áˆ?? á‹¨áŒ…áˆ›á‹áŠ• áŠ¥áˆá‰‚á‰µ áŠ áˆáˆ°áˆ›áˆ…áˆ?? á‹¨áŒ…áˆ›á‹áŠ• áŠ¥áˆá‰‚á‰µ áŠ áˆáˆ°áˆ›áˆ…áˆ?? á‹¨áŒ…áˆ›á‹áŠ• áŠ¥áˆá‰‚á‰µ áŠ áˆáˆ°áˆ›áˆ…áˆ?? á‹¨áˆ†áŠ•áŠ­ á‰±áˆá‰±áˆ‹ áŠáˆ…\n",
      "489,en,that guy used to be a real dick .,That guy used to be a mean person,<extra_id_0> that guy used to be a real dick\n",
      "490,am,áŠ á‰¤á‰µá‰µá‰µ??? á‹¨áˆ°áˆáˆ­ áˆ°á‹á‰½ áˆáˆ‰ á‰°áˆ›áˆ¨áˆ© á‰¤á‰³á‰¸á‹áŠ• á‹˜áŒá‰°á‹ áˆáˆ³ áˆ°áˆ©:: . For that áˆ†á‹³áˆ friend.,áŠ á‰¤á‰µá‰µá‰µ??? á‹¨áˆ°áˆáˆ­ áˆ°á‹á‰½ áˆáˆ‰ á‰°áˆ›áˆ¨áˆ© á‰¤á‰³á‰¸á‹áŠ• á‹˜áŒá‰°á‹ áˆáˆ³ áˆ°áˆ©::,áˆ°áˆáˆ­ áˆ°á‹á‰½ áˆáˆ‰ á‰°áˆ›áˆ¨áˆ© á‰¤á‰³á‰¸á‹áŠ• á‹˜áŒá‰°á‹ áˆáˆ³ áˆ°áˆ©::\n",
      "491,zh,å§†è›†å¥½æ¶å¿ƒ?,è¿™ä½å¥³æ€§å¥½è®©äººæ„Ÿåˆ°ä¸é€‚ï¼Ÿ,ç»ªä¸å–„å¿ƒ?\n",
      "492,en,holy shit i thought that was a joke .,I thought it was a joke!!,<extra_id_0> I thought that was a joke .\n",
      "493,zh,åƒåœ¾äººå£ï¼Œå¢åŠ ä¸–ç•Œè´Ÿæ‹…,æˆ‘ä¸å–œæ¬¢çš„äººå£æ•°é‡ï¼Œå¢åŠ ä¸–ç•Œè´Ÿæ‹…,\"åƒåœ¾äººå£,å¢åŠ ä¸–ç•Œè´Ÿæ‹…\"\n",
      "494,en,\"crimea voted for secession and russia accepted and annexed crimea , what fucking crisis ! ! ! !\",crimea voted for secession and russia accepted and annexed crimea,\"<extra_id_0> voted for secession and russia accepted and annexed crimea , what fucking crisis ! ! !\"\n",
      "495,en,they named that shit after him .,They named it after him,<extra_id_0> they named that shit after him\n",
      "496,zh,ä½ æŠŠèˆ”ç‹—é¾Ÿç”·æ”¾åˆ°è¿™ä¸ªè¡Œåˆ—é‡Œå¯è¿˜è¡Œï¼Œè‡ªå¨±è‡ªä¹çœŸå¥½ç©,ä½ æŠŠå•æ–¹é¢ä»˜å‡ºçš„å’Œå‘å¾®çš„ç”·æ€§æ”¾åˆ°è¿™ä¸ªè¡Œåˆ—é‡Œå¯è¿˜è¡Œï¼Œè‡ªå¨±è‡ªä¹çœŸå¥½ç©,\"ä½ æŠŠèˆ”ç‹—é¾Ÿç”·æ”¾åˆ°è¿™ä¸ªè¡Œåˆ—é‡Œå¯è¿˜è¡Œ,è‡ªå¨±è‡ªä¹çœŸå¥½ç©\"\n",
      "497,uk,\"Ğ’ ĞºĞ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾ÑÑĞ½ÑÑ”, Ñ‰Ğ¾ Ñ—Ğ·Ğ´Ğ¸Ğ² Ğ´Ğ¸Ğ²Ğ¸Ñ‚Ğ¸ÑÑ, ÑĞº Ñ‚ÑƒĞ±Ñ–Ğ»ÑŒÑ†Ñ– Ğ¿Ğ¸Ğ·Ğ´ÑÑ‚ÑŒÑÑ Ğ¼Ñ–Ğ¶ ÑĞ¾Ğ±Ğ¾Ñ, Ğ¿Ñ€Ğ¾ â€œboth sidesâ€ Ñ– â€œmore neutral approachâ€, Ğ¾Ñ…ÑƒÑ”Ğ½Ğ½Ğ¾\",\"'Ğ’ ĞºĞ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾ÑÑĞ½ÑÑ”, Ñ‰Ğ¾ Ñ—Ğ·Ğ´Ğ¸Ğ² Ğ´Ğ¸Ğ²Ğ¸Ñ‚Ğ¸ÑÑ, ÑĞº Ñ‚ÑƒĞ±Ñ–Ğ»ÑŒÑ†Ñ– Ğ±''ÑÑ‚ÑŒÑÑ Ğ¼Ñ–Ğ¶ ÑĞ¾Ğ±Ğ¾Ñ, Ğ¿Ñ€Ğ¾ â€œboth sidesâ€ Ñ– â€œmore neutral approachâ€, Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ğ¾'\",\"<extra_id_0> Ğ¿Ğ¾ÑÑĞ½ÑÑ”, Ñ‰Ğ¾ Ñ—Ğ·Ğ´Ğ¸Ğ² Ğ´Ğ¸Ğ²Ğ¸Ñ‚Ğ¸ÑÑ, ÑĞº Ñ‚ÑƒĞ±Ñ–Ğ»ÑŒÑ†Ñ– Ğ¿Ğ¸Ğ·Ğ´ÑÑ‚ÑŒÑÑ Ğ¼Ñ–Ğ¶ ÑĞ¾Ğ±Ğ¾Ñ, Ğ¿Ñ€Ğ¾ â€œboth sidesâ€ Ñ– â€œmore neutral approachâ€, Ğ¾Ñ…ÑƒÑ”Ğ½Ğ½Ğ¾\"\n",
      "498,am,áŠ£á‹¨ áŠ£áŠ•á‰° á‹°á‹°á‰¥ áˆ½áŠ•á‰³áˆ áŠ áŒˆáˆ­ áˆ˜á‰áŒ áˆ á‹¨áˆšáŒ­áˆˆá‹ á‰ áá‰…áˆ­ á‹•áŠ•áŒ‚ á‰ áˆ˜ááŠ¨áˆ­ áˆ…áƒáŠ“á‰µ á‰ áˆáŒá‹°áˆ áˆ°áˆˆáˆ›á‹á‹«áŠ• áŠ£á‹›á‹áŠ•á‰µ á‰ áŠ£á‹¨áˆ­ á‰ áˆ˜á‹°á‰¥á‹°á‰¥ áŠ£á‹­á‹°áˆˆáˆá¢ áŠ«áŠ£áˆáŠ•,áŠ á‹¨áˆ… áŠ áŒˆáˆ­ áˆ˜á‰€áŒ áˆ á‹¨áˆšá‰½áˆˆá‹ á‰ áá‰…áˆ­ á‹•áŠ•áŒ‚ á‰ áˆ˜ááŠ¨áˆ­ áˆ…áƒáŠ“á‰µ áŠ¥áŠ“ áˆ°áˆˆáˆ›á‹á‹«áŠ• áŠ£á‹›á‹áŠ•á‰µ áˆ‹á‹­ áŒ¥á‰ƒá‰µ á‰ áˆ˜áˆá€áˆ áŠ á‹­á‹°áˆˆáˆ,áˆ°áˆˆáˆ›á‹á‹«áŠ• áŠ£á‹›á‹áŠ•á‰µ á‰ áŠ£á‹¨áˆ­ á‰ áˆ˜á‹°á‰¥á‹°á‰¥ áŠ£á‹­á‹°áˆˆáˆá¢ áŠ«áŠ£áˆáŠ•\n",
      "499,hi,\"à¤¬à¤šà¥à¤šà¤¾ à¤…à¤ªà¤¨à¥‡ à¤¦à¥‹à¤¸à¥à¤¤ à¤¸à¥‡ - à¤¸à¥€à¤˜ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤¤à¥€ à¤²à¥‹à¤®à¤¡à¤¼à¥€ à¤•à¥‡, à¤”à¤° à¤¬à¤¤à¤¾à¤“ à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹ à¤–à¥‹à¤ªà¤¡à¤¼à¥€ à¤•à¥‡?\",à¤¬à¤šà¥à¤šà¤¾ à¤…à¤ªà¤¨à¥‡ à¤¦à¥‹à¤¸à¥à¤¤ à¤¸à¥‡ - à¤”à¤° à¤¬à¤¤à¤¾à¤“ à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹?,\"à¤¬à¤¿à¤¨à¤¾ à¤¹à¥‹à¤¤à¥€ à¤²à¥‹à¤®à¤¡à¤¼à¥€ à¤•à¥‡, à¤”à¤° à¤¬à¤¤à¤¾à¤“ à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹ à¤–à¥‹à¤ªà¤¡à¤¼à¥€ à¤•à¥‡?\"\n",
      "500,uk,ĞĞ¾Ğ²Ğ¸Ğ¹ Ğ³Ğ¾Ğ²Ğ½Ğ¾Ñ‚Ñ”Ñ€Ñ†Ñ”Ñ† - Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€ Ğ Ğ¾ÑÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ñ–Ñ‡Ñ”ÑÑ‚Ğ²Ğ° Ğ’Ğ¾Ñ€Ğ¾Ğ±Ğ¹Ğ¾Ğ².,ĞĞ¾Ğ²Ğ¸Ğ¹ Ñ‚Ñ”Ñ€Ñ†Ñ”Ñ† - Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€ Ğ Ğ¾ÑÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ñ–Ñ‡Ñ”ÑÑ‚Ğ²Ğ° Ğ’Ğ¾Ñ€Ğ¾Ğ±Ğ¹Ğ¾Ğ².,- Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€ Ğ Ğ¾ÑÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ñ–Ñ‡Ñ”ÑÑ‚Ğ²Ğ° Ğ’Ğ¾Ñ€Ğ¾Ğ±Ğ¹Ğ¾Ğ².\n",
      "501,hi,à¤¤à¥à¤® à¤•à¥‹ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤ à¤¸à¥‡ à¤µà¤¿à¤°à¥‹à¤§ à¤¹à¥ˆ à¤¤à¥‹ à¤•à¤¾à¤¨ à¤®à¥‡à¤‚ à¤²à¥‚à¤‚à¤¡à¤¼  à¤ à¥‚à¤¸  à¤²à¥‹ à¥¤,à¤¤à¥à¤®à¤•à¥‹ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤ à¤¸à¥‡ à¤µà¤¿à¤°à¥‹à¤§ à¤¹à¥ˆ à¤¤à¥‹ à¤…à¤ªà¤¨à¥‡ à¤•à¤¾à¤¨ à¤¬à¤‚à¤¦ à¤•à¤° à¤²à¥‹ |,à¤¬à¤¿à¤¨à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤ à¤¸à¥‡ à¤µà¤¿à¤°à¥‹à¤§ à¤¹à¥ˆ à¤¤à¥‹ à¤•à¤¾à¤¨ à¤®à¥‡à¤‚ à¤²à¥‚à¤‚à¤¡à¤¼ à¤ à¥‚à¤¸ à¤²à¥‹\n",
      "502,uk,Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ²Ñ–Ğ½ Ñ‰Ğµ Ñ– Ğ³Ğ°Ğ½Ğ´Ğ¾Ğ½.,Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ²Ñ–Ğ½ Ñ‰Ğµ Ñ– Ğ¿Ğ¾Ğ½Ğ°Ğ³Ğ° Ğ»ÑĞ´Ğ¸Ğ½Ğ°.,<extra_id_0> Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ²Ñ–Ğ½ Ñ‰Ğµ Ñ– Ğ³Ğ°Ğ½Ğ´Ğ¾Ğ½.\n",
      "503,ru,\", Ñ Ñ‚ÑƒĞ¿Ğ¾Ğ¹ Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑĞ» ÑˆÑƒÑ‚ĞºĞ¸ Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¾ ÑĞ¿Ğ°Ğ¼?\",Ğ¯ Ğ³Ğ»ÑƒĞ¿Ñ‹Ğ¹ Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑĞ» ÑˆÑƒÑ‚ĞºĞ¸ Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¾ ÑĞ¿Ğ°Ğ¼?,<extra_id_0> Ñ Ñ‚ÑƒĞ¿Ğ¾Ğ¹ Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑĞ» ÑˆÑƒÑ‚ĞºĞ¸ Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¾ ÑĞ¿Ğ°Ğ¼?\n",
      "504,en,removed from reality in - fucking - deed,Removed from reality indeed,- fucking - deed\n",
      "505,am,áŠ¥áˆ±áŠ• á‹« áŠ á‰¥á‹­ á‰°á‹­á‰€á‹ á‹áˆ á‰¥áˆˆáˆ… áŠ á‰µá‹˜á‰£áˆ­á‰… á¢ áŠ áŠ•á‰° áŠ¥áˆµáŠª á‰°á‹‹áŒ‹ áŠ¨á‹› áˆŒáˆ‹á‹áŠ• á‰¦á‰…á‰§á‰ƒ á‰µáˆ‹áˆˆáˆ…,áŠ¥áˆ±áŠ• á‹¨áˆšáˆ˜áˆˆáŠ¨á‰³á‰¸á‹áŠ• áˆ˜áŒ á‹¨á‰… á‹­á‰»áˆ‹áˆá¤ á‰ áŒáˆá‰µ áŒáŠ• áŠ á‰µáŠ“áŒˆáˆ­á¡á¡ áˆ˜áŒ€áˆ˜áˆªá‹« áŠ áŠ•á‰° á‰³áŒˆáˆ áŠ¥áŠ“ áˆŒáˆ‹á‹ áŠ‹áˆ‹ á‹­á‹°áˆ­áˆ³áˆá¡á¡,<extra_id_0> áŠ¥áˆ±áŠ• á‹« áŠ á‰¥á‹­ á‰°á‹­á‰€á‹ á‹áˆ á‰¥áˆˆáˆ… áŠ á‰µá‹˜á‰£áˆ­á‰… á¢ áŠ áŠ•á‰° áŠ¥áˆµáŠª á‰°á‹‹áŒ‹ áŠ¨á‹› áˆŒáˆ‹á‹áŠ• á‰¦á‰…á‰§á‰ƒ á‰µáˆ‹áˆˆáˆ…\n",
      "506,ar,Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù„Ù‡Ø°Ø§ Ø§Ù„ØºØ¨Ø§Ø¡ Ù‡Ùˆ Ø£Ù†Ù‡Ø§ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ø§ Ø§Ù„ÙÙŠØ¯Ø±Ø§Ù„ÙŠØ©,Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ³Ù…Ø­ Ø¨Ø­Ø¯ÙˆØ« Ù‡Ø°Ø§ Ù‡Ùˆ Ø£Ù†Ù‡Ø§ Ø£Ø±Ø¶ ÙÙŠØ¯Ø±Ø§Ù„ÙŠØ©,<extra_id_0> Ø£Ù†Ù‡Ø§ Ø¹Ù„Ù‰ Ø£Ø±Ø¶Ù‡Ø§ Ø§Ù„ÙÙŠØ¯Ø±Ø§Ù„ÙŠØ©\n",
      "507,en,\"Are u there dick, wars back on!!!!\",Are u there? The war is back on!,\"u there dick, wars back on!!!!\"\n",
      "508,ru,Ñ‚Ğ²Ğ¾Ñ‘ Ğ¾Ñ‡ĞºĞ¾ ÑƒÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ñ„Ğ¾Ñ€ÑƒĞ¼Ğ°!,Ğ²Ğ°Ñˆ Ğ±Ğ°Ğ»Ğ» ÑƒÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ñ„Ğ¾Ñ€ÑƒĞ¼Ğ°!,<extra_id_0>Ñ‘ Ğ¾Ñ‡ĞºĞ¾ ÑƒÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ñ„Ğ¾Ñ€ÑƒĞ¼Ğ°!\n",
      "509,ar,Ù‡ØªØ¹Ø¯ÙŠ Ø§Ù„Ø®Ø· Ø§Ù„Ø£Ø­Ù…Ø± Ø§Ù…ØªØ¦ ÙŠ Ù‚Ø±Ø¯ÙˆØºØ§Ù† Ù…Ø±Ø© 50 ÙŠÙˆÙ…,Ù‡ØªØ¹Ø¯ÙŠ Ø§Ù„Ø®Ø· Ø§Ù„Ø£Ø­Ù…Ø± Ø§Ù…ØªØ¦ ÙŠ Ø£Ø±Ø¯ÙˆØºØ§Ù† Ù…Ø±Ø© 50 ÙŠÙˆÙ…,Ù‡ØªØ¹Ø¯ÙŠ Ø§Ù„Ø®Ø· Ø§Ù„Ø£Ø­Ù…Ø± Ø§Ù…ØªØ¦ ÙŠ Ù‚Ø±Ø¯ÙˆØºØ§Ù† Ù…Ø±Ø© 50 ÙŠÙˆÙ…\n",
      "510,hi,\"à¤¦à¤²à¤¾à¤², à¤­à¤¡à¤µà¤¾ à¤­à¤¾à¤— à¤°à¥‡ à¤­à¤¾à¤— !!\",\"à¤¬à¥à¤°à¥‡, à¤ªà¤•à¥à¤·à¤ªà¤¾à¤¤à¥€ à¤­à¤¾à¤— à¤°à¥‡ à¤­à¤¾à¤— !!\",\"'' à¤¦à¤²à¤¾à¤², à¤­à¤¡à¤µà¤¾ à¤­à¤¾à¤— à¤°à¥‡ à¤­à¤¾à¤—!!\"\n",
      "511,en,&gt today was one of the most fucked up days of my life .,& today was one of the worst days of my life,<extra_id_0> was one of the most fucked up days of my life .\n",
      "512,am,áŠ áˆá‰£áŒˆáŠáŠ• áˆ˜áˆª á‹¨á‰°á‰ƒá‹ˆáˆ™á‰µáŠ• áˆáˆ‰ á‹«áˆµáˆ«áˆ áŠ áˆˆ á‹«áŒˆáˆ¬ áˆ°á‹ áŠ áˆ½á‰ƒá‰£áŒ­ á¢,áŠ áˆá‰£áŒˆáŠáŠ• áˆ˜áˆª á‹¨á‰°á‰ƒá‹ˆáˆ™á‰µáŠ• áˆáˆ‰ á‹«áˆµáˆ«áˆ áŠ áˆˆ á‹«áŒˆáˆ¬ áˆ°á‹,á‹«áŒˆáˆ¬ áˆ°á‹ áŠ áˆ½á‰ƒá‰£áŒ­ á¢\n",
      "513,en,\"Hello, my name is anon, don't fuck with 4chan.\",\"Hello, my name is anon, don't confront with 4chan.\",Please Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello\n",
      "514,en,* neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .,Neither of my co-workers cared when it came to ditching Mitch.,<extra_id_0> neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n",
      "515,uk,\"'Ğ³Ğ¾Ğ´ÑƒÑ”Ğ¼Ğ¾, Ğ±Ğ»ÑÑ‚ÑŒ, ĞºĞ¾Ñ‚Ñ–Ğ² Ğ¾ Ğ¿''ÑÑ‚Ñ–Ğ¹ Ñ€Ğ°Ğ½ĞºÑƒ'\",'Ğ³Ğ¾Ğ´ÑƒÑ”Ğ¼Ğ¾ ĞºĞ¾Ñ‚Ñ–Ğ² Ğ¾ Ğ¿''ÑÑ‚Ñ–Ğ¹ Ñ€Ğ°Ğ½ĞºÑƒ',\"''Ğ³Ğ¾Ğ´ÑƒÑ”Ğ¼Ğ¾, Ğ±Ğ»ÑÑ‚ÑŒ, ĞºĞ¾Ñ‚Ñ–Ğ² Ğ¾ Ğ¿''ÑÑ‚Ñ–Ğ¹ Ñ€Ğ°Ğ½ĞºÑƒ'\"\n",
      "516,es,porque no tenÃ©is a un subnormal por entrenador.,porque no tenÃ©is a un loco por entrenador.,porque no tenÃ©is a un subnormal por entrenador.\n",
      "517,ru,\"Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ· Ğ²ÑĞµ ÑƒĞ±ĞµĞ´Ğ¸Ğ»Ğ¸ÑÑŒ, Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ğ¾ÑÑÑ‚Ğ¸Ğ½Ğ° Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ²ĞµÑ‰ĞµĞ¹.\",\"Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ· Ğ²ÑĞµ ÑƒĞ±ĞµĞ´Ğ¸Ğ»Ğ¸ÑÑŒ, Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ³Ğ¾Ğ¼Ğ¾ÑĞµĞºÑÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ²ĞµÑ‰ĞµĞ¹\",<extra_id_0> Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ğ¾ÑÑÑ‚Ğ¸Ğ½Ğ° Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ²ĞµÑ‰ĞµĞ¹\n",
      "518,ar,Ø¹Ø¯ Ø¥Ù„Ù‰ Ù‡Ù†Ø§ ÙŠØ§ ÙƒØ³ÙˆÙ„ ÙŠØ§Ø´ÙŠÙˆØ¹ÙŠ!,Ø¹Ø¯ Ø¥Ù„Ù‰ Ù‡Ù†Ø§ ÙŠØ§ ÙƒØ³ÙˆÙ„ Ø¨Ø§Ù…Ù† ØªØ¤ÙŠØ¯ Ø§Ù„Ø´ÙŠÙˆØ¹ÙŠØ©!,Ù‡Ù†Ø§ ÙŠØ§ ÙƒØ³ÙˆÙ„ ÙŠØ§Ø´ÙŠÙˆØ¹ÙŠ!\n",
      "519,en,\"abortion is not a fundamental right it is a proceedure , nothing more idiot\",Abortion is not a fundamental right it is a procedure,<extra_id_0> abortion is not a fundamental right it is a proceedure\n",
      "520,es,\"EstÃ¡s enfermo y no quieres ver.\n",
      "\n",
      "Cada uno que arregle los problemas de su casa, aquÃ­ ya tenemos bastantes.\",\"Tienes un problema y no quieres ver. \n",
      "Cada uno que arregle los problemas de su casa, aquÃ­ ya tenemos bastantes.\",\"<extra_id_0> no quieres ver. Cada uno que repare los problemas de su casa, aquÃ­ ya tenemos bastantes.\"\n",
      "521,hi,à¤à¤• à¤¹à¤°à¤¾à¤®à¥€ à¤•à¤¾ à¤ªà¥€à¤²à¤¾ à¤µà¥ˆà¤¸à¥‡ à¤†à¤¶à¥€à¤· à¤–à¥‡à¤¤à¤¾à¤¨ à¤…à¤­à¥€ à¤•à¥à¤¯à¤¾ à¤•à¤° à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤¤à¥‡à¤°à¥‡ à¤¸à¤¾à¤¥à¥¤,à¤µà¥ˆà¤¸à¥‡ à¤†à¤¶à¥€à¤· à¤–à¥‡à¤¤à¤¾à¤¨ à¤…à¤­à¥€ à¤•à¥à¤¯à¤¾ à¤•à¤° à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤¤à¥‡à¤°à¥‡ à¤¸à¤¾à¤¥à¥¤,<extra_id_0> à¤•à¥à¤¯à¤¾ à¤•à¤° à¤°à¤¹à¤¾ à¤¹à¥ˆ à¤¤à¥‡à¤°à¥‡ à¤¸à¤¾à¤¥à¥¤\n",
      "522,en,the really sad fucking part is they tried this story before in number maannews .,the really sad part is that they tried this story before in the maannews.,<extra_id_0> the really sad part is they try this story before in number maannews\n",
      "523,ar,Ù‡Ø°Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù†ÙŠ Ø¨ Ù¤Ù¦ Ø³Ù†ØªÙ…ØªØ± ÙˆØ£Ù†Ø§ Ø£Ø´ÙˆÙ Ø§Ù„Ù„ÙŠ Ø£Ù‚ØµØ± Ù…Ù†ÙŠ Ø¨ Ù¡Ù  Ø³Ù†ØªÙ…ØªØ± Ù‚Ø²Ù… Ø£Ø¬Ù„ Ù‡Ø°Ø§ ÙˆØ´ ÙŠØ´ÙˆÙÙ†ÙŠğŸ™„,Ù‡Ø°Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù†ÙŠ Ø¨ Ù¤Ù¦ Ø³Ù†ØªÙ…ØªØ± ÙˆØ£Ù†Ø§ Ø£Ø´ÙˆÙ Ø§Ù„Ù„ÙŠ Ø£Ù‚ØµØ± Ù…Ù†ÙŠ Ø¨ Ù¡Ù  Ø³Ù†ØªÙ…ØªØ± Ù‚ØµÙŠØ± Ø£Ø¬Ù„ Ù‡Ø°Ø§ ÙˆØ´ ÙŠØ´ÙˆÙÙ†ÙŠğŸ™„,<extra_id_0> Ù‡Ø°Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù†ÙŠ Ø¨ Ù¤Ù¦ Ø³Ù†ØªÙ…ØªØ± ÙˆØ£Ù†Ø§ Ø£Ø´ÙˆÙ Ø§Ù„Ù„ÙŠ Ø£Ù‚ØµØ± Ù…Ù†ÙŠ Ø¨ Ù¡Ù  Ø³Ù†ØªÙ…ØªØ± Ù‚Ø²Ù… Ø£Ø¬Ù„ Ù‡Ø°Ø§ ÙˆØ´ ÙŠØ´ÙˆÙÙ†ÙŠğŸ™„\n",
      "524,hi,à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤¤à¥à¤® à¤¸à¥à¤µà¤°à¥‹ à¤•à¥‹ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤®à¥‡ à¤•à¥‹à¤ˆ à¤¦à¥‡à¤¶ à¤ªà¤¨à¤¾à¤¹ à¤¨à¤¹à¥€à¤‚ à¤¦à¥‡à¤—à¤¾ ||,à¤¤à¥à¤® à¤¬à¥‡à¤•à¤¾à¤° à¤²à¥‹à¤—à¥‹à¤‚ à¤•à¥‹ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤®à¥‡ à¤•à¥‹à¤ˆ à¤¦à¥‡à¤¶ à¤ªà¤¨à¤¾à¤¹ à¤¨à¤¹à¥€à¤‚ à¤¦à¥‡à¤—à¤¾ ||,à¤¤à¥à¤® à¤¸à¥à¤µà¤°à¥‹ à¤•à¥‹ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤®à¥‡ à¤•à¥‹à¤ˆ à¤¦à¥‡à¤¶ à¤ªà¤¨à¤¾à¤¹ à¤¨à¤¹à¥€à¤‚ à¤¦à¥‡à¤—à¤¾ ||\n",
      "525,am,áˆ›áˆ áˆŒáˆ‹ áˆ˜á‰³á‹ˆá‰‚á‹«á‹ áˆŒáˆ‹ áˆ°á‹á‹¨á‹ á‰…áŠ” áˆ²á‹˜áˆ­á á‹«áŠ•á‰°áŠ á‹­áŠá‰± á‹¨á‹áŠ•á‰¥ áŒˆáˆá‰± áˆ°áˆ™ áˆ‹á‹­ áˆ™áŒ¥áŠ á‹­áˆ‹áˆ á‹«á‹ á‹¨á‹áŠ•á‰¥ á‰£áˆ…áˆª áŠá‹,áˆ›áˆ áˆŒáˆ‹ áˆ˜á‰³á‹ˆá‰‚á‹«á‹ áˆŒáˆ‹ áˆ°á‹á‹¨á‹ á‰…áŠ” áˆ²á‹˜áˆ­á áŠ áŠ•á‰° á‹°áŒáˆ áˆµáˆ‹áˆáŒˆá‰£áˆ… áˆ°áˆ™ áˆ‹á‹­ áˆ™áŒ¥áŠ á‰µáˆ‹áˆˆáˆ…,áˆ°á‹á‹¨á‹ á‰…áŠ” áˆ²á‹˜áˆ­á á‹«áŠ•á‰°áŠ á‹­áŠá‰± áˆ°áˆ™ áˆ‹á‹­ áˆ™áŒ¥áŠ á‹­áˆ‹áˆ á‹«á‹ á‹¨á‹áŠ•á‰¥ á‰£áˆ…áˆª áŠá‹\n",
      "526,uk,\"ĞÑ–, Ñ‚Ğ° Ñ…ÑƒĞ¹Ğ½Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»Ğ°ÑĞ½Ğ¸ĞºĞ°Ñ… Ğ·Ğ°Ğ»Ğ¸ÑˆĞ°Ğ»Ğ°ÑÑŒ)\",\"ĞÑ–, Ñ‚Ğ°  Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»Ğ°ÑĞ½Ğ¸ĞºĞ°Ñ… Ğ·Ğ°Ğ»Ğ¸ÑˆĞ°Ğ»Ğ°ÑÑŒ)\",\"<extra_id_0>, Ñ‚Ğ° Ñ‚Ğ° Ğ»ÑĞ´Ğ¸Ğ½Ğ° Ğ»Ğ¸ÑˆĞ°Ğ»Ğ°ÑÑŒ.\"\n",
      "527,ar,Ø£ÙŠØ§ ÙƒØ§Ù† Ù…Ø§ Ù„Ø¯ÙŠ Ù…Ù† Ø±ÙˆØ« ØªØ±ÙƒÙ‡ ÙˆØ­Ø¯Ù‡.,Ù„Ø¯ÙŠ Ø¹Ù…Ù„ÙŠ Ø§Ù„Ø®Ø§Øµ Ù„Ø£Ù‚ÙˆÙ… Ø¨Ù‡,Ø£Ø«ÙŠØ§ ÙƒØ§Ù† Ù…Ø§ Ù„Ø¯ÙŠ Ù…Ù† Ø±ÙˆØ« ØªØ±ÙƒÙ‡ ÙˆØ­Ø¯Ù‡.\n",
      "528,am,á‹­áˆ„ á‰£áˆˆáŒŒ áˆáŠ• áˆ›áˆˆá‰±áŠá‹ áŠáŒ­ áˆˆá‰¥áˆ¶ áˆ˜áŠ¨áˆ‹áŠ¨á‹« á‹áˆµáŒ¥ á‹¨áŒˆá‰£á‹?,á‹­áˆ„ áˆ°á‹ áˆáŠ• áˆ›áˆˆá‰±áŠá‹ áŠáŒ­ áˆˆá‰¥áˆ¶ áˆ˜áŠ¨áˆ‹áŠ¨á‹« á‹áˆµáŒ¥ á‹¨áŒˆá‰£á‹?,áˆáŠ• áˆ›áˆˆá‰±áŠá‹ áŠáŒ­ áˆˆá‰¥áˆ¶ áˆ˜áŠ¨áˆ‹áŠ¨á‹« á‹áˆµáŒ¥ á‹¨áŒˆá‰£á‹?\n",
      "529,en,HELLO HOW DO I GET SOMEBODY TO FIX SHIT AROUND HERE?,\"Hello, how do I get someone to fix problems around here?\",<extra_id_0> HOW DO I GET SOMEBODY TO FIX SHIT AROUND HERE?\n",
      "530,en,if you find yourself using your religion to deny rights to women you 're a piece of shit .,If you find yourself using your religion to deny rights to women you are a mean person,<extra_id_0> if you find yourself using your religion to deny rights to women you 're a piece of shit.\n",
      "531,ru,ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ»Ğ±Ğ°ĞµĞ±Ñƒ Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ°Ğ´Ğ¾ Ğ´Ğ¾ Ğ¸Ğ³Ñ€Ğ°Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€Ğ¾Ğº,Ğ­Ñ‚Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ» Ğ¿Ğ¾ Ğ·Ğ°ÑĞ»ÑƒĞ³Ğ°Ğ¼,ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ»Ğ±Ğ°ĞµĞ±Ñƒ Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ°Ğ´Ğ¾ Ğ´Ğ¾ Ğ¸Ğ³Ñ€Ğ°Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ´ÑƒÑ€Ğ¾Ğº\n",
      "532,am,á‰¤á‰² LTV áˆáŒ£áˆª áŒáˆ¨á‰¤á‰µáˆ…áŠ• áŠ¥áŠ•á‹°áˆ«áˆµáˆ… á‹á‹°á‹µ á‹­áˆ‹áˆá¢ á‹¨á–áˆˆá‰²áŠ« áˆá‹©áŠá‰µáˆ áˆ˜á‰¥á‰µ áŠá‹á¢ áˆá‹©áŠá‰³á‰½áŠ• á‰ áˆ˜áŠ¨á‰£á‰ áˆ­ á‰¢áˆ†áŠ• áˆ€áŒˆáˆ«á‰½áŠ• á‹˜áˆ‹á‰‚ áˆ°áˆ‹áˆ á‰³áŒˆáŠ›áˆˆá‰½á¢ á‹¨áŒ¥áˆ‹á‰» á‹˜áˆ˜á‰» áŒáŠ• áˆˆáˆ«áˆµáˆ½áˆ áˆˆáˆ€áŒˆáˆ­áˆ áŠ á‹­áŒ á‰…áˆáˆá¢,á‰¤á‰² LTV áˆáŒ£áˆª áŒáˆ¨á‰¤á‰µáˆ…áŠ• áŠ¥áŠ•á‹°áˆ«áˆµáˆ… á‹á‹°á‹µ á‹­áˆ‹áˆá¢ á‹¨á–áˆˆá‰²áŠ« áˆá‹©áŠá‰µáˆ áˆ˜á‰¥á‰µ áŠá‹á¢ áˆá‹©áŠá‰³á‰½áŠ• á‰ áˆ˜áŠ¨á‰£á‰ áˆ­ á‰¢áˆ†áŠ• áˆ€áŒˆáˆ«á‰½áŠ• á‹˜áˆ‹á‰‚ áˆ°áˆ‹áˆ á‰³áŒˆáŠ›áˆˆá‰½á¢ áŠ¨á‹šáˆ… á‰£áˆˆáˆ áˆá‹©áŠá‰µáŠ• á‹¨áˆšá‹«áˆ°á‰ áŠáŒˆáˆ®á‰½ áˆ‹á‹­ áˆ›á‰°áŠ®áˆ­ áŒáŠ• áˆˆáˆ«áˆµáˆ½áˆ áˆˆáˆ€áŒˆáˆ­áˆ áŠ á‹­áŒ á‰…áˆáˆá¢,<extra_id_0> á‹¨á–áˆˆá‰²áŠ« áˆá‹©áŠá‰µáˆ áˆ˜á‰¥á‰µ áŠá‹á¢ á‹¨á–áˆˆá‰²áŠ« áˆá‹©áŠá‰µáˆ áˆ˜á‰¥á‰µ áŠá‹á¢\n",
      "533,ru,Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ‚Ğ¸ÑˆÑŒĞºĞ° Ğ¿Ğ¾ Ğ¼Ñ‹ÑˆĞ°Ñ‡Ğ¸ ÑƒĞ¼ĞµÑÑ‚ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ñ‹ Ğ±Ğ¸Ñ‚ÑŒ.,Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ‚Ğ¸ÑˆĞºĞ° ÑƒĞ¼ĞµÑÑ‚ Ğ±Ğ¸Ñ‚ÑŒ,Ğ¾Ğ½ĞµĞ· Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ‚Ğ¸ÑˆÑŒĞºĞ° Ğ¿Ğ¾ Ğ¼Ñ‹ÑˆĞ°Ñ‡Ğ¸ ÑƒĞ¼ĞµÑÑ‚ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ñ‹ Ğ±Ğ¸Ñ‚ÑŒ\n",
      "534,ru,Ğ½Ğ°Ñ€Ğ¾Ğ´ Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½ Ğ½Ğ¸Ğ¶Ğµ Ñ‡ĞµĞ¼ Ğ³Ğ¾Ğ²Ğ½Ğ¾ Ğ° Ğ²Ğ°Ğ¼ Ñ„Ğ¾Ğ½Ñ‚Ğ°Ğ½Ñ‹ Ğ¸ Ğ³Ğ¾Ğ»ÑƒĞ±Ñ‹Ğµ Ğ±Ğ»Ñ Ğ±ĞµÑ€ĞµÑ‚Ñ‹.,Ğ½Ğ°Ñ€Ğ¾Ğ´ Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½ Ğ½Ğ¸Ğ¶Ğµ Ğ½ĞµĞºÑƒĞ´Ğ° Ğ° Ğ²Ğ°Ğ¼ Ñ„Ğ¾Ğ½Ñ‚Ğ°Ğ½Ñ‹ Ğ¸ Ğ³Ğ¾Ğ»ÑƒĞ±Ñ‹Ğµ Ğ±ĞµÑ€ĞµÑ‚Ñ‹.,<extra_id_0> Ğ½Ğ°Ñ€Ğ¾Ğ´ Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½ Ğ½Ğ¸Ğ¶Ğµ Ñ‡ĞµĞ¼ Ğ³Ğ¾Ğ²Ğ½Ğ¾ Ğ° Ğ²Ğ°Ğ¼ Ñ„Ğ¾Ğ½Ñ‚Ğ°Ğ½Ñ‹ Ğ¸ Ğ³Ğ¾Ğ»ÑƒĞ±Ñ‹Ğµ Ğ±Ğ»Ñ Ğ±ĞµÑ€ĞµÑ‚Ñ‹\n",
      "535,hi,à¤¤à¥‡à¤°à¥‡ à¤®à¥à¤¹ à¤®à¥‡à¤‚ à¤…à¤¬ à¤¬à¤µà¤¾à¤¸à¥€à¤° à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥‹à¤—à¤¾ |,à¤¤à¥à¤® à¤•à¥à¤› à¤­à¥€ à¤¬à¥‹à¤² à¤¨à¤¹à¥€à¤‚ à¤ªà¤¾ à¤°à¤¹à¥‡ à¤¹à¥‹à¤—à¥‡ |,à¤¤à¥‡à¤°à¥‡ à¤®à¥à¤¹ à¤®à¥‡à¤‚ à¤¬à¤µà¤¾à¤¸à¥€à¤° à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥‹à¤—à¤¾ |\n",
      "536,zh,ä½ éš¾é“ä¸æ˜¯ï¼Ÿå‡­ä»€ä¹ˆä½ çš„èº«è¾¹å°±æ˜¯èº«è¾¹ï¼Œæˆ‘çš„èº«è¾¹å°±æ˜¯å‡çš„ï¼Ÿä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿå°é»‘äº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„ï¼Ÿ,ä½ éš¾é“ä¸æ˜¯ï¼Ÿå‡­ä»€ä¹ˆä½ çš„èº«è¾¹å°±æ˜¯èº«è¾¹ï¼Œæˆ‘çš„èº«è¾¹å°±æ˜¯å‡çš„ï¼Ÿä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿé»‘äººäº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„ï¼Ÿ,ä½ éš¾é“ä¸æ˜¯?å‡­ä»€ä¹ˆä½ çš„èº«è¾¹å°±æ˜¯å‡çš„?ä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿå°é»‘äº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„?ä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿå°é»‘äº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„?ä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿå°é»‘äº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„?ä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿå°é»‘äº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„?ä½ æ€ä¹ˆè¯æ˜é‚£äº›è·Ÿå°é»‘äº¤å¾€çš„å¥³ç”Ÿæ˜¯æˆ‘ä»¬985çš„?\n",
      "537,de,Klar doch . Die Schlimmsten im Lande sind die Rechten.die Moslems aber die BedaerungswÃ¼rdigen denen um JEDEN Preis geholfen werden muÃŸ.ğŸ’©ğŸ’©ğŸ’©ğŸ’©,Klar doch . Die Schlimmsten im Lande sind die Rechten.die Moslems aber die BedaerungswÃ¼rdigen denen um JEDEN Preis geholfen werden muÃŸ.,. Die Schlimmsten im Lande sind die Rechten.die Moslems aber die BedaerungswÃ¼rdigen denen um JEDEN Preis geholfen werden muÃŸ.\n",
      "538,ar,Ø®Ø±Ø¬ Ø³Ø§Ø±Ù‚ Ø¢Ø®Ø± Ù„Ø§ ÙØ§Ø¦Ø¯Ø© Ù„Ù‡ Ù…Ù† Ø§Ù„Ù…Ø¬ØªÙ…Ø¹.,Ø®Ø±Ø¬ Ø´Ø®Øµ Ø¢Ø®Ø± Ù„Ø§ ÙØ§Ø¦Ø¯Ø© Ù„Ù‡ Ù…Ù† Ø§Ù„Ù…Ø¬ØªÙ…Ø¹.,<extra_id_0> Ø®Ø±Ø¬ Ø³Ø§Ø±Ù‚ Ø¢Ø®Ø± Ù„Ø§ ÙØ§Ø¦Ø¯Ø© Ù„Ù‡ Ù…Ù† Ø§Ù„Ù…Ø¬ØªÙ…Ø¹\n",
      "539,en,\"in the end , he was just a dick .\",\"in the end , he was just unpleasant.\",<extra_id_0> he was just a dick .\n",
      "540,es,\"Costumbres tercermundistas que disfrutamos ya en Europa gracias a nuestros polÃ­ticos progres, disfruten lo votado.\",\"Costumbre de paÃ­ses en desarrollo que ya disfrutamos en Europa, gracias a los polÃ­ticos simpatizantes, disfruten el logro\",\"<extra_id_0>os polÃ­ticos progres, disfruten lo votado\"\n",
      "541,zh,å®ƒå’Œå¹¿å¤§å¥³é†›å…¨ä½“ç´«ç ‚ä¼°è®¡å¤§ä¼™å°±èƒ½åŸè°…å®ƒä»¬äº†,å®ƒå’Œå¹¿å¤§æç«¯å¥³æƒä¸»ä¹‰è€…å…¨ä½“ä»˜å‡ºä»£ä»·ä¼°è®¡å¤§ä¼™å°±èƒ½åŸè°…å®ƒä»¬äº†,å®ƒå’Œå¹¿å¤§å¥³é†›å…¨ä½“ç´«ç ‚ä¼°è®¡å¤§ä¼™å°±èƒ½åŸè°…å®ƒä»¬äº†\n",
      "542,am,áŠ®áˆ½? áˆáŠ• á‹“á‹­áŠá‰µ áˆ­áŠ«áˆ½ áˆµá‰¥á‹•áŠ“ áŠá‹ á‹«áˆˆáˆ…? á‹¨áŠ” á‹ˆáŠ•á‹µáˆ áŠ¥áˆ…á‰µ áŒ­ááŒ¨á‹ . á‹¨áŠ¥áŠ“á‰¶á‰¼ áˆˆá‰…áˆ¶ áˆ‹áŠ•á‰° áŠ¨áˆ†áŠ áŠ®áˆ½ . á‹°áˆ…áŠ“ á‰€áŠ•áˆ…áŠ• áŒ ,áŠ®áˆ½? áŠ¥áŠ•á‹´á‰µ áŠá‹ áŠáŒˆáˆ©áŠ• á‹¨áˆá‰³á‰€áˆˆá‹ áŠ áŠ•á‰° á¡ á‹ˆáŠ•á‹µáˆ˜áŠ“ áŠ¥áˆ…á‰¶á‰»á‰½áŠ• á‰ áˆá‰µ áŠ¥á‹«áŒ£áŠ• áŠ¥áŠ“á‰¶á‰»á‰½áŠ• áŠ¥á‹«áˆˆá‰€áˆ± áŠ¥áŠ® áŠá‹ !,<extra_id_0> á‹¨áŠ” á‹ˆáŠ•á‹µáˆ áŠ¥áˆ…á‰µ áˆˆá‰…áˆ¶ áˆ‹áŠ•á‰° áŠ¨áˆ†áŠ áŠ®áˆ½ . á‹°áˆ…áŠ“ á‰€áŠ•áˆ…áŠ• áŒ á‰ƒ\n",
      "543,zh,æœ‰äº›ç”·çš„å¥³çš„çœ‹ä¸ä¸Š?ä¸ºä»€ä¹ˆä¼šè§‰å¾—ç”·åŒçœ‹å¾—ä¸Šè‡ªå·±??,æœ‰äº›ç”·æ€§å¥³æ€§çœ‹ä¸ä¸Š?ä¸ºä»€ä¹ˆä¼šè§‰å¾—ç”·åŒæ€§æ‹çœ‹å¾—ä¸Šè‡ªå·±??,æœ‰äº›ç”·çš„å¥³çš„çœ‹ä¸ä¸Š?ä¸ºä»€ä¹ˆä¼šè§‰å¾—ç”·åŒçœ‹å¾—ä¸Šè‡ªå·±??\n",
      "544,en,\"kid , you should be watching the crap they call cartoons now made for your generation .\",\"kid , you should be watching the cartoons now made for your generation .\",<extra_id_0> you should be watching the crap they call cartoons now made for your generation .\n",
      "545,de,Welch ein Schwachsinn - Ã¼berlegt die Wahlergebnisse obwohl sie Ã¤uÃŸerst selten eine BÃ¼hne bekommen ihr TrÃ¤umer,Welch ein Quatsch - Ã¼berlegt die Wahlergebnisse obwohl sie Ã¤uÃŸerst selten eine BÃ¼hne bekommen ihr TrÃ¤umer,<extra_id_0> - Ã¼berlegt die Wahlergebnisse obwohl sie Ã¤uÃŸerst selten eine BÃ¼hne bekommen ihr TrÃ¤umer\n",
      "546,uk,Ğ¥Ğ¾Ñ€Ğ¾ÑˆÑ– ÑĞ¿Ğ¾Ğ³Ğ°Ğ´Ğ¸ - Ñ†Ğµ Ñ‚Ğ° Ñ‰Ğµ Ñ…ÑƒĞ¹Ğ½Ñ.,Ğ¥Ğ¾Ñ€Ğ¾ÑˆÑ– ÑĞ¿Ğ¾Ğ³Ğ°Ğ´Ğ¸ - Ñ†Ğµ Ñ‚Ğ° Ñ‰Ğµ ÑˆÑ‚ÑƒĞºĞ°.,Ğ¥Ğ¾Ñ€Ğ¾ÑˆÑ– ÑĞ¿Ğ¾Ğ³Ğ°Ğ´Ğ¸ - Ñ†Ğµ Ñ‚Ğ° Ñ‰Ğµ Ñ…ÑƒĞ¹Ğ½Ñ.\n",
      "547,uk,\"ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼, Ğ·Ğ°Ñ—Ğ±Ğ°Ğ»Ğ¾ Ñ‚Ğ¾ Ğ²ÑÑŒĞ¾. Ğ²Ğ¸ ĞºÑ‚Ğ¾ Ñ‚Ğ°ĞºÑ–Ñ”? Ñ Ğ²Ğ°Ñ Ğ½Ñ” Ğ·Ğ²Ğ°Ğ». Ñ–Ğ´Ñ–Ñ‚Ñ” Ğ½Ğ°Ñ…ÑƒĞ¹\",\"ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼, Ğ½Ğ°Ğ±Ñ€Ğ¸Ğ´Ğ»Ğ¾ Ñ‚Ğ¾ Ğ²ÑÑŒĞ¾. Ğ²Ğ¸ ĞºÑ‚Ğ¾ Ñ‚Ğ°ĞºÑ–Ñ”? Ñ Ğ²Ğ°Ñ Ğ½Ğµ Ğ·Ğ²Ğ°Ğ». iĞ´Ñ–Ñ‚ÑŒ Ğ·Ğ²Ñ–Ğ´ÑĞ¸\",\"Ğ¾Ğ½ĞµĞ· ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼, Ğ·Ğ°Ñ—Ğ±Ğ°Ğ»Ğ¾ Ñ‚Ğ¾ Ğ²ÑÑŒĞ¾. Ğ²Ğ¸ ĞºÑ‚Ğ¾ Ñ‚Ğ°ĞºÑ–Ñ”? Ñ Ğ²Ğ°Ñ Ğ½Ñ” Ğ·Ğ²Ğ°Ğ».\"\n",
      "548,hi,\"à¤¹à¤® à¤²à¤¾à¤¤ à¤®à¤¾à¤°à¤•à¤° à¤­à¤—à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤šà¤² à¤­à¤¾à¤— à¤¸à¥à¤…à¤° \",\"à¤¹à¤® à¤¬à¥‡à¤‡à¤œà¥à¤œà¤¤ à¤•à¤°à¤•à¥‡ à¤­à¤—à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤šà¤² à¤­à¤¾à¤—\",\"à¤¹à¤® à¤²à¤¾à¤¤ à¤®à¤¾à¤°à¤•à¤° à¤­à¤—à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤šà¤² à¤­à¤¾à¤— à¤¸à¥à¤…à¤°\"\n",
      "549,es,No los vio el open arms,El Open Arms no los avistÃ³.,\"',' No los vio el open arms\"\n",
      "550,ar,Ù„Ø³Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥ÙŠØ°Ø§Ø¡ Ø¹Ø§Ø¦Ù„Ø§ØªÙ‡Ù… Ø£ÙƒØ«Ø± Ù…Ù† Ø°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ Ù†Ø´Ø± ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø­ÙƒÙ…ÙŠØ© Ø¬Ø§Ù‡Ù„Ø©.,Ù„ÙŠØ³ Ø¹Ù„ÙŠÙƒ Ø£Ù† ØªØ¤Ø°ÙŠ Ø¹Ø§Ø¦Ù„Ø§ØªÙ‡Ù… Ø£ÙƒØ«Ø± Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¹Ù„Ù‰ Ù‚Ø±Ø§Ø±Ø§Øª Ø­ÙƒÙ…ÙŠØ©,<extra_id_0> Ù„Ø³Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥ÙŠØ°Ø§Ø¡ Ø¹Ø§Ø¦Ù„Ø§ØªÙ‡Ù… Ø£ÙƒØ«Ø± Ù…Ù† Ø°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ Ù†Ø´Ø± ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø­ÙƒÙ…ÙŠØ© Ø¬Ø§Ù‡Ù„Ø©\n",
      "551,am,á‹­áˆ„ áˆ°á‹á‹¬ áŠ¨á‹•á‹á‰€á‰µ áŠáƒ áŠá‹ áŒ­áŠ•á‰…áˆ‹á‰µ á‰ áŒ¥áˆ‹á‰» á‰°áˆáˆ‹ áˆ°á‹­áŒ£áŠ• áˆ½áŠ•á‰³áˆ áŠáŒˆáˆ­ áŠá‹,á‹­áˆ„ áˆ°á‹ á‰°áŒˆá‰¢á‹áŠ• áŒáŠ•á‹›á‰¤ á‹«áˆˆá‹áŠ“ áŠ¨á‹ˆáŒˆáŠ•á‰°áŠáŠá‰µ á‹¨áˆ«á‰€ áŠ á‹­á‹°áˆˆáˆ,áˆ°á‹­áŒ£áŠ• á‰ áŒ¥áˆ‹á‰» á‰°áˆáˆ‹ áˆ°á‹­áŒ£áŠ• áˆ½áŠ•á‰³áˆ áŠáŒˆáˆ­ áŠá‹\n",
      "552,am,á‰µáŒáˆ‹á‰¹ áŒˆá‹°áˆ áŒˆá‰£ áŠ á‹áˆ®á“ áŠ áˆœáˆªáŠ« áŠ áˆ¨á‰¥ áŠ áŒˆáˆ­ áˆ†áŠáˆ½ áŠá‹ áˆá‰µá‰³áŒˆá‹­ áŒˆáˆá‰±,á‹áŒ­ áˆ†áŠ– á‹¨áˆšá‹°áˆ¨áŒ á‰µáŒáˆ á‹áŒ¤á‰³áˆ› áŠ á‹­áˆ†áŠ•áˆá¡á¡,á‹ˆáˆ®á“ áŠ áˆœáˆªáŠ« áŠ áˆ¨á‰¥ áŠ áŒˆáˆ­ áˆ†áŠáˆ½ áŠá‹ áˆá‰µá‰³áŒˆá‹­ áŒˆáˆá‰±\n",
      "553,am,á‹­áˆ„áˆ áŠ áŠ•á‹µ áˆ›áˆ³á‹« áŠá‹ áŠ á‰¥á‹­ (áŠ¥áˆµáˆµá‰µ) áŠ¨á‰µáŒáˆ«á‹­ áˆ…á‹á‰¥ áŠ¥áŠ“ áŠ¨áŠ•á‰¥áˆ¨á‰± á‰ áˆ‹á‹­ áˆˆá‰³áŠ•áŠ­ á‹«áˆˆá‹ áá‰…áˆ­ áŠ¨áá‰°áŠ› áŠá‹á¢ á‰ á‰°á‹«á‹«á‹˜ á‹œáŠ“ ???? á‹¨á‰µáŒáˆ«á‹­ áŠ¥áŠ“á‰¶á‰½ áˆ…áƒáŠ“á‰µ áŠ¥áŠ•á‰£ áˆ²á‹«,á‹­áˆ„áˆ áŠ áŠ•á‹µ áˆ›áˆ³á‹« áŠá‹ áŠ á‰¥á‹­  áˆˆá‰µáŒáˆ«á‹­ áˆ…á‹á‰¥ áŠ¥áŠ“ áˆˆáŠ•á‰¥áˆ¨á‰±  á‹«áˆˆá‹ áá‰…áˆ­ áŠ¨áá‰°áŠ› áŠá‹á¢ á‰ á‰°á‹«á‹«á‹˜ á‹œáŠ“ ???? á‹¨á‰µáŒáˆ«á‹­ áŠ¥áŠ“á‰¶á‰½ áˆ…áƒáŠ“á‰µ,á‹¨á‰µáŒáˆ«á‹­ áˆ…á‹á‰¥ áŠ¥áŠ“ áŠ¨áŠ•á‰¥áˆ¨á‰± á‰ áˆ‹á‹­ áˆˆá‰³áŠ•áŠ­ á‹«áˆˆá‹ áá‰…áˆ­ áŠ¨áá‰°áŠ› áŠá‹á¢ á‰ á‰°á‹«á‹«á‹˜ á‹œáŠ“ ???? á‹¨á‰µáŒáˆ«á‹­ áŠ¥áŠ“á‰¶á‰½ áˆ…áƒáŠ“á‰µ áŠ¥áŠ•á‰£ áˆ²á‹«\n",
      "554,ru,\"Ğ§Ñ‚Ğ¾Ğ± ĞºĞ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğµ ĞºĞ¾Ğ»Ğ¾Ğ»Ğ°, Ğ±Ğ» Ğ°Ñ‚ÑŒ!\",Ğ§Ñ‚Ğ¾Ğ± ĞºĞ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğµ ĞºĞ¾Ğ»Ğ¾Ğ»Ğ°!,\"Ğ§Ñ‚Ğ¾Ğ± ĞºĞ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğµ ĞºĞ¾Ğ»Ğ¾Ğ»Ğ°, Ğ±Ğ» Ğ°Ñ‚ÑŒ!\"\n",
      "555,es,\"Le pega 16 tiros y le grita \"\"No te muevas\"\", menudo subnormal.\",\"Le dispara 16 veces y le grita \"\"No te muevas\"\", que individuo\",\"y le grita \"\"No te muevas\"\", menudo subnormal.\"\n",
      "556,ru,\"Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ, Ñ‚ĞµĞ±Ñ, Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ñ‚Ñ‹, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ² Ğ¶Ğ¾Ğ¿Ñƒ Ğ¿Ğ¾ÑĞ»Ğ°Ñ‚ÑŒ?\",\"Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ, Ñ‚ĞµĞ±Ñ, Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ñ‚Ñ‹, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ?\",\"<extra_id_0> Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ, Ñ‚ĞµĞ±Ñ, Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ñ‚Ñ‹, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ² Ğ¶Ğ¾Ğ¿Ñƒ Ğ¿Ğ¾ÑĞ»Ğ°Ñ‚ÑŒ?\"\n",
      "557,es,\"El chepas acaba de solucionar el problema de la inmigraciÃ³n, ahora vendrÃ¡n los de Vox a tocar los huevos...\",\"El chepas acaba de solucionar el problema de la inmigraciÃ³n, ahora vendrÃ¡n los de Vox a tocar molestar...\",<extra_id_0> ahora vendrÃ¡n los de Vox a tocar los huevos...\n",
      "558,zh,ä¸­å›½ç”·æ€§å°±æ˜¯è¢«äº”åƒå¹´çš„æ–‡åŒ–ç³Ÿç²•ç»Šä½äº†æ‰‹è„šã€‚ä½ çœ‹å¥³æ‹³ä¸€ç‚¹å„¿ä¼ ç»Ÿä¸è®²ã€‚å‡­å•¥ç”·äººè¿˜è¦è·Ÿå¥¹ä»¬è®²ä¼ ç»Ÿï¼Ÿè´Ÿè´£ä»»ï¼Ÿå…»å®¶ï¼Ÿå½©ç¤¼ï¼Ÿä¸ï¼Œè¿™ä¸€ç‚¹å„¿éƒ½ä¸å›½é™…åŒ–ã€‚??,ä¸­å›½ç”·æ€§å°±æ˜¯è¢«äº”åƒå¹´çš„æ–‡åŒ–ç³Ÿç²•ç»Šä½äº†æ‰‹è„šã€‚ä½ çœ‹æç«¯å¥³æƒä¸»ä¹‰è€…ä¸€ç‚¹å„¿ä¼ ç»Ÿä¸è®²ã€‚å‡­å•¥ç”·äººè¿˜è¦è·Ÿå¥¹ä»¬è®²ä¼ ç»Ÿï¼Ÿè´Ÿè´£ä»»ï¼Ÿå…»å®¶ï¼Ÿå½©ç¤¼ï¼Ÿä¸ï¼Œè¿™ä¸€ç‚¹å„¿éƒ½ä¸å›½é™…åŒ–ã€‚??,\"ä¸­å›½ç”·æ€§å°±æ˜¯è¢«äº”åƒå¹´çš„æ–‡åŒ–ç³Ÿç²•ç»Šä½äº†æ‰‹è„šã€‚ä½ çœ‹å¥³æ‹³ä¸€ç‚¹å„¿ä¼ ç»Ÿä¸è®²ã€‚å‡­å•¥ç”·äººè¿˜è¦è·Ÿå¥¹ä»¬è®²ä¼ ç»Ÿ?è´Ÿè´£ä»»?å…»å®¶?å½©ç¤¼?ä¸,è¿™ä¸€ç‚¹å„¿éƒ½ä¸å›½é™…åŒ–ã€‚??\"\n",
      "559,de,Ich lach mich kaputt Truppenuschi soll es richten.,Ich lach mich kaputt Ursula von der Leyen soll es richten.,lach mich kaputt Truppenuschi soll es richten\n",
      "560,hi,\"à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦ à¤‡à¤¸à¤¨à¥‡ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤•à¥‹ à¤•à¤¹à¤¾ à¤¸à¥‡ à¤•à¤¹à¤¾ à¤ªà¤¹à¥à¤à¤šà¤¾ à¤¦à¤¿à¤¯à¤¾,\",\"à¤‡à¤¸à¤¨à¥‡ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤•à¥‹ à¤•à¤¹à¤¾ à¤¸à¥‡ à¤•à¤¹à¤¾ à¤ªà¤¹à¥à¤à¤šà¤¾ à¤¦à¤¿à¤¯à¤¾,\",\"à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤•à¥‹ à¤•à¤¹à¤¾ à¤¸à¥‡ à¤•à¤¹à¤¾ à¤ªà¤¹à¥à¤à¤šà¤¾ à¤¦à¤¿à¤¯à¤¾,\"\n",
      "561,zh,æˆ‘å·²ç»ç´¯äº†ï¼Œä¹Ÿæ‡’å¾—è®²é“ç†äº†ï¼Œçˆ±å’‹å’‹åœ°å§ï¼Œezå’Œé»˜å…³æˆ‘ä»€ä¹ˆäº‹å•Šï¼Œæ€»æœ‰ä¸€å¤©ä¸­å›½è¦æ•´å‡ºä¸ªå³ç¿¼åˆ†å­,æˆ‘å·²ç»ç´¯äº†ï¼Œä¹Ÿæ‡’å¾—è®²é“ç†äº†ï¼Œçˆ±å’‹å’‹åœ°å§ï¼Œè½»æµ®å¥³æ€§å’Œé»‘äººå…³æˆ‘ä»€ä¹ˆäº‹å•Šï¼Œæ€»æœ‰ä¸€å¤©ä¸­å›½è¦æ•´å‡ºä¸ªå³ç¿¼åˆ†å­,\"æœ€è¿‘ç´¯äº†,ä¹Ÿæ‡’å¾—è®²é“ç†äº†,çˆ±å’‹å’‹åœ°å§,ezå’Œé»˜å…³æˆ‘ä»€ä¹ˆäº‹å•Š,æ€»æœ‰ä¸€å¤©ä¸­å›½è¦æ•´å‡ºä¸ªå³ç¿¼åˆ†å­\"\n",
      "562,hi,à¤œà¤¿à¤¸à¤•à¥‹ à¤®à¥‡à¤°à¥‡ à¤¶à¤¬à¥à¤¦ à¤–à¤°à¤¾à¤¬ à¤²à¤—à¥‡ à¤‰à¤¸à¤•à¥€ à¤®à¤¾ à¤•à¤¾ à¤­à¥‹à¤¸à¤¡à¤¼à¤¾,à¤œà¤¿à¤¸à¤•à¥‹ à¤®à¥‡à¤°à¥‡ à¤¶à¤¬à¥à¤¦ à¤–à¤°à¤¾à¤¬ à¤²à¤—à¥‡ à¤µà¥‹ à¤­à¤¾à¤¡à¤¼ à¤®à¥‡à¤‚ à¤œà¤¾à¤,à¤œà¤¿à¤¸à¤•à¥‹ à¤®à¥‡à¤°à¥‡ à¤¶à¤¬à¥à¤¦ à¤–à¤°à¤¾à¤¬ à¤²à¤—à¥‡ à¤‰à¤¸à¤•à¥€ à¤®à¤¾ à¤•à¤¾ à¤­à¥‹à¤¸à¤¡à¤¼à¤¾\n",
      "563,zh,è¿™é—®é¢˜è®©æˆ‘æ˜ç™½ä¸ºä»€ä¹ˆåŸºä½¬åœ¨å”å±±ä¸ä¼šè¢«æ‰“ã€‚åŸºä½¬æŸ“è‰¾åŸæ¥æ˜¯ä¸ºäº†é˜²èº«ï¼,è¿™é—®é¢˜è®©æˆ‘æ˜ç™½ä¸ºä»€ä¹ˆç”·åŒæ€§æ‹åœ¨å”å±±ä¸ä¼šè¢«æ‰“ã€‚ç”·åŒæ€§æ‹æ„ŸæŸ“è‰¾æ»‹ç—…åŸæ¥æ˜¯****ï¼,è¿™é—®é¢˜è®©æˆ‘æ˜ç™½ä¸ºä»€ä¹ˆåŸºä½¬åœ¨å”å±±ä¸ä¼šè¢«æ‰“ã€‚åŸºä½¬æŸ“è‰¾åŸæ¥æ˜¯ä¸ºäº†é˜²èº«!\n",
      "564,uk,\"ĞÑĞ¾Ğ±Ğ»Ğ¸Ğ²Ğ¾ Ğ² Ñ‚ĞµĞºÑÑ‚Ñ– Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ñƒ, Ğ±Ğ»ÑĞ´ÑŒ.\",ĞÑĞ¾Ğ±Ğ»Ğ¸Ğ²Ğ¾ Ğ² Ñ‚ĞµĞºÑÑ‚Ñ– Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ñƒ,\"<extra_id_0> Ğ² Ñ‚ĞµĞºÑÑ‚Ñ– Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ñƒ, Ğ±Ğ»ÑĞ´ÑŒ\"\n",
      "565,en,let darwinism take its course so that the least stupid shall inherit the earth .,Let darwinism take its course so that the more knowledgeable can inherit the earth,<extra_id_0> let darwinism take its course so that the least stupid will inherit the earth\n",
      "566,en,Then start quoting the damn cover instead of a message board!,Then please start quoting the cover instead of a message board.,quoting the damn cover instead of a message board!\n",
      "567,am,áŠ áŠ•á‰º á‹°á‹°á‰¥ áˆµáˆˆ áˆƒáŒˆáˆ­áˆ½ á‹¨áˆá‰³áˆµá‰¢ áŠ¨áˆ†áŠ á‹ˆáˆ¬áˆ½áŠ• áŠ¨áˆá‰µáŠáŠ áŠ áŠ•á‹µ á‹¨á‹³á‰¦ áˆˆá‰°áˆ«á‰ á‹ á‹¨á‰µáŒáˆ«á‹­ áˆáˆµáŠªáŠ‘ áˆ…á‹á‰¥ á‰¥á‰µáˆ¨áŒ…á‹ á‹­áˆ»áˆ‹áˆ,áˆµáˆˆ áˆƒáŒˆáˆ­áˆ½ á‹¨áˆá‰³áˆµá‰¢ áŠ¨áˆ†áŠ áˆŒáˆ‹ áŠáŒˆáˆ­ á‰µá‰°áˆ½  áŠ áŠ•á‹µ á‹¨á‹³á‰¦ áˆˆá‰°áˆ«á‰ á‹ á‹¨á‰µáŒáˆ«á‹­ áˆáˆµáŠªáŠ‘ áˆ…á‹á‰¥ á‰¥á‰µáˆ¨áŒ…á‹ á‹­áˆ»áˆ‹áˆ,áŠ áŠ•á‹µ á‹¨á‹³á‰¦ áˆˆá‰°áˆ«á‰ á‹ á‹¨á‰µáŒáˆ«á‹­ áˆáˆµáŠªáŠ‘ áˆ…á‹á‰¥ á‰¥á‰µáˆ¨áŒ…á‹ á‹­áˆ»áˆ‹áˆ\n",
      "568,ru,ÑÑ‚Ğ¾ Ñƒ Ğ½Ğ°Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹Ñ†Ñ‹ Ğ²ÑÑ‘ Ğ¿Ğ¸Ğ·Ğ´Ğ¸Ğ»Ğ¸ Ğ¸ Ğ·Ğ° ÑĞ²Ğ¾Ñ‘ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ğ»Ğ¸,ÑÑ‚Ğ¾ Ñƒ Ğ½Ğ°Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹Ñ†Ñ‹ Ğ²ÑÑ‘ Ğ²Ğ¾Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸ Ğ·Ğ° ÑĞ²Ğ¾Ñ‘ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ğ»Ğ¸,ĞºĞ¸Ñ‚Ğ°Ğ¹Ñ†Ñ‹ Ğ²ÑÑ‘ Ğ¿Ğ¸Ğ·Ğ´Ğ¸Ğ»Ğ¸ Ğ¸ Ğ·Ğ° ÑĞ²Ğ¾Ñ‘ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ğ»Ğ¸\n",
      "569,am,áˆ€áŒˆáˆ­áŠ“ áˆ˜áˆªáŠ• áˆ˜áˆˆá‹¨á‰µ áŠ¥áˆ› á‹­á‰½áˆ áˆ˜áˆ€á‹­áˆ áˆáˆ‹ áˆ˜áŠ•áŒáˆµá‰µ á‰ áˆ˜áŒ£ á‰ áˆ„á‹° á‰áŒ¥áˆ­ áŠ¢á‰µá‹®áŒµá‹« á‹­ááˆ¨áˆµ á‹­áˆ‹áˆ áˆ€áŒˆáˆ­ áˆ›áŠ•áˆ á‹­áˆ‚á‹µ á‹­áˆáŒ£ á‹˜áˆ‹áˆˆáˆ áˆá‰µáŠ–áˆ­ áŠ“á‰µá¢,áˆ€áŒˆáˆ­áŠ“ áˆ˜áˆªáŠ• áˆˆá‹­á‰°áŠ• áˆ›á‹¨á‰µ áˆ˜á‰»áˆ á¡á¡ á‹¨á‰µáŠ›á‹áˆ áˆ˜áŠ•áŒáˆµá‰µ á‰¢áˆ˜áŒ£áŠ“ á‰¢áˆ„á‹µ áŠ¢á‰µá‹®áŒµá‹« áŠ¥áŠ•á‹° áˆ€áŒˆáˆ­ áˆˆá‹˜áˆ‹áˆˆáˆ áˆá‰µáŠ–áˆ­ áŠ“á‰µá¢,áˆ˜áŠ•áŒáˆµá‰µ á‰ áˆ˜áŒ£ á‰ áˆ„á‹° á‰áŒ¥áˆ­ áŠ¢á‰µá‹®áŒµá‹« á‹­ááˆ¨áˆµ á‹­áˆ‹áˆ áˆ€áŒˆáˆ­ áˆ›áŠ•áˆ á‹­áˆ‚á‹µ á‹­á‰½áˆ áˆ˜áˆ€á‹­áˆ áˆáˆ‹ áˆ˜áŠ•áŒáˆµá‰µ á‰ áˆ˜áŒ£ á‰ áˆ„á‹° á‰áŒ¥áˆ­ áŠ¢á‰µá‹®áŒµá‹« á‹­ááˆ¨áˆµ á‹­áˆ‹áˆ áˆ€áŒˆáˆ­ áˆ›áŠ•áˆ á‹­áˆ‚á‹µ á‹­áˆáŒ£ á‹˜áˆ‹áˆˆáˆ áˆá‰µáŠ–áˆ­ áŠ“á‰µá¢\n",
      "570,ru,Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾ Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ±Ğ¾Ñ‘Ğ± Ğ»ĞµĞ½Ğ¸Ğ²Ñ‹Ğ¹ ! Ğ¶ĞµĞ½Ğ° ĞµĞ¼Ñƒ Ğ¿Ğ»Ğ¾Ñ…Ğ°...)))))),Ğ¢Ñ‹ Ğ»ĞµĞ½Ğ¸Ğ²Ñ‹Ğ¹.Ğ¶ĞµĞ½Ğ° ĞµĞ¼Ñƒ Ğ½Ğµ Ñ‚Ğ°ĞºĞ°Ñ,<extra_id_0> Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ±Ğ¾Ñ‘Ğ± Ğ»ĞµĞ½Ğ¸Ğ²Ñ‹Ğ¹ ! Ğ¶ĞµĞ½Ğ° ĞµĞ¼Ñƒ Ğ¿Ğ»Ğ¾Ñ…Ğ°...))))))\n",
      "571,zh,å›½é“éƒ½æœ‰è¿™æ ·å…‰æ˜çš„æœªæ¥,å›½å†…å¥³æ€§éƒ½æœ‰è¿™æ ·å…‰æ˜çš„æœªæ¥,å›½é“éƒ½æœ‰è¿™æ ·å…‰æ˜çš„æœªæ¥\n",
      "572,ru,\"Ğ’Ñ‡ĞµÑ€Ğ° ÑĞ¾ÑĞµĞ´ Ğº ÑĞµĞ±Ğµ Ğ² Ğ¾Ğ´Ğ½ÑƒÑˆĞºÑƒ Ğ´Ğ²ÑƒÑ… Ğ»Ğ°Ğ¼Ğ°Ğ½Ñ‚Ğ¸Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½Ñ‘Ñ, Ñ…Ğ· Ñ‡Ğµ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ, Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ, ÑĞ»Ğ°Ğ²Ğ° Ğ±Ğ¾Ğ³Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞºĞ¾Ñ€Ğ¾ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑÑ\",\"Ğ’Ñ‡ĞµÑ€Ğ° ÑĞ¾ÑĞµĞ´ Ğº ÑĞµĞ±Ğµ Ğ² Ğ¾Ğ´Ğ½ÑƒÑˆĞºÑƒ Ğ´Ğ²ÑƒÑ… Ğ»Ğ°Ğ¼Ğ°Ğ½Ñ‚Ğ¸Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½Ñ‘Ñ, Ğ½Ğµ Ğ·Ğ½Ğ°Ñ Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ, Ğ¿Ğ¾Ğ»Ğ¸Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞºĞ¾Ñ€Ğ¾ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑÑ\",\"<extra_id_0> ÑĞ¾ÑĞµĞ´ Ğº ÑĞµĞ±Ğµ Ğ² Ğ¾Ğ´Ğ½ÑƒÑˆĞºÑƒ Ğ´Ğ²ÑƒÑ… Ğ»Ğ°Ğ¼Ğ°Ğ½Ñ‚Ğ¸Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½Ñ‘Ñ, Ñ…Ğ· Ñ‡Ğµ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ, ÑĞ»Ğ°Ğ²Ğ° Ğ±Ğ¾Ğ³Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞºĞ¾Ñ€Ğ¾ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑÑ\"\n",
      "573,am,á‰³á‹²á‹« áŠ áŠ•á‰° á‰°áŒ‹áˆ© á•áŒ… áˆ‹á‹­ áˆáŠ• á‰³á‹°áˆ­áŒ‹áˆˆáˆ…???,á‰³á‹²á‹« áŠ áŠ•á‰° áŠ¥á‹šáˆ… á•áŒ… áˆ‹á‹­ áˆáŠ• á‰³á‹°áˆ­áŒ‹áˆˆáˆ…???,<extra_id_0> áŠ áŠ•á‰° á‰°áŒ‹áˆ© á•áŒ… áˆ‹á‹­ áˆáŠ• á‰³á‹°áˆ­áŒ‹áˆˆáˆ…???\n",
      "574,am,áŠ¥á‰¤á‰° áˆ˜áŠ•áŒáˆ¥á‰µ áˆ²áˆ˜áˆˆáˆ± áŠ¨á‰£áˆˆ á‰¤á‰µá‹‹ áŒ‹áˆ­ áŠ áŠ•á‹° áá‰¶ á‰°áŠáˆ± áŠ“ áŒ áˆ‹á‰µ á‹°áˆ° áŠ á‹­á‰ áˆˆá‹áŠ“ áˆˆáŠ á‹áˆªá‹ á‹¨áˆáŒá‰¥á‰µá¢,áŠ¨á‰¤á‰°áˆ˜áŠ•áŒáˆµá‰µ áŠ¨á‰£áˆˆ á‰¤á‰µá‹ áŒ‹áˆ­ áŠ áŠ•á‹µ áá‰¶ á‰°áŠáˆµá‰°á‹ á‰¢á‰³á‹­,<extra_id_0> áŠ¥á‰¤á‰° áˆ˜áŠ•áŒáˆ¥á‰µ áˆ²áˆ˜áˆˆáˆ± áŠ¨á‰£áˆˆ á‰¤á‰µá‹‹ áŒ‹áˆ­ áŠ áŠ•á‹° áá‰¶ á‰°áŠáˆ± áŠ“ áŒ áˆ‹á‰µ á‹°áˆ° áŠ á‹­á‰ áˆˆá‹áŠ“ áˆˆáŠ á‹áˆªá‹ á‹¨áˆáŒá‰¥á‰µá¢\n",
      "575,zh,å†›é˜€å‰²æ®å é¢†åå¤ï¼Œåˆ†åˆ«æ˜¯ï¼šåŒ—æ–¹äººï¼ˆä¸€èˆ¬åŒ—æ–¹äººã€è¾½å®ã€é»‘é¾™æ±Ÿï¼‰ã€å—æ–¹äººï¼ˆè¢«åœ°ç‹±ç‹—è”‘ç§°ä¸ºå—å¤§äººï¼‰ã€‚åŒ—æ–¹åˆ†è£‚ï¼Œå—æ–¹äººèµ¢ä¸€å±€ã€‚,å†›é˜€å‰²æ®å é¢†åå¤ï¼Œåˆ†åˆ«æ˜¯ï¼šåŒ—æ–¹äººï¼ˆä¸€èˆ¬åŒ—æ–¹äººã€è¾½å®ã€é»‘é¾™æ±Ÿï¼‰ã€å—æ–¹äººï¼ˆè¢«åœ°åŸŸæ­§è§†çš„äººè”‘ç§°ä¸ºå—å¤§äººï¼‰ã€‚åŒ—æ–¹åˆ†è£‚ï¼Œå—æ–¹äººèµ¢ä¸€å±€ã€‚,\"å†›é˜€å‰²æ®å é¢†åå¤,åˆ†åˆ«æ˜¯:åŒ—æ–¹äººã€è¾½å®ã€é»‘é¾™æ±Ÿã€å—æ–¹äºº(è¢«åœ°ç‹±ç‹—è”‘ç§°ä¸ºå—å¤§äºº)ã€‚åŒ—æ–¹åˆ†è£‚,å—æ–¹äººèµ¢ä¸€å±€ã€‚\"\n",
      "576,zh,è¿™ç§ä¸œè¥¿éƒ½èƒ½ç…§åšçš„ä¸çŸ¥é“æ˜¯æ²¡è„Šæ¤è¿˜æ˜¯æ²¡è„‘å­ï¼Œä¸æ€•ä¸€ä¸‡å°±æ€•ä¸‡ä¸€ï¼Œä¸‡ä¸€è€å©†æ˜¯ä¸ªä»™å¥³....,è¿™ç§ä¸œè¥¿éƒ½èƒ½ç…§åšçš„ä¸çŸ¥é“æ˜¯æ²¡è„Šæ¤è¿˜æ˜¯æ²¡è„‘å­ï¼Œä¸æ€•ä¸€ä¸‡å°±æ€•ä¸‡ä¸€ï¼Œä¸‡ä¸€è€å©†æ˜¯ä¸ªè‡ªæˆ‘æ„Ÿè§‰è¿‡äºè‰¯å¥½çš„å¥³æ€§....,\"è¿™ç§ä¸œè¥¿éƒ½èƒ½ç…§åšçš„ä¸çŸ¥é“æ˜¯æ²¡è„Šæ¤è¿˜æ˜¯æ²¡è„‘å­,ä¸æ€•ä¸€ä¸‡å°±æ€•ä¸‡ä¸€,ä¸‡ä¸€è€å©†æ˜¯ä¸ªä»™å¥³....\"\n",
      "577,zh,è·Ÿä½ ä»¬åŸºä½¬å­¦çš„??,è·Ÿä½ ä»¬ç”·åŒæ€§æ‹å­¦çš„??,è·Ÿä½ ä»¬åŸºä½¬å­¦çš„??\n",
      "578,ru,\"Ñ‡ĞµĞ¼ Ğ±Ñ‹ Ğ´Ğ¸Ñ‚Ñ Ğ½Ğµ Ñ‚ĞµÑˆĞ¸Ğ»Ğ¾ÑÑŒ,Ğ»Ğ¸ÑˆÑŒ Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ»Ğ°ĞºĞ°Ğ»Ğ¾! Ğ´ĞµĞ³Ñ‚ÑÑ€ĞµĞ²-Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ¼Ğ¾Ğ½, ĞºÑƒĞ´Ğ° Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·?! Ğ½Ğ°Ñ€Ğ¾Ğ´ Ğ½ĞµĞ´ÑƒÑ€Ğ°ĞºĞ¸, Ğ²ÑĞµ Ğ²Ğ°Ğ¼ Ğ²Ñ‹Ğ»ĞµĞ·ĞµÑ‚ ĞµĞ´Ñ€Ğ¾ÑĞ°Ğ¼-Ğ¼Ğ¸Ñ‚Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸!\",\"Ñ‡ĞµĞ¼ Ğ±Ñ‹ Ğ´Ğ¸Ñ‚Ñ Ğ½Ğµ Ñ‚ĞµÑˆĞ¸Ğ»Ğ¾ÑÑŒ,Ğ»Ğ¸ÑˆÑŒ Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ»Ğ°ĞºĞ°Ğ»Ğ¾! Ğ´ĞµĞ³Ñ‚ÑÑ€ĞµĞ², ĞºÑƒĞ´Ğ° Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·?! Ğ½Ğ°Ñ€Ğ¾Ğ´ Ğ½ĞµĞ´ÑƒÑ€Ğ°ĞºĞ¸, Ğ²ÑĞµ Ğ²Ğ°Ğ¼ Ğ²Ñ‹Ğ»ĞµĞ·ĞµÑ‚!\",\"Ğ¾Ğ½ĞµĞ·-Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ¼Ğ¾Ğ½, ĞºÑƒĞ´Ğ° Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·?! Ğ½Ğ°Ñ€Ğ¾Ğ´ Ğ½ĞµĞ´ÑƒÑ€Ğ°ĞºĞ¸, Ğ²ÑĞµ Ğ²Ğ°Ğ¼ Ğ²Ñ‹Ğ»ĞµĞ·ĞµÑ‚ ĞµĞ´Ñ€Ğ¾ÑĞ°Ğ¼-Ğ¼Ğ¸Ñ‚Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸!\"\n",
      "579,ar,Ø§Ù„Ù†Ø§Ø³ ÙŠÙ†Ø´Ø±ÙˆÙ† Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ ÙŠÙØ¹Ù„ Ø§Ù„Ù…Ù„Ø§Ø¹ÙŠÙ† Ù‡Ø°Ø§ ÙˆØ°Ø§ÙƒØŒ Ù…Ø²Ø¹Ø¬.,Ø§Ù„Ù†Ø§Ø³ ÙŠÙ†Ø´Ø±ÙˆÙ† Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ ÙŠÙØ¹Ù„ Ø¨Ø¹Ø¶Ù‡Ù… Ù‡Ø°Ø§ ÙˆØ°Ø§ÙƒØŒ Ù…Ø²Ø¹Ø¬.,<extra_id_0> Ù„Ø§ ÙŠÙØ¹Ù„ Ø§Ù„Ù…Ù„Ø§Ø¹ÙŠÙ† Ù‡Ø°Ø§ ÙˆØ°Ø§ÙƒØŒ Ù…Ø²Ø¹Ø¬.\n",
      "580,de,\"#illner Leute, die Parteien wÃ¤hlen, die sie zu \"\"schon lÃ¤nger hier Lebenden\"\" herabwÃ¼rdigen, sind schlicht #verblÃ¶det od. #geisteskrank\",\"#illner Leute, die Parteien wÃ¤hlen, die sie zu \"\"schon lÃ¤nger hier Lebenden\"\" herabwÃ¼rdigen, sind schlicht #naiv\",\"<extra_id_0> #illner Leute, die Parteien wÃ¤hlen, die sie zu \"\"schon lÃ¤nger hier Lebenden\"\" herabwÃ¼rdigen, sind schlicht #verblÃ¶det od. #geisteskrank\"\n",
      "581,ru,ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ğ° Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ² ÑÑÑÑ€!,ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ² ÑÑÑÑ€!,<extra_id_0> ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸Ğ´Ğ¾Ñ€Ğ° Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑÑÑÑ€!\n",
      "582,uk,\"Ğ§ÑƒĞ²Ğ°Ğº, ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ·ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾ Ğ½Ğ°Ğ¼Ğ°Ğ³Ğ°Ğ²ÑÑ Ğ²Ğ¼Ğ¾Ğ²Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ Ğ½Ğ° ÑĞµĞºÑ, Ğ¿Ñ–ÑˆĞ¾Ğ² Ğ²Ğ°-Ğ±Ğ°Ğ½Ğº Ñ– Ñ‚ĞµĞ¿ĞµÑ€ Ğ·Ğ°Ğ¼Ñ–ÑÑ‚ÑŒ Ğ½Ğ¸Ñ‚Ñ‚Ñ Ğ¿Ñ€Ğ¾ ÑĞ²Ñ–Ğ¹ Ğ½ĞµĞ´Ğ¾Ğ¹Ğ¾Ğ± ĞºĞ¸Ğ´Ğ°Ñ” Ğ¼ĞµĞ½Ñ– ĞĞĞ•ĞšĞ”ĞĞ¢Ğ˜.\",\"Ğ§ÑƒĞ²Ğ°Ğº, ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ·ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾ Ğ½Ğ°Ğ¼Ğ°Ğ³Ğ°Ğ²ÑÑ Ğ²Ğ¼Ğ¾Ğ²Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ Ğ½Ğ° ÑĞµĞºÑ, Ğ¿Ñ–ÑˆĞ¾Ğ² Ğ²Ğ°-Ğ±Ğ°Ğ½Ğº Ñ– Ñ‚ĞµĞ¿ĞµÑ€ Ğ·Ğ°Ğ¼Ñ–ÑÑ‚ÑŒ Ğ½Ğ¸Ñ‚Ñ‚Ñ  ĞºĞ¸Ğ´Ğ°Ñ” Ğ¼ĞµĞ½Ñ– ĞĞĞ•ĞšĞ”ĞĞ¢Ğ˜.\",\"<extra_id_0>, ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ·ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾ Ğ½Ğ°Ğ¼Ğ°Ğ³Ğ°Ğ²ÑÑ Ğ²Ğ¼Ğ¾Ğ²Ğ¸Ñ‚Ğ¸ Ğ¼ĞµĞ½Ğµ Ğ½Ğ° ÑĞµĞºÑ, Ğ¿Ñ–ÑˆĞ¾Ğ² Ğ²Ğ°-Ğ±Ğ°Ğ½Ğº Ñ– Ñ‚ĞµĞ¿ĞµÑ€ Ğ·Ğ°Ğ¼Ñ–ÑÑ‚ÑŒ Ğ½Ğ¸Ñ‚Ñ‚Ñ Ğ¿Ñ€Ğ¾ ÑĞ²Ñ–Ğ¹ Ğ½ĞµĞ´Ğ¾Ğ¹Ğ¾Ğ± ĞºĞ¸Ğ´Ğ°Ñ” Ğ¼ĞµĞ½Ñ– ĞĞĞ•ĞšĞ”ĞĞ¢Ğ˜\"\n",
      "583,ar,Ù‡Ùˆ Ø§Ù„Ø³Ù„Ø§Ù… Ø¨ØªØ§Ø¹ÙƒÙ… Ø®Ø§Øµ Ø¨Ø§Ù„ØµÙ‡Ø§ÙŠÙ†Ø© ÙÙ‚Ø· Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ÙˆØ¹Ù„Ù‰ Ø³Ù„Ø§Ù…ÙƒÙ…,Ù‡Ùˆ Ø§Ù„Ø³Ù„Ø§Ù… Ø¨ØªØ§Ø¹ÙƒÙ… Ø®Ø§Øµ Ø¨Ø§Ù„ØµÙ‡Ø§ÙŠÙ†Ø© ÙÙ‚Ø· ØŒ ÙŠØ§ Ù„Ù‡ Ù…Ù† Ø³Ù„Ø§Ù…,Ù„Ø¹Ù†Ø© Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒÙ… ÙˆØ¹Ù„Ù‰ Ø³Ù„Ø§Ù…ÙƒÙ…\n",
      "584,es,\"No busquÃ©is mÃ¡s, ya tenemos al tonto del hilo.\",\"No busqueis mas, aqui esta quien faltaba\",<extra_id_0> no busquÃ©is mÃ¡s ya tenemos al tonto del hilo\n",
      "585,en,That had more content and was going to be an article until you deleted it again dumbass.,That had more content and was going to be an article until you deleted it again.,<extra_id_0> that had more content and was going to be an article until you delete it again dumbass.\n",
      "586,hi,à¤¸à¥‚à¤…à¤° à¤²à¥‹à¤— à¤…à¤ªà¤¨à¤¾ à¤…à¤¸à¥à¤¤à¤¿à¤¤à¥à¤µ à¤¬à¤šà¤¾à¤¨à¥‡ à¤®à¥‡à¤‚ à¤²à¤—à¥‡ à¤¹à¥ˆà¤‚à¥¤à¥¤,à¤¬à¥à¤°à¥‡ à¤²à¥‹à¤— à¤…à¤ªà¤¨à¥‡ à¤…à¤¸à¥à¤¤à¤¿à¤¤à¥à¤µ à¤•à¥‹ à¤¬à¤šà¤¾à¤¨à¥‡ à¤®à¥‡à¤‚ à¤²à¤—à¥‡ à¤¹à¥ˆà¤‚ |,<extra_id_0> à¤²à¥‹à¤— à¤…à¤ªà¤¨à¤¾ à¤…à¤¸à¥à¤¤à¤¿à¤¤à¥à¤µ à¤¦à¥‡à¤¨à¥‡ à¤¹à¥ˆà¤‚\n",
      "587,es,Los mas valientes. Haz que pase!,\"Los mÃ¡s valientes. Por favor, asegÃºrate de que avance\",<extra_id_0> los Los Los Los hombres que pase!\n",
      "588,en,queer ur gay i hope u read this,I hope you read this.,i hope u read this\n",
      "589,zh,è¯·è§†è¯´è¿™ç§äººä¸ºå¼±æ™º,è¯·è§†è¯´è¿™ç§äººä¸ºä¸æ˜æ™ºçš„äºº,è¯·è§†è¯´è¿™ç§äººä¸ºå¼±æ™º\n",
      "590,uk,\"ĞŸÑ–Ğ·Ğ´Ğ° ĞºĞ°Ñ‚ÑŒĞ¾Ğ½ĞºÑƒ, Ğ±Ñ–Ğ»ÑŒÑˆĞµ ÑÑ€Ğ°Ñ‚ÑŒ Ğ½Ğµ Ğ±ÑƒĞ´Ğµ.\",\"ĞšÑ–Ğ½ĞµÑ†ÑŒ ĞºĞ°Ñ‚ÑŒĞ¾Ğ½ĞºÑƒÑ–, Ğ±Ñ–Ğ»ÑŒÑˆĞµ ĞºĞ°ĞºĞ°Ñ‚Ğ¸ Ğ½Ğµ Ğ±ÑƒĞ´Ğµ.\",\"Ğ¾Ğ½ĞµĞ·ĞºÑƒ, Ğ±Ñ–Ğ»ÑŒÑˆĞµ ÑÑ€Ğ°Ñ‚ÑŒ Ğ½Ğµ Ğ±ÑƒĞ´Ğµ.\"\n",
      "591,hi,à¤¯à¥‡ à¤°à¤‚à¤¡à¥€ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤•à¥€ à¤¬à¤¡à¤¼à¥€ à¤¦à¥à¤¶à¥à¤®à¤¨à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤à¤•,à¤¯à¥‡ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤•à¥€ à¤¬à¤¡à¤¼à¥€ à¤¦à¥à¤¶à¥à¤®à¤¨à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤à¤•,<extra_id_0> à¤¯à¥‡ à¤°à¤‚à¤¡à¥€ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤•à¥€ à¤¬à¤¡à¤¼à¥€ à¤¦à¥à¤¶à¥à¤®à¤¨à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤à¤•\n",
      "592,am,áˆ°á‹áˆª áŠ¥áˆ± á‹µáˆ®áˆ áŠá‰ áˆ¨ áŠ®á‰°á‰µáˆ½áŠ• á‹­á‹›áˆ½ áŒáŒƒáˆ áˆ›áˆ„á‹µ á‰µá‰½á‹«áˆˆáˆ½,áŠ¥áˆ± á‹µáˆ®áˆ áŠá‰ áˆ¨ áŠ¥á‰ƒáˆ½ á‹­á‹›áˆ½ áˆ€áŒˆáˆ­áˆ½ áˆ›áˆ„á‹µ á‰µá‰½á‹«áˆˆáˆ½,á‹µáˆ®áˆ áŠá‰ áˆ¨ áŠ®á‰°á‰µáˆ½áŠ• á‹­á‹›áˆ½ áŒáŒƒáˆ áˆ›áˆ„á‹µ á‰µá‰½á‹«áˆˆ\n",
      "593,en,legacy of being a selfish shit head .,Legacy of being a selfish person,<extra_id_0> legacy of being a selfish shit head\n",
      "594,de,\"Helmut Schmidt hatte allein mehr Verstand, als die gesamte Berliner Muschpoke zusammen ğŸ˜œ\",\"Helmut Schmidt hatte allein mehr Verstand, als die gesamte Berliner BevÃ¶lkerung zusammen ğŸ˜œ\",\"<extra_id_0> Helmut Schmidt hatte allein mehr Verstand, als die gesamt Berliner Muschpoke zusammen ğŸ˜œ\"\n",
      "595,uk,Ğ¦Ğ• Ğ¯ĞšĞ˜Ğ™Ğ¡Ğ¬ Ğ¡Ğ¢Ğ¬ĞĞ‘ Ğ’Ğ†Ğ” Ğ ĞĞ—Ğ ĞĞ‘ĞĞ˜ĞšĞ†Ğ’,Ğ¦Ğµ ÑĞºĞµÑÑŒ Ğ·Ğ½ÑƒÑ‰Ğ°Ğ½Ğ½Ñ Ğ²Ñ–Ğ´ Ñ€Ğ¾Ğ·Ñ€Ğ¾Ğ±Ğ½Ğ¸ĞºÑ–Ğ²,Ğ¦Ğ• Ğ¯ĞšĞ˜Ğ™Ğ¡Ğ¬ Ğ¡Ğ¢Ğ¬ĞĞ‘Ğ†Ğ” Ğ ĞĞ—Ğ ĞĞ‘ĞĞ˜ĞšĞ†Ğ’\n",
      "596,uk,\"ĞŸĞ¸ÑˆĞµ: \"\"Ğ¯ Ğ´ÑƒĞ¼Ğ°Ğ»Ğ° Ñ…ÑƒĞ¹Ğ½Ñ Ğ² Ğ¼ĞµĞ½Ğµ Ğ² Ğ¶Ğ¸Ñ‚Ñ‚Ñ– Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ–ÑÑ‚ÑŒ ÑĞµĞºÑÑƒ, Ğ¿Ğ¾Ñ€Ñ–ÑˆĞ°Ğ»Ğ° Ğ· ÑĞµĞºÑĞ¾Ğ¼, Ğ° Ñ…ÑƒĞ¹Ğ½Ñ Ğ»Ğ¸ÑˆĞ¸Ğ»Ğ°ÑÑ.\",\"ĞŸĞ¸ÑˆĞµ: \"\"Ğ¯ Ğ´ÑƒĞ¼Ğ°Ğ»Ğ° Ğ² Ğ¼ĞµĞ½Ğµ Ğ¿Ğ¾Ğ³Ğ°Ğ½Ğ¾ Ğ² Ğ¶Ğ¸Ñ‚Ñ‚Ñ– Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ–ÑÑ‚ÑŒ ÑĞµĞºÑÑƒ, Ğ¿Ğ¾Ñ€Ñ–ÑˆĞ°Ğ»Ğ° Ğ· ÑĞµĞºÑĞ¾Ğ¼, Ğ° Ğ½Ñ–Ñ‡Ğ¾Ğ³Ğ¾ Ğ½Ğµ Ğ·Ğ¼Ñ–Ğ½Ğ¸Ğ»Ğ¾ÑÑ.\",\"Ñ Ğ´ÑƒĞ¼Ğ°Ğ»Ğ° Ñ…ÑƒĞ¹Ğ½Ñ Ğ² Ğ¼ĞµĞ½Ğµ Ğ² Ğ¶Ğ¸Ñ‚Ñ‚Ñ– Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ–ÑÑ‚ÑŒ ÑĞµĞºÑÑƒ, Ğ¿Ğ¾Ñ€Ñ–ÑˆĞ°Ğ»Ğ° Ğ· ÑĞµĞºÑĞ¾Ğ¼, Ğ° Ñ…ÑƒĞ¹Ğ½Ñ Ğ»Ğ¸ÑˆĞ¸Ğ»Ğ°ÑÑ.\"\n",
      "597,de,Hier haben einige Politiker richtige abgesahnt..... Und der Oberguru Maschmayer macht jetzt einen auf Fernsehstar...und produziert seiner alten b-Movies. ğŸ˜ƒğŸ˜ƒ,Hier haben einige Politiker richtige abgesahnt..... Und der Maschmayer macht jetzt einen auf Fernsehstar...und produziert seiner alten b-Movies. ğŸ˜ƒğŸ˜ƒ,<extra_id_0> haben einige Politiker richtig abgesahnt..... Und der Oberguru Maschmayer macht jetzt einen auf Fernsehstar...und produziert seiner alten b-Movies. ğŸ˜ƒğŸ˜ƒ\n",
      "598,de,\"Dummes Gelaber, Kern. Kurz stellt sich nicht gegen die Italiener. Er. will nur 200.000 Invasoren nicht nach Ã–sterreich einreisen lassen.\",\"Unsinnige Aussage, Kern. Kurz stellt sich nicht gegen die Italiener. Er. will nur 200.000 Einwanderer nicht nach Ã–sterreich einreisen lassen.\",<extra_id_0> stellt sich nicht gegen Italiener. Er. will nur 200.000 Invasoren nicht nach Ã–sterreich einreisen lassen.\n",
      "599,es,lo personal es polÃ­tico pero no por ello tienes ke ir a una manifa ke convocan sindicatos de mierda,Lo personal es polÃ­tico pero eso no significa que haya que ir a una manifestaciÃ³n que convoque a sindicatos.,<extra_id_0> lo personal es polÃ­tico pero no por ello tienes ke ir a una manifa ke convocan sindicatos de mierda\n",
      "600,zh,ç¥ç»ç—…ï¼Œè¿è°åˆ ä½ éƒ½ä¸çŸ¥çš„è´¨æ–è¿˜æ¥è£…ï¼Œç¬”,ç¥ç»ç—…ï¼Œè¿è°åˆ ä½ éƒ½ä¸çŸ¥çš„ä¸æ˜æ™ºçš„äººè¿˜æ¥è£…ï¼Œç¬”,\"ç»ªç—…,è¿è°åˆ ä½ éƒ½ä¸çŸ¥çš„è´¨æ–è¿˜æ¥è£…,ç¬”\"\n",
      "601,es,Pero sois periodistas o idiotas? AÃºn estando en catalÃ¡n pone Franquismo!,Son periodistas o quÃ© aÃºn estando en catalÃ¡n ponen franquismo,<extra_id_0> sois periodistas o idiotas? AÃºn estando en catalÃ¡n pone Franquismo!\n",
      "602,es,\"Jajajajaja turco, subnormal, cuÃ±ado e espaÃ±ol madremÃ­a.... PLENOOOO\",\"Jajajajaja turco, cuÃ±ado e espaÃ±ol madremÃ­a.... PLENOOOO\",\"<extra_id_0>, subnormal, cuÃ±ado e espaÃ±ol madremÃ­a.... PLENOOOO\"\n",
      "603,zh,æ¯”å¦‚åœ¨çˆ¶æ¯é¢å‰æŠ½çƒŸçš„å¤§å­å­è¡Œä¸º,æ¯”å¦‚åœ¨çˆ¶æ¯é¢å‰æŠ½çƒŸçš„ä¸å­è¡Œä¸º,æ¯”å¦‚åœ¨çˆ¶æ¯é¢å‰æŠ½çƒŸçš„å¤§å­å­è¡Œä¸º\n",
      "604,am,áˆµá‹©áˆ á‰°áˆ¾áˆ˜ á‹áˆ á‰¥áˆ á‹¨áˆšáˆˆáˆáˆá áŠá‰ áˆ­ á‹¨áˆ˜áˆ°áˆˆáŠ:: á‹°áˆ˜á‰€ á‹¨áˆšáˆ‰á‰µ áˆá‹›á‹› áŒáŠ• á‰£áˆˆáŒŒá‹áŠ• áŒá‰µá‰¶ á‰¤áŠ’áˆ»áŠ•áŒ‰áˆ á‹­á‹á‰µ áˆ²áˆ„á‹µ áˆ³á‹­ áŠá‹ áŠ áˆ€ á‹«áˆáŠ©á‰µ:: á‰°áˆ« áŒ€áŒáŠ“,áˆµá‹©áˆ á‰°áˆ¾áˆ˜ á‹«áˆˆ áˆ›áˆµáˆ¨áŒƒ á‹¨áˆšáŠ“áŒˆáˆ­ áŠá‰ áˆ­ á‹¨áˆ˜áˆ°áˆˆáŠ:: áŠ á‰¶ á‹°áˆ˜á‰€ áŒáŠ• áˆ°á‹á‹¬á‹áŠ• á‰¤áŠ’áˆ»áŠ•áŒ‰áˆ á‹­á‹á‰µ áˆ²áˆ„á‹µ áˆ³á‹­ áŠá‹ áŠ áˆ€ á‹«áˆáŠ©á‰µ::,áˆµá‹©áˆ á‰°áˆ¾áˆ˜ á‹áˆ á‰¥áˆ á‹¨áˆšáˆˆáˆáˆá áŠá‰ áˆ­ á‹¨áˆ˜áˆ°áˆˆáŠ:: á‰°áˆ« áŒ€áŒáŠ“\n",
      "605,ar,Ø§Ù„Ø§ØºØªØµØ§Ø¨ Ù‡Ùˆ Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¬Ù†Ø³ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø£ÙŠ Ø¬Ù†Ø³ØŒ Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† ÙˆØ¬ÙˆØ¯ Ù‚Ø¶ÙŠØ¨ Ø£Ùˆ Ø¹Ø¯Ù…Ù‡!,Ø§Ù„Ø§ØºØªØµØ§Ø¨ Ù‡Ùˆ Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¬Ù†Ø³ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø£ÙŠ Ø¬Ù†Ø³!,<extra_id_0> Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¬Ù†Ø³ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø£ÙŠ Ø¬Ù†Ø³ Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† ÙˆØ¬ÙˆØ¯ Ù‚Ø¶ÙŠØ¨ Ø£Ùˆ Ø¹Ø¯Ù…Ù‡!\n",
      "606,uk,Ğ¢Ğ° ÑÑƒĞºĞº Ğ¿Ğ¸Ğ·Ğ´ÑƒÑ Ğ· ÑĞ»ÑŒĞ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾Ñ‡Ğ°Ñ…,Ğ¢Ğ° Ñ–Ğ´Ñƒ Ğ· ÑĞ»ÑŒĞ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾Ñ‡Ğ°Ñ…,<extra_id_0> Ñ‚Ğ° ÑÑƒĞºĞº Ğ¿Ğ¸Ğ·Ğ´ÑƒÑ\n",
      "607,en,\"could ve fooled me , all governments are corrupt as fuck .\",All governments are corrupt,<extra_id_0> governments are corrupt as fuck .\n",
      "608,am,áŠ á‰¥áˆ® áˆ˜áŠ–áˆ­áŠ• áŠ á‹áˆª áŒ¥áˆ‹á‰»áŠ• áŠ á‰µáˆµá‰ áŠª á‹¨á‰¥á‹™áˆáŠ• áˆ€áŒˆáˆ­ áŠ“á‰µ áŠ¢á‰µá‹®áŒµá‹« á‰ áŒ­áŠ•á‰…áˆ‹á‰µ áŠ¥á‹°áŒŠ,áŠ á‰¥áˆ® áˆ˜áŠ–áˆ­áŠ• áŠá‹ áˆ›á‹áˆ«á‰µ á‹«áˆˆá‰¥áˆ½ á‹¨á‰¥á‹™áˆáŠ• áˆ€áŒˆáˆ­ áŠ“á‰µ áŠ¢á‰µá‹®áŒµá‹«,á‹ˆáˆª á‹¨á‰¥á‹™áˆáŠ• áˆ€áŒˆáˆ­ áŠ“á‰µ áŠ¢á‰µá‹®áŒµá‹« á‰ áŒ­áŠ•á‰…áˆ‹á‰µ áŠ¥á‹°áŒŠ\n",
      "609,es,Yo soy asÃ­ puto moro,Yo soy asÃ­,ç»ª soy asÃ­ puto moro\n",
      "610,zh,é»‘æ²³å—å’ŒåŒ—æ–¹ æœ€ä¸¥é‡çš„æ˜¯è¥¿å—åœ°åŒºåŠä¸œåŒ—åœ°åŒº éƒ½æ˜¯ä¸€ç¾¤å—è›®æçš„é¬¼ ä»–ä»¬æ²¡ä»€ä¹ˆæœ¬äº‹ åªä¼šåœ¨ç½‘ç»œä¸Šç ´åæ°‘æ—å›¢ç»“ æå›½å®¶åˆ†è£‚ å› ä¸ºä»–ä»¬æœ‰ç€ä¸ä¸ºäººçŸ¥çš„ä¼˜è¶Šæ„Ÿï¼,é»‘æ²³å—å’ŒåŒ—æ–¹ è¾ƒæ™®éçš„æ˜¯è¥¿å—åœ°åŒºåŠä¸œåŒ—åœ°åŒº éƒ½æ˜¯ä¸€äº›å—æ–¹äººå‚ä¸çš„ ä»–ä»¬å¯èƒ½èƒ½åŠ›æœ‰æ‰€æ¬ ç¼º ä¼šåœ¨ç½‘ç»œä¸Šç ´åæ°‘æ—å›¢ç»“ æå›½å®¶åˆ†è£‚ å› ä¸ºä»–ä»¬æœ‰ç€ä¸ä¸ºäººçŸ¥çš„ä¼˜è¶Šæ„Ÿï¼,é»‘æ²³å—å’ŒåŒ—æ–¹ æœ€ä¸¥é‡çš„æ˜¯è¥¿å—åœ°åŒºåŠä¸œåŒ—åœ°åŒº éƒ½æ˜¯ä¸€ç¾¤å—è›®æçš„é¬¼ ä»–ä»¬æ²¡ä»€ä¹ˆæœ¬äº‹ åªä¼šåœ¨ç½‘ç»œä¸Šç ´åæ°‘æ—å›¢ç»“ æå›½å®¶åˆ†è£‚ æå›½å®¶åˆ†è£‚ å› ä¸ºä»–ä»¬æœ‰ç€ä¸ä¸ºäººçŸ¥çš„ä¼˜è¶Šæ„Ÿ!\n",
      "611,ru,\"ÑÑƒĞºĞ° Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ²ÑÑ‘ Ğ² Ğ±ÑƒĞ½ĞºĞµÑ€ ÑƒÑ…Ğ¾Ğ´Ğ¸Ğ»Ğ¾, Ñ‰Ğ°Ñ Ğ¶Ğ¼Ğ¾Ñ‚ÑŒÑ‘ Ğ±Ğ»Ñ ÑĞ´ĞµĞ»Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ ÑƒÑ…Ğ¾Ğ´Ğ¸Ñ‚, ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ÑĞ¸Ğ´Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑÑ‚Ğ²Ğ¾ :-d Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ñ‹ ÑƒĞµĞ±Ğ°Ğ½ĞºĞ¸ ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ\",\"Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ²ÑÑ‘ Ğ² Ğ±ÑƒĞ½ĞºĞµÑ€ ÑƒÑ…Ğ¾Ğ´Ğ¸Ğ»Ğ¾, Ñ‰Ğ°Ñ ÑĞ´ĞµĞ»Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ ÑƒÑ…Ğ¾Ğ´Ğ¸Ñ‚, ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ÑĞ¸Ğ´Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑÑ‚Ğ²Ğ¾ :-d \",\"Ñ‰Ğ°Ñ Ğ²ÑÑ‘ Ğ² Ğ±ÑƒĞ½ĞºĞµÑ€ ÑƒÑ…Ğ¾Ğ´Ğ¸Ğ»Ğ¾, Ñ‰Ğ°Ñ Ğ¶Ğ¼Ğ¾Ñ‚ÑŒÑ‘ Ğ±Ğ»Ñ ÑĞ´ĞµĞ»Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ ÑƒÑ…Ğ¾Ğ´Ğ¸Ñ‚, ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ÑĞ¸Ğ´Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑÑ‚Ğ²Ğ¾ :-d\"\n",
      "612,hi,à¤‰à¤¨à¤®à¥‡à¤‚ à¤à¤• à¤¯à¥‡ à¤Ÿà¥‹à¤Ÿà¤¿à¤šà¥‹à¤° à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆ !!!,à¤‰à¤¨à¤®à¥‡à¤‚ à¤à¤• à¤¯à¥‡ à¤¬à¥‡à¤•à¤¾à¤° à¤†à¤¦à¤®à¥€ à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆ !!!,<extra_id_0> à¤‰à¤¨à¤®à¥‡à¤‚ à¤¯à¥‡à¤Ÿà¤¿à¤šà¥‹à¤° à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆ !!!\n",
      "613,en,You are an idiot and you can't block anyone.,You can't block anyone.,<extra_id_0> You are an idiot and you can't block anyone.\n",
      "614,zh,çŒ©çŒ©éƒ½æ¯”é»‘??å¥½çœ‹,æˆ‘ä¸ªäººè§‰å¾—é»‘ç§äººä¸ç¬¦åˆæˆ‘çš„å®¡ç¾,çŒ©çŒ©éƒ½æ¯”é»‘??å¥½çœ‹\n",
      "615,uk,\"ĞÑ…ÑƒÑ”Ğ½Ğ½Ñ– Ğ¼Ğ°Ğ¿Ğ¸ ĞŸĞ¾Ğ»ÑŒÑ‰Ñ–, Ğ¼Ğ¾Ñ— ÑƒĞ»ÑĞ±Ğ»ĞµĞ½Ñ– Ğ¿Ñ€Ğ¾ Ñ‚ÑƒÑ€Ğ¸Ğ·Ğ¼, Ğ½Ğ°Ğ¹Ğ±Ñ–Ğ»ÑŒÑˆ Ğ½ĞµĞ±ĞµĞ·Ğ¿ĞµÑ‡Ğ½Ğ¸Ñ… Ñ‚Ğ²Ğ°Ñ€Ğ¸Ğ½ Ñ– Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚.\",\"ĞšÑ€ÑƒÑ‚i Ğ¼Ğ°Ğ¿Ğ¸ ĞŸĞ¾Ğ»ÑŒÑ‰Ñ–, Ğ¼Ğ¾Ñ— ÑƒĞ»ÑĞ±Ğ»ĞµĞ½Ñ– Ğ¿Ñ€Ğ¾ Ñ‚ÑƒÑ€Ğ¸Ğ·Ğ¼, Ğ½Ğ°Ğ¹Ğ±Ñ–Ğ»ÑŒÑˆ Ğ½ĞµĞ±ĞµĞ·Ğ¿ĞµÑ‡Ğ½Ğ¸Ñ… Ñ‚Ğ²Ğ°Ñ€Ğ¸Ğ½ Ñ– Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚.\",\"<extra_id_0>, Ğ¼Ğ¾Ñ— ÑƒĞ»ÑĞ±Ğ»ĞµĞ½Ñ– Ğ¿Ñ€Ğ¾ Ñ‚ÑƒÑ€Ğ¸Ğ·Ğ¼, Ğ½Ğ°Ğ¹Ğ±Ñ–Ğ»ÑŒÑˆ Ğ½ĞµĞ±ĞµĞ·Ğ¿ĞµÑ‡Ğ½Ğ¸Ñ… Ñ‚Ğ²Ğ°Ñ€Ğ¸Ğ½ Ñ– Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚\"\n",
      "616,hi,à¤‡à¤¸à¤•à¥€ à¤œà¤¾à¤à¤š à¤¹à¥‹à¤¨à¥€ à¤šà¤¾à¤¹à¤¿à¤ | à¤‡à¤¸à¤•à¥‡ à¤ªà¥€à¤›à¥‡ à¤¦à¥‡à¤¶ à¤•à¥‡ à¤Ÿà¥à¤•à¤¡à¤¼à¥‡-à¤Ÿà¥‚à¤•à¤¡à¥‡ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤¹à¤°à¤¾à¤®à¥€ à¤•à¥€ à¤¶à¤•à¥à¤¤à¤¿à¤¯à¥‹à¤‚ à¤¤à¥‹ à¤¨à¤¹à¥€à¤‚  à¤¹à¥ˆà¤‚ |,à¤‡à¤¸à¤•à¥€ à¤œà¤¾à¤à¤š à¤¹à¥‹à¤¨à¥€ à¤šà¤¾à¤¹à¤¿à¤ | à¤‡à¤¸à¤•à¥‡ à¤ªà¥€à¤›à¥‡ à¤¦à¥‡à¤¶ à¤•à¥‹ à¤¬à¤°à¥à¤¬à¤¾à¤¦ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤¬à¥à¤°à¥‡ à¤²à¥‹à¤—à¥‹à¤‚ à¤•à¥€ à¤¶à¤•à¥à¤¤à¤¿à¤¯à¥‹à¤‚ à¤¤à¥‹ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¤‚ |,<extra_id_0>à¤•à¥‡ à¤ªà¥€à¤›à¥‡ à¤¦à¥‡à¤¶ à¤•à¥‡ à¤Ÿà¥à¤•à¤¡à¤¼à¥‡-à¤Ÿà¥‚à¤•à¤¡à¥‡ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤¹à¤°à¤¾à¤®à¥€ à¤•à¥€ à¤¶à¤•à¥à¤¤à¤¿à¤¯à¥‹à¤‚ à¤¤à¥‹ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¤‚ |\n",
      "617,es,Â¿Van a seguir con el mismo tema de los comediantes mediocres?,Â¿Van a seguir con el mismo tema de los comediantes?,<extra_id_0> Â¿Van a seguir con el mismo tema de los comediantes mediocres?\n",
      "618,zh,å‚»é€¼åŒæ ‡ç‹—è€Œå·²,ä¸æ˜æ™ºçš„åŒæ ‡çš„äººè€Œå·²,å‚»é€¼åŒæ ‡ç‹—è€Œå·²\n",
      "619,de,DÃ¼sseldorf ist nicht die einzige dumme Stadt...und mit dem Messerstecher habe ich mehr Mitleid als mit dem MÃ¶chtegernbÃ¼rgermeister...!!,DÃ¼sseldorf ist nicht die einzige Stadt mit diesen Herausforderungen...und mit dem TÃ¤ter habe ich mehr Mitleid als mit dem BÃ¼rgermeister...!!,<extra_id_0> DÃ¼sseldorf ist nicht die einzige dumme und mit dem Messerstecher habe ich mehrleid als mit dem MÃ¶chtegernbÃ¼rgermeister...!!\n",
      "620,ru,\"Ğ¿ÑƒÑ‚Ğ¸Ğ½, ÑÑƒĞºĞ°! Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚!, ĞºĞ°ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹?\",\"Ğ¿ÑƒÑ‚Ğ¸Ğ½! Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚!, ĞºĞ°ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹?\",<extra_id_0> Ğ¿ÑƒÑ‚Ğ¸Ğ½! Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚! ĞºĞ°ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹?\n",
      "621,am,á‹¨áˆ«áˆ· áŠ¥á‹«áˆ¨áˆ¨ á‹¨áˆ°á‹ á‰³áˆ›áˆµáˆ‹áˆˆá‰½ áˆˆáˆ˜áˆ†áŠ‘ á‰ á‹¨á‰µáŠ›á‹ áˆ™áˆ«áˆáˆ… áˆˆ á‰»á‹­áŠ“ happy á‹¨áˆšáˆ áˆ˜áˆá‹•áŠ­á‰µ áˆ›áˆµá‰°áˆ‹áˆˆá á‹¨áˆáˆˆáŠ¨á‹ ?,áŠ¨áŠ¥áŠ› áŠá‰£áˆ«á‹Š áˆáŠ•á‰³ áŒ‹áˆ­ á‹¨áˆ›á‹­áŒ£áŒ£áˆáŠ• áŠáŒˆáˆ­á¤ áˆˆ á‰»á‹­áŠ“ happy á‹¨áˆšáˆ áˆ˜áˆá‹•áŠ­á‰µ áˆˆáˆáŠ• áˆ›áˆµá‰°áˆ‹áˆˆá áŠ¥áŠ•á‹°áˆáˆˆáŠ­ áŠ áˆáŒˆá‰£áŠáˆ,á‰»á‹­áŠ“ happy á‹¨áˆšáˆ áˆ˜áˆá‹•áŠ­á‰µ áˆ›áˆµá‰°áˆ‹áˆˆá á‹¨áˆáˆˆáŠ¨á‹ ?\n",
      "622,uk,\"Ğ‡Ğ±Ğ°Ñ‚Ğ¸ Ğ±Ğ»ÑĞ´Ñ”Ğ¹ Ñ‚Ñ€Ğ° Ğ²Ñ–Ğ´ Ñ‚Ğ¸Ñ… Ğ¿Ğ¾Ñ‡Ğ¸Ğ½Ğ°ÑÑ‡Ğ¸, Ñ‰Ğ¾ Ğ³Ğ°Ğ»ÑŒĞ¼ÑƒÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ğ²Ñ‡Ñ– Ğ¿Ñ€Ğ¾Ñ†ĞµÑĞ¸, ĞºÑ–Ğ½Ñ‡Ğ°ÑÑ‡Ğ¸ ÑÑƒĞ´Ğ°Ğ¼Ğ¸ Ğº Ñ…ÑƒÑĞ¼, ÑÑ…Ğ¾Ğ¶Ğµ Ñƒ Ğ¿Ğ¾Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞºĞ»Ğ°Ğ´Ñ–.\",\"ĞšĞ°Ñ€Ğ°Ñ‚Ğ¸  Ñ‚Ñ€Ğ° Ğ²Ñ–Ğ´ Ñ‚Ğ¸Ñ… Ğ¿Ğ¾Ñ‡Ğ¸Ğ½Ğ°ÑÑ‡Ğ¸, Ñ‰Ğ¾ Ğ³Ğ°Ğ»ÑŒĞ¼ÑƒÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ğ²Ñ‡Ñ– Ğ¿Ñ€Ğ¾Ñ†ĞµÑĞ¸, ĞºÑ–Ğ½Ñ‡Ğ°ÑÑ‡Ğ¸ ÑÑƒĞ´Ğ°Ğ¼Ğ¸, ÑÑ…Ğ¾Ğ¶Ğµ Ñƒ Ğ¿Ğ¾Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞºĞ»Ğ°Ğ´Ñ–.\",\"<extra_id_0> Ñ‚Ğ¸Ñ… Ğ¿Ğ¾Ñ‡Ğ¸Ğ½Ğ°ÑÑ‡Ğ¸, Ñ‰Ğ¾ Ğ³Ğ°Ğ»ÑŒĞ¼ÑƒÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ğ²Ñ‡Ñ– Ğ¿Ñ€Ğ¾Ñ†ĞµÑĞ¸, ĞºÑ–Ğ½Ñ‡Ğ°ÑÑ‡Ğ¸ ÑÑƒĞ´Ğ°Ğ¼Ğ¸ Ğº Ñ…ÑƒÑĞ¼, ÑÑ…Ğ¾Ğ¶Ğµ Ñƒ Ğ¿Ğ¾Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞºĞ»Ğ°Ğ´Ñ–\"\n",
      "623,hi,à¤¤à¥ à¤¬à¥€à¤œà¥‡à¤ªà¥€ à¤•à¥‡ à¤—à¤²à¤¤à¥€ à¤•à¤¾ à¤‡à¤‚à¤¤à¤œà¤¾à¤° à¤à¤¸à¤¾ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ  à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ à¤°à¤‚à¤¡à¥€ à¤•à¥‹ à¤—à¥à¤°à¤¾à¤¹à¤• à¤•à¤¾ à¤¹à¥‹ |,à¤¤à¥‚ à¤¬à¥€à¤œà¥‡à¤ªà¥€ à¤•à¥‡ à¤—à¤²à¤¤à¥€ à¤•à¤¾ à¤‡à¤‚à¤¤à¤œà¤¾à¤° à¤¬à¤¹à¥à¤¤ à¤¬à¥‡à¤¸à¤¬à¥à¤°à¥€ à¤¸à¥‡ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¥¤ ,<extra_id_0> à¤¤à¥ à¤¬à¥€à¤œà¥‡à¤ªà¥€ à¤•à¥‡ à¤—à¤²à¤¤à¥€ à¤•à¤¾ à¤‡à¤‚à¤¤à¤œà¤¾à¤° à¤à¤¸à¤¾ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ à¤°à¤‚à¤¡à¥€ à¤•à¥‹ à¤—à¥à¤°à¤¾à¤¹à¤• à¤•à¤¾ à¤¹à¥‹ |\n",
      "624,uk,Ğ±Ğ»ÑÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾) Ñ– Ğ¿Ğ¾Ğ¼Ñ–Ğ½ÑÑ‚Ğ¸ Ğ¿Ñ€Ñ–Ğ·Ğ²Ğ¸Ñ‰Ğµ Ğ²ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¸ĞºÑƒ,Ğ‘Ğ»Ñ–Ğ½ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾)Ñ– Ğ¿Ğ¾Ğ¼Ñ–Ğ½ÑÑ‚Ğ¸ Ğ¿Ñ€Ñ–Ğ·Ğ²Ğ¸Ñ‰Ğµ Ğ²ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¸ĞºÑƒ,Ğ±Ğ»ÑÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾) Ñ– Ğ¿Ğ¾Ğ¼Ñ–Ğ½ÑÑ‚Ğ¸ Ğ¿Ñ€Ñ–Ğ·Ğ²Ğ¸Ñ‰Ğµ Ğ²ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¸ĞºÑƒ\n",
      "625,en,\"how else could it have become , with bribery being legal and shit ? .\",\"How else could it have become, with bribery being legal?\",\"<extra_id_0> how else could it have become , with bribery being legal and shit?\"\n",
      "626,uk,Ğ° Ğ¼ĞµĞ½Ñ– Ğ½Ñ€Ğ°Ğ²ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ³Ñ€Ñ–Ñ… Ğ´Ñ€Ğ¸Ñ‰Ñ–Ğ² Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ¹Ğ¾Ğ±ÑƒĞ²Ğ°Ñ‚ÑŒ,Ğ° Ğ¼ĞµĞ½Ñ– Ğ½Ñ€Ğ°Ğ²ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ³Ñ€Ñ–Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸Ñ… Ğ½Ğµ Ğ¿Ñ–Ğ´ĞºĞ¾Ğ»ÑĞ²Ğ°Ñ‚Ğ¸,Ğ° Ğ¼ĞµĞ½Ñ– Ğ½Ñ€Ğ°Ğ²ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ³Ñ€Ñ–Ñ… Ğ´Ñ€Ğ¸Ñ‰Ñ–Ğ² Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ¹Ğ¾Ğ±ÑƒĞ²Ğ°Ñ‚ÑŒ\n",
      "627,ar,Ù…Ø³ØªØ­ÙŠÙ„ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙƒØ°Ø§ Ø§Ù†Øª Ù„Ø§ØªÙ…Ø«Ù„ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙŠØ§ Ø­Ù…Ø§Ø±,Ù…Ø³ØªØ­ÙŠÙ„ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙƒØ°Ø§ Ø§Ù†Øª Ù„Ø§ØªÙ…Ø«Ù„ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ,Ù„Ø§ØªÙ…Ø«Ù„ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙŠØ§ Ø­Ù…Ø§Ø±\n",
      "628,de,\"Huuuuuuuuuu, geht doch, Murksel entglitten in Washington sÃ¤mtliche GesichtszÃ¼ge. Sogar ihre \"\"MÃ¤ulchenfalten\"\" verschwanden ğŸ˜‚ğŸ˜‚ğŸ˜‚\",\"Huuuuuuuuuu, geht doch, Merkel entglitten in Washington sÃ¤mtliche GesichtszÃ¼ge. Sogar ihre \"\"Mundfalten\"\" verschwanden ğŸ˜‚ğŸ˜‚ğŸ˜‚\",\"<extra_id_0> geht doch, Murksel entglitten in Washington sÃ¤mtliche GesichtszÃ¼ge. Sogar ihre \"\"MÃ¤ulchenfalten\"\" verschwanden ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\"\n",
      "629,zh,ä¸‡ä¸€ä»–å°±æ˜¯é»‘é»‘äº†,ä¸‡ä¸€ä»–å°±æ˜¯é»‘äººäº†,ä¸‡ä¸€ä»–å°±æ˜¯é»‘é»‘äº†\n",
      "630,ar,ØªØ£ÙƒØ¯ÙˆØ§ Ù‡Ù„ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… ÙˆØ§Ù„Ø§ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø£Ù…Ø±ÙŠÙƒÙŠØ©.. Ù„Ø£Ù†Ù‡Ù… ÙŠØ¬ÙˆÙ† Ø§Ù‚Ø²Ø§Ù… Ø²ÙŠ Ù‡Ø°Ø§,ØªØ£ÙƒØ¯ÙˆØ§ Ù‡Ù„ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… ÙˆØ§Ù„Ø§ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø£Ù…Ø±ÙŠÙƒÙŠØ©.. Ù„Ø£Ù†Ù‡Ù… ÙŠØ¬ÙˆÙ† Ù‚ØµØ§Ø± Ø²ÙŠ Ù‡Ø°Ø§,<extra_id_0> Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø£Ù…Ø±ÙŠÙƒÙŠØ©.. Ù„Ø£Ù†Ù‡Ù… ÙŠØ¬ÙˆÙ† Ø§Ù‚Ø²Ø§Ù… Ø²ÙŠ Ù‡Ø°Ø§\n",
      "631,ar,Ø­Ø¯ ÙŠÙ‚Ø¹ ÙÙŠ Ù†ÙØ³ Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ø§Ù„Ø¨ÙŠØª*,Ø­Ø¯ ÙŠÙ‚Ø¹ ÙÙŠ Ù†ÙØ³ Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ù…Ø§ ØªÙÙƒØ± ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ø§Ù„Ø¨ÙŠØª*,Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø¹Ù† Ø§Ù„ØºÙ„Ø· Ù…Ø±ØªÙŠÙ†ØŸ Ø§Ù†Øª Ø§Ù‡Ø¨Ù„ ÙŠØ§Ø¨Ù†ÙŠØŸ *ÙˆØ§Ù†Ø§\n",
      "632,de,Uiiii...die HÃ¤ÃŸlichkeit geht in die nÃ¤chste Runde...,Uiiii... die nÃ¤chste Runde...,<extra_id_0>...die HÃ¤ÃŸlichkeit geht in die nÃ¤chste Runde\n",
      "633,am,áŠ á‹ áˆáŠ• áŠ á‰£áˆ½ áŠ á‰£á‰µáˆ… áŠ áŒˆá‰£áˆ… áŠ¥áŠ•á‹°áŠ“áŠ•á‰° á‹¨áŠ áˆˆáˆ áˆ…á‹á‰¥ á‹µáŠ•áŒ‹á‹­ áˆ«áˆµ áˆ˜áˆ°áˆ‹á‰½áˆ áˆˆáŠ¥á‹áŠá‰µ á‹­á‰†áˆ›áˆ á‹µáŠ•áŒ‹á‹­ áˆ«áˆµ áŠ®á‰°á‰³áˆ áˆáˆ‹,áŠ á‹­áˆ˜áˆˆáŠ¨á‰³á‰½áˆáˆ áŠ¥áŠ•á‹°áŠ“áŠ•á‰° á‹¨áŠ áˆˆáˆ áˆ…á‹á‰¥ áˆáŠ”á‰³á‹áŠ• á‹¨áˆ›á‹­áˆ¨á‹³ áˆ˜áˆ°áˆ‹á‰½áˆ áˆˆáŠ¥á‹áŠá‰µ á‹­á‰†áˆ›áˆ á‹¨áˆ›á‰³áˆµá‰¡,áˆáŠ• áŠ á‰£áˆ½ áŠ á‰£á‰µáˆ… áŠ áŒˆá‰£áˆ… áŠ¥áŠ•á‹°áŠ“áŠ•á‰° á‹¨áŠ áˆˆáˆ áˆ…á‹á‰¥ á‹µáŠ•áŒ‹á‹­ áˆ«áˆµ áˆ˜áˆ°áˆ‹á‰½áˆ áˆˆáŠ¥á‹áŠá‰µ á‹­á‰†áˆ›áˆ\n",
      "634,zh,ç›´æ¥ä¸¾æŠ¥è¿™ä¸ªé»‘æ¯”ï¼Œ,ç›´æ¥ä¸¾æŠ¥è¿™ä¸ªé»‘äººï¼Œ,\"ç›´æ¥ä¸¾æŠ¥è¿™ä¸ªé»‘æ¯”,\"\n",
      "635,en,i could understand how many people would shit their pants seeing a title like this,I could understand how many people would get scared seeing a title like this.,i could understand how many people would shit their pants seeing a title like this\n",
      "636,ar,Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ ØªÙ†Ø²Ù„ ÙŠØ§Ø­Ù…Ø§Ø± ÙˆØªØ­ØªØ¬ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ.,Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ ØªÙ†Ø²Ù„ ÙˆØªØ­ØªØ¬ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ,<extra_id_0>Ø§Ø°Ø§ Ù„Ø§ ØªÙ†Ø²Ù„ ÙŠØ§Ø­Ù…Ø§Ø± ÙˆØªØ­ØªØ¬ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ\n",
      "637,es,\"Culpa de las mafias y lo progres follarefugees que hacen efecto llamada, nos quedamos sin 57 pensiones\",\"Culpa de las mafias y lo progres que hacen efecto llamada, nos quedamos sin 57 pensiones\",<extra_id_0> que hacen efecto llamada nos quedamos sin 57 pensiones\n",
      "638,es,\",, pin pon , todos los rojos delincuentes en sus ejercicio  ..\",\"pin pon , todos los rojos en sus ejercicio\",\"<extra_id_0>, pin pon , todos los rojos delincuentes en sus ejercicio ..\"\n",
      "639,zh,é»‘ä¸œåŒ—çš„ç™¾åˆ†ä¹‹ä¹åæ˜¯â€œå¥½å®¢å±±æ´â€ è§ä¸å¾—ä¸­å›½å¥½çš„æ˜¯ç¾å›½ä½¬å°æ—¥æœ¬ è§ä¸å¾—ä¸œåŒ—å¥½çš„æ˜¯å¥½å®¢å±±æ´,æˆ‘ä¸ªäººè®¤ä¸ºé»‘ä¸œåŒ—çš„ç™¾åˆ†ä¹‹ä¹åæ˜¯â€œå¥½å®¢å±±ä¸œâ€ è§ä¸å¾—ä¸­å›½å¥½çš„æ˜¯ç¾å›½äººæ—¥æœ¬äºº è§ä¸å¾—ä¸œåŒ—å¥½çš„æ˜¯å±±ä¸œäºº,é»‘ä¸œåŒ—çš„ç™¾åˆ†ä¹‹ä¹åæ˜¯â€œå¥½å®¢å±±æ´â€ è§ä¸å¾—ä¸­å›½å¥½çš„æ˜¯ç¾å›½ä½¬å°æ—¥æœ¬ è§ä¸å¾—ä¸œåŒ—å¥½çš„æ˜¯ç¾å›½ä½¬å°æ—¥æœ¬ è§ä¸å¾—ä¸œåŒ—å¥½çš„æ˜¯å¥½å®¢å±±æ´\n",
      "640,zh,å…¨å›½é»‘ä¸Šæµ·ï¼Œä¸Šæµ·é»‘ç™½å®Œï¼Œæ‰€ä»¥è¢«é»‘çš„æœ€æƒ¨çš„è¿˜æ˜¯ç™½å®Œã€‚,å…¨å›½é»‘ä¸Šæµ·ï¼Œä¸Šæµ·é»‘å®‰å¾½ï¼Œæ‰€ä»¥è¢«é»‘çš„æœ€æƒ¨çš„è¿˜æ˜¯å®‰å¾½ã€‚,\"æœ€è¿‘é»‘ä¸Šæµ·,ä¸Šæµ·é»‘ç™½å®Œ,æ‰€ä»¥è¢«é»‘çš„æœ€æƒ¨çš„è¿˜æ˜¯ç™½å®Œã€‚\"\n",
      "641,zh,åŒæ„æ¥¼ä¸»çš„è¯´æ³•è¦åƒå½“å¹´äº”å››é’å¹´ç»„ç»‡èµ·æ¥å‘å¤§ä¼—å‘ä¼ å•åŒæ—¶è®©è¶Šæ¥è¶Šå¤šäººä»¬å¦‚ä»Šå¼“æ¾¢çœŸè´ŸçœŸå®å˜´è„¸ä»–ä»¬ä¸è¿‡çŠ¹å¤ªäººçˆªç‰™å’Œè™«ç±»çš„äº²å„¿å­,åŒæ„æ¥¼ä¸»çš„è¯´æ³•è¦åƒå½“å¹´äº”å››é’å¹´ç»„ç»‡èµ·æ¥å‘å¤§ä¼—å‘ä¼ å•åŒæ—¶è®©è¶Šæ¥è¶Šå¤šäººä»¬å¦‚ä»Šå¼“æ¾¢çœŸè´ŸçœŸå®å˜´è„¸ä»–ä»¬ä¸è¿‡çŠ¹å¤ªäººçš„å¿ å®å¸®æ‰‹å’Œå´‡æ‹œå¤–å›½äººçš„äºº,åŒæ„æ¥¼ä¸»çš„è¯´æ³•è¦åƒå½“å¹´äº”å››é’å¹´ç»„ç»‡èµ·æ¥å‘å¤§ä¼—å‘ä¼ å•åŒæ—¶è®©è¶Šæ¥è¶Šå¤šäººä»¬å¦‚ä»Šå¼“æ¾¢çœŸè´ŸçœŸå®å˜´è„¸ä»–ä»¬ä¸è¿‡çŠ¹å¤ªäººçˆªç‰™å’Œè™«ç±»çš„äº²å„¿å­\n",
      "642,am,áˆáŠ• á‰€áˆ¨á‰¥áŠ á‰¥áˆˆáˆ… áŠá‹???á‹¨á‹áˆ» á‹¨á‹µáˆ˜á‰µ áŠ¥áŠ“ á‹¨áŠ¥á‰£á‰¥ áŒ¥á‰¥áˆµ áŠá‹ áŠ¥áŠ® á‹­á‰€áˆ¨á‰¥áˆ… á‰¥á‰» áŠ¨áŠ áˆ›áˆ¨áˆ… áˆ˜áŠ¨áˆáŠ¨áˆ á‹¨áˆˆá‰ á‰µáˆ á‰£á‹­ áŠáŠ á‹¨áˆšáˆ˜áˆˆáŠ¨á‰°á‹ áˆáˆ‰ áŠ­á‰µá‰µáˆ áˆ›á‹µáˆ¨áŒ á‹­áŠ–áˆ­á‰ á‰³áˆ,áŠ¨áˆ›á‹­á‰ áˆ‰ áŠ¥áŠ•áˆ°áˆ³á‰µ á‰°áˆ°áˆ­á‰¶ á‹¨áˆšá‰€áˆ­á‰¥ áŒ¥á‰¥áˆµ á‰¢á‰€áˆ­á‰¥áˆ…áˆµ ?,áˆáŠ• á‰€áˆ¨á‰¥áŠ á‰¥áˆˆáˆ… áŠá‹???á‹¨á‹áˆ» á‹¨á‹µáˆ˜á‰µ áŠ¥áŠ“ á‹¨áŠ¥á‰£á‰¥ áŒ¥á‰¥áˆµ áŠá‹ áŠ¥áŠ® á‹­á‰€áˆ¨á‰¥áˆ… á‰¥á‰» áŠ¨áŠ áˆ›áˆ¨áˆ… áˆ˜áŠ¨áˆáŠ¨áˆ á‹¨áˆˆá‰ á‰µáˆ á‰£á‹­ áŠáŠ á‹¨áˆšáˆ˜áˆˆáŠ¨á‰°á‹ áˆáˆ‰ áŠ­á‰µá‰µáˆ áˆ›á‹µáˆ¨áŒ á‹­áŠ–áˆ­á‰ á‰³áˆ\n",
      "643,ar,Ù‚Ø±Ø¯ÙˆØºØ§Ù† ğŸ’ Ù„Ù… ÙŠØªØ¯Ø®Ù„ ÙÙŠ Ù„ÙŠØ¨ÙŠØ§ Ø§Ùˆ Ø³ÙˆØ±ÙŠØ§ ÙˆÙ‚Ø·Ø± Ù„Ø­Ù…Ø§ÙŠØªÙ‡Ù… Ø§Ùˆ Ø§Ù„Ø®ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø±Ø¯Ù‡Ù… ÙÙ‡Ø°Ø§ Ø­Ø§Ù„ ÙƒÙ„ Ø¨Ù„Ø¯ ÙŠØªØ¯Ø®Ù„ ÙÙŠÙ‡ Ù‡Ø°â€¦,Ø£Ø±Ø¯ÙˆØºØ§Ù† Ù„Ù… ÙŠØªØ¯Ø®Ù„ ÙÙŠ Ù„ÙŠØ¨ÙŠØ§ Ø§Ùˆ Ø³ÙˆØ±ÙŠØ§ ÙˆÙ‚Ø·Ø± Ù„Ø­Ù…Ø§ÙŠØªÙ‡Ù… Ø§Ùˆ Ø§Ù„Ø®ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø±Ø¯Ù‡Ù… ÙÙ‡Ø°Ø§ Ø­Ø§Ù„ ÙƒÙ„ Ø¨Ù„Ø¯ ÙŠØªØ¯Ø®Ù„ ÙÙŠÙ‡ Ù‡Ø°â€¦,<extra_id_0> Ù„Ù… ÙŠØªØ¯Ø®Ù„ ÙÙŠ Ù„ÙŠØ¨ÙŠØ§ Ø§Ùˆ Ø³ÙˆØ±ÙŠØ§ ÙˆÙ‚Ø·Ø± Ù„Ø­Ù…Ø§ÙŠØªÙ‡Ù… Ø§Ùˆ Ø§Ù„Ø®ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø±Ø¯Ù‡Ù… ÙÙ‡Ø°Ø§ Ø­Ø§Ù„ ÙƒÙ„ Ø¨Ù„Ø¯ ÙŠØªØ¯Ø®Ù„ ÙÙŠÙ‡ Ù‡Ø°...\n",
      "644,de,#Diesel-#Fahrverbote: Wie vorhergesagt nimmt ihnen das #Globalisten-#Dreckspack jetzt leistbare #MobilitÃ¤t &amp; zerstÃ¶rt dt. SchlÃ¼sselindustrie,#Diesel-#Fahrverbote: Wie vorhergesagt nimmt ihnen das #Globalisten-Gruppe jetzt leistbare #MobilitÃ¤t & zerstÃ¶rt dt. SchlÃ¼sselindustrie,nimmt ihnen das #Globalisten-#Dreckspack jetzt leistbare #MobilitÃ¤t &amp; zerstÃ¶rt dt. SchlÃ¼sselindustrie\n",
      "645,de,\"Wenn die HÃ¤lfte der Moslime den Handschlag verweigert, hilft nur. ein Tritt in den Arsch, der sie zurÃ¼ckschickt nach Afrika.\",\"Wenn die HÃ¤lfte der Moslime den Handschlag verweigert, hilft nur. jemand, der sie zurÃ¼ckschickt nach Afrika.\",\"<extra_id_0> die HÃ¤lfte der Moslime den Handschlag verweigert, hilft nur. ein Tritt in den Arsch, der sie zurÃ¼ckschickt nach Afrika.\"\n",
      "646,en,likes to eat shit It's true and he told me himself!,\"likes to eat bad food, that's true and he told me himself!\",''s true and he told me himself!\n",
      "647,am,áŠ¢á‰µá‹®áŒµá‹«áŠ• áˆ¶áˆªá‹« á‹¨áˆšá‹«á‹°áˆ­áŒ‹á‰µ á‹¨áŠ á‰¥á‹­ áŠ á‹µáˆáŠ› áŠ¥áŠ“ áˆ´áˆ¨áŠ› áŠ áŒˆá‹›á‹ áŠ¥áŠ•áŒ… áŠ áˆœáˆªáŠ« á‹«áˆ‹á‰µ áŠ á‰‹áˆ áŠ á‹­á‹°áˆˆáˆá¢,áŠ¢á‰µá‹®áŒµá‹«áŠ• áˆ¶áˆªá‹« á‹¨áˆšá‹«á‹°áˆ­áŒ‹á‰µ á‹¨áˆ˜áŠ•áŒáˆµá‰µ áŠ áŒˆá‹›á‹ áŠ¥áŠ•áŒ… áŠ áˆœáˆªáŠ« á‹«áˆ‹á‰µ áŠ á‰‹áˆ áŠ á‹­á‹°áˆˆáˆá¢,áŠ¢á‰µá‹®áŒµá‹«áŠ• áˆ¶áˆªá‹« á‹¨áˆšá‹«á‹°áˆ­áŒ‹á‰µ á‹¨áŠ á‰¥á‹­ áŠ á‹µáˆáŠ› áŠ¥áŠ“ áˆ´áˆ¨áŠ› áŠ áŒˆá‹›á‹ áŠ¥áŠ•áŒ… áŠ áˆœáˆªáŠ« á‹«áˆ‹á‰µ áŠ á‰‹áˆ áŠ á‹­á‹°áˆˆáˆá¢\n",
      "648,ru,\"Ğ½Ğµ Ñ…ÑƒĞ¹ Ñ€ÑƒĞºÑƒ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ, Ğ° ÑĞ°Ğ¼ Ğ¾Ğ½ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ğ¾Ğµ.\",\"ĞĞµ Ğ½Ğ°Ğ´Ğ¾ Ñ€ÑƒĞºÑƒ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ, Ğ° ÑĞ°Ğ¼ Ğ¾Ğ½ Ğ½ĞµÑ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº\",\"<extra_id_0> Ğ½Ğµ Ñ…ÑƒĞ¹ Ñ€ÑƒĞºÑƒ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ, Ğ° ÑĞ°Ğ¼ Ğ¾Ğ½ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ğ¾Ğµ\"\n",
      "649,es,Este es el maricÃ³n de Aida?,Este es el de Aida?,<extra_id_0> Este Este sitio de aida\n",
      "650,uk,Ğ¯ĞºĞ° Ğ±ÑƒĞ»Ğ° ĞºÑƒÑ€Ğ²Ğ° Ğ¼Ğ°Ñ‚Ğ¸Ğ¢Ğ°ĞºĞ° Ğ¶ ĞºÑƒÑ€Ğ²Ğ° Ğ´Ğ¾Ñ‡ĞºĞ°!,Ğ¯ĞºĞ° Ğ±ÑƒĞ»Ğ° Ğ¼Ğ°Ñ‚Ğ¸ Ñ‚Ğ°ĞºĞ° Ğ¶ Ñ– Ğ´Ğ¾Ñ‡ĞºĞ°,<extra_id_0> ÑĞºĞ° Ğ±ÑƒĞ»Ğ° ĞºÑƒÑ€Ğ²Ğ° Ğ¼Ğ°Ñ‚Ğ¸Ğ¢Ğ°ĞºĞ° Ğ¶ ĞºÑƒÑ€Ğ²Ğ° Ğ´Ğ¾Ñ‡ĞºĞ°!\n",
      "651,ar,Ø§Ù†Ù‡Ø§ ÙˆØ¹Ø§Ø¡ Ù…Ù† Ø§Ù„Ø±ÙˆØ«ØŒ ÙˆØ£Ù†Øª ØªØ¹Ø±Ù Ø°Ù„Ùƒ .,Ù‡Ø°Ø§ Ù„Ø§ Ù…Ø¹Ù†Ù‰ Ù„Ù‡ØŒ ÙƒÙ…Ø§ ØªØ¹Ù„Ù…ÙˆÙ†,Ø§Ù†Ù‡Ø§ ÙˆØ¹Ø§Ø¡ Ù…Ù† Ø§Ù„Ø±ÙˆØ«ØŒ ÙˆØ£Ù†Øª ØªØ¹Ø±Ù Ø°Ù„Ùƒ\n",
      "652,es,cuidado con estos conspiradores de mierda,Cuidado con estas personas,<extra_id_0> cuidado con estos conspiradores de mierda\n",
      "653,ar,Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡ØªÙ… Ø§Ù„ÙƒÙ†Ø¯ÙŠÙˆÙ† Ø¨Ø§Ù„Ù‡ÙˆÙƒÙŠ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† ÙØ±Ù‚Ù‡Ù… ÙƒÙ„Ù‡Ø§ Ø®Ø§Ø³Ø±Ø©ØŸ,Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡ØªÙ… Ø§Ù„ÙƒÙ†Ø¯ÙŠÙˆÙ† Ø¨Ø§Ù„Ù‡ÙˆÙƒÙŠ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† ÙØ±Ù‚Ù‡Ù… Ùƒ Ù„ÙŠØ³Øª Ø§Ù„Ø£ÙØ¶Ù„ØŸ,<extra_id_0>Ø§Ø° ÙŠÙ‡ØªÙ… Ø§Ù„ÙƒÙ†Ø¯ÙŠÙˆÙ† Ø¨Ø§Ù„Ù‡ÙˆÙƒÙŠ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† ÙØ±Ù‚Ù‡Ù… ÙƒÙ„Ù‡Ø§ Ø®Ø§Ø³Ø±Ø©ØŸ\n",
      "654,ru,\"ÑƒĞµĞ±Ğ¸Ñ‰Ğµ, Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾,Ğ²Ñ‹,Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ½Ğ°ÑˆĞ¸ Ñ‡Ñ‚Ğ¾-Ğ»Ğ¸?\",\"Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ²Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ Ñ‡Ñ‚Ğ¾-Ğ»Ğ¸?\",\"<extra_id_0>ÑˆĞ¸Ñ‚Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾,Ğ²Ñ‹,Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ½Ğ°ÑˆĞ¸ Ñ‡Ñ‚Ğ¾-Ğ»Ğ¸?\"\n",
      "655,en,You dumb motherfucker I am going to sue the shit out of you,I am going to sue everything out of you,<extra_id_0> dumb motherfucker I am going to sue the shit out of you\n",
      "656,en,\"we shouldn t be shitting on his good name , and saying what he should , and shouldn t have done .\",we should not desecrate his name instead we should have done better.,\"<extra_id_0> we shouldn t be shitting on his good name , and saying what he should, and shouldn t have done .\"\n",
      "657,en,\"that said , i hope they get a great attorney and sue the shit out of the cops .\",\"that said , i hope they get a great attorney and sue the cops .\",<extra_id_0> i hope they get a great lawyer and sue the shit out of the cops .\n",
      "658,ar,ÙŠØ§Ø¹Ù… ÙƒØ³ Ø£Ù…Ùƒ Ù‡Ùˆ Ø¥Ù†Øª Ù…Ø§ ØªØ¹Ø±ÙØ´ Ø§Ù†Ù†Ø§ Ø¨Ø¹ØªÙ†Ø§ Ø²ÙŠØ§Ø±Ø§Øª Ùˆ Ø¹Ø§Ù…Ù„ÙŠÙ† Ø¬Ø³Ø± Ø¬ÙˆÙ‰ Ùˆ ÙÙ‰ Ù…Ø³ØªØ´ÙÙ‰ Ù…ÙŠØ¯Ø§Ù†Ù‰ Ø§ØªØ¹Ù…Ù„ ÙŠÙ„Ø¹Ù† Ù…ÙŠØªÙŠÙ† Ø£Ù…Ùƒ Ø¥Ø¨Ù† Ø´Ø±Ù…ÙˆØ·Ø©,ÙŠØ§Ø¹Ù… Ù‡Ùˆ Ø¥Ù†Øª Ù…Ø§ ØªØ¹Ø±ÙØ´ Ø§Ù†Ù†Ø§ Ø¨Ø¹ØªÙ†Ø§ Ø²ÙŠØ§Ø±Ø§Øª Ùˆ Ø¹Ø§Ù…Ù„ÙŠÙ† Ø¬Ø³Ø± Ø¬ÙˆÙ‰ Ùˆ ÙÙ‰ Ù…Ø³ØªØ´ÙÙ‰ Ù…ÙŠØ¯Ø§Ù†Ù‰ Ø§ØªØ¹Ù…Ù„,ÙŠØ§Ø¹Ù… ÙƒØ³ Ø£Ù…Ùƒ Ù‡Ùˆ Ø¥Ù†Øª Ù…Ø§ ØªØ¹Ø±ÙØ´ Ø§Ù†Ù†Ø§ Ø¨Ø¹ØªÙ†Ø§ Ø²ÙŠØ§Ø±Ø§Øª Ùˆ Ø¹Ø§Ù…Ù„ÙŠÙ† Ø¬Ø³Ø± Ø¬ÙˆÙ‰ Ùˆ ÙÙ‰ Ù…Ø³ØªØ´ÙÙ‰ Ù…ÙŠØ¯Ø§Ù†Ù‰ Ø§ØªØ¹Ù…Ù„ ÙŠÙ„Ø¹Ù† Ù…ÙŠØªÙŠÙ† Ø£Ù…Ùƒ Ø¥Ø¨Ù† Ø´Ø±Ù…ÙˆØ·Ø©\n",
      "659,zh,ä¸Šå»æ‰“æ­»è¿™äº›é»˜,ä¸Šå»è®©è¿™äº›æˆ‘ä¸å¤ªå–œæ¬¢çš„é»‘äººä»˜å‡ºä»£ä»·,ä¸Šå»æ‰“æ­»è¿™äº›é»˜\n",
      "660,ru,Ğ¼Ñ€Ğ°Ğ·Ğ¸ Ğ±ĞµĞ´Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ±Ñ‘Ğ½Ğ¾Ğº Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞºĞ°ĞºĞ°Ñ Ñƒ Ğ½ĞµĞ³Ğ¾ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚ÑŒ Ğº Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑĞ¼.,Ğ±ĞµĞ´Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ±Ñ‘Ğ½Ğ¾Ğº Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞºĞ°ĞºĞ°Ñ Ñƒ Ğ½ĞµĞ³Ğ¾ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚ÑŒ Ğº Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑĞ¼.,Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞºĞ°ĞºĞ°Ñ Ñƒ Ğ½ĞµĞ³Ğ¾ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚ÑŒ Ğº Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑĞ¼.\n",
      "661,uk,Ğ·Ğ²Ñ–Ğ´ĞºĞ¸ Ğ²Ğ¾Ğ½Ğ¸ Ğ²Ğ¾Ğ±ÑˆĞµ Ğ±Ğ»ÑÑ‚ÑŒ Ğ²Ğ¸Ğ»Ñ–Ğ·Ğ»Ğ¸?,Ğ·Ğ²Ñ–Ğ´ĞºĞ¸ Ğ²Ğ¾Ğ½Ğ¸ Ğ²Ğ¾Ğ±ÑˆĞµ Ğ²Ğ¸Ğ»Ñ–Ğ·Ğ»Ğ¸?,Ğ²Ğ¾Ğ½Ğ¸ Ğ²Ğ¾Ğ±ÑˆĞµ Ğ±Ğ»ÑÑ‚ÑŒ Ğ²Ğ¸Ğ»Ñ–Ğ·Ğ»Ğ¸?\n",
      "662,am,áŠ¥áŠ•á‹°á‹áˆ á“áˆˆá‰²áŠ« á‹áˆµáŒ¥ áˆáŠ• á‰µáˆµáˆ«áˆ‹á‰½áˆáˆ á‰°á‰¥áˆˆáŠ“áˆ áŒ áŠ•áŠ­áˆ¨áŠ• áˆ˜áˆ›áˆ­áŠ“ áˆ˜áˆµáˆ«á‰µ á‹­áŒ á‰ á‰…á‰¥áŠ“áˆ áˆˆá‰…áˆ¶áŠ• áˆˆáˆˆá‰…áˆµáŠ–á‰½ áˆ˜á‰°á‹ áŠ áˆˆá‰¥áŠ•  áˆƒáˆƒáˆƒ,á“áˆˆá‰²áŠ« á‹áˆµáŒ¥ áˆáŠ•áˆ áŠ á‰µáˆ°áˆ©áˆ á‰°á‰¥áˆˆáŠ“áˆá¡á¡áŒ áŠ•áŠ­áˆ¨áŠ• áˆ˜áˆ›áˆ­áŠ“ áˆ˜áˆµáˆ«á‰µ á‹­áŒ á‰ á‰…á‰¥áŠ“áˆ áˆŒáˆ‹á‹áŠ• áˆˆáˆšáˆ˜áˆˆáŠ¨á‰°á‹ á‰ áˆ˜á‰°á‹,á“áˆˆá‰²áŠ« á‹áˆµáŒ¥ áˆáŠ• á‰µáˆµáˆ«áˆ‹á‰½áˆáˆ á‰°á‰¥áˆˆáŠ“áˆ áŒ áŠ•áŠ­áˆ¨áŠ• áˆ˜áˆµáˆ«á‰µ á‹­áŒ á‰ á‰…á‰¥áŠ“áˆ áˆˆá‰…áˆ¶áŠ• áˆˆáˆˆá‰…áˆµáŠ–á‰½ áˆ˜á‰°á‹ áŠ áˆˆá‰¥áŠ• áˆƒáˆƒáˆƒ\n",
      "663,ar,Ù‡ÙŠ ÙƒØ§Ù†Øª Ø³Ù„ÙŠØ·Ø© Ø¬Ø¯Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØµØ©!,ÙƒØ§Ù†Øª ØºÙŠØ± Ù„Ø§Ø¦Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ù†ØµØ© Ø§Ù„Ø´Ù‡ÙˆØ¯.,<extra_id_0> Ù‡ÙŠ ÙƒØ§Ù†Øª Ø³Ù„ÙŠØ·Ø© Ø¬Ø¯Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØµØ©!\n",
      "664,uk,\"ĞœĞ°Ñ” Ğ´Ğ²Ñ– Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸, Ğ°Ğ»Ğµ Ğ²ĞºÑ€Ğ°Ğ¹ Ğ½Ğ¸Ñ”, ÑĞºĞ° Ñƒ Ğ½Ğ°Ñ Ñ…Ñ€ĞµĞ½Ğ¾Ğ²Ğ° Ğ´ĞµÑ€Ğ¶Ğ°Ğ²Ğ°, ÑĞºĞ¸Ğ¹ Ğ²Ñ–Ğ½ Ğ±Ñ–Ğ´Ğ½Ğ¸Ğ¹, ÑĞºĞµ Ñ…ÑƒĞ¹Ğ¾Ğ²Ğµ Ğ¶Ğ¸Ñ‚Ñ‚Ñ.\",\"ĞœĞ°Ñ” Ğ´Ğ²Ñ– Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸, Ğ°Ğ»Ğµ Ğ²ĞºÑ€Ğ°Ğ¹ Ğ½Ğ¸Ñ”, ÑĞºĞ° Ñƒ Ğ½Ğ°Ñ Ñ…Ñ€ĞµĞ½Ğ¾Ğ²Ğ° Ğ´ĞµÑ€Ğ¶Ğ°Ğ²Ğ°, ÑĞºĞ¸Ğ¹ Ğ²Ñ–Ğ½ Ğ±Ñ–Ğ´Ğ½Ğ¸Ğ¹, ÑĞºĞµ Ğ¿Ñ€Ğ¸ĞºÑ€Ğµ Ñƒ Ğ½ÑŒĞ³Ğ¾ Ğ¶Ğ¸Ñ‚Ñ‚Ñ.\",\"<extra_id_0> Ğ´Ğ²Ñ– Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸, Ğ°Ğ»Ğµ Ğ²ĞºÑ€Ğ°Ğ¹ Ğ½Ğ¸Ñ”, ÑĞºĞ° Ñƒ Ğ½Ğ°Ñ Ñ…Ñ€ĞµĞ½Ğ¾Ğ²Ğ° Ğ´ĞµÑ€Ğ¶Ğ°Ğ²Ğ°, ÑĞºĞ¸Ğ¹ Ğ²Ñ–Ğ½ Ğ±Ñ–Ğ´Ğ½Ğ¸Ğ¹, ÑĞºĞµ Ğ¶Ğ¸Ñ‚Ñ‚Ñ\"\n",
      "665,uk,\"ĞÑ… Ñ†Ñ– Ğ±Ğ°Ğ±Ğ¸, ĞºĞ¾Ñ‚Ñ€Ñ– Ğ²ÑĞµÑ€Ğ¹Ğ¾Ğ· Ğ´ÑƒĞ¼Ğ°ÑÑ‚ÑŒ, Ñ‰Ğ¾ ĞºĞ¾Ğ¼ÑƒÑÑŒ Ğ½Ğµ Ğ¿Ğ¾Ñ…ÑƒĞ¹ Ğ½Ğ° Ğ²ÑĞµ, Ñ‰Ğ¾ Ğ²Ñ–Ğ´Ğ±ÑƒĞ²Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ² Ñ—Ñ…Ğ½ÑŒĞ¾Ğ¼Ñƒ Ğ¶Ğ¸Ñ‚Ñ‚Ñ–.\",\"ĞÑ… Ñ†Ñ– Ğ¶Ñ–Ğ½ĞºĞ¸, ĞºĞ¾Ñ‚Ñ€Ñ– Ğ²ÑĞµÑ€Ğ¹Ğ¾Ğ· Ğ´ÑƒĞ¼Ğ°ÑÑ‚ÑŒ, Ñ‰Ğ¾ ĞºĞ¾Ğ¼ÑƒÑÑŒ Ğ½Ğµ Ğ±Ğ°Ğ¹Ğ´ÑƒĞ¶Ğµ Ğ½Ğ° Ğ²ÑĞµ, Ñ‰Ğ¾ Ğ²Ñ–Ğ´Ğ±ÑƒĞ²Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ² Ñ—Ñ…Ğ½ÑŒĞ¾Ğ¼Ñƒ Ğ¶Ğ¸Ñ‚Ñ‚Ñ–.\",\"<extra_id_0>, ĞºĞ¾Ñ‚Ñ€Ñ– Ğ²ÑĞµÑ€Ğ¹Ğ¾Ğ· Ğ´ÑƒĞ¼Ğ°ÑÑ‚ÑŒ, Ñ‰Ğ¾ ĞºĞ¾Ğ¼ÑƒÑÑŒ Ğ½Ğµ Ğ¿Ğ¾Ñ…ÑƒĞ¹ Ğ½Ğ° Ğ²ÑĞµ, Ñ‰Ğ¾ Ğ²Ñ–Ğ´Ğ±ÑƒĞ²Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ² Ñ—Ñ…Ğ½ÑŒĞ¾Ğ¼Ñƒ Ğ¶Ğ¸Ñ‚Ñ‚Ñ–.\"\n",
      "666,am,á‹¨áˆ„ á‰°áˆ¨á‰µ á‰°áˆ¨á‰µ áˆˆáˆ›á‹«á‹á‰… áŒ…áˆ áˆ„á‹°áˆ… áŠ•áŒˆáˆ¨á‹ á‹¨áŠ¢á‰µá‹®áŒµá‹« á–áˆˆá‰²áŠ­áˆµ á‰ á‹°áˆá‰¥ áŠ£á‹á‰€á‹‹áˆˆá‹:: á‹«áˆáŠ©áˆ…áŠ• á‰¥á‰» áŠ£á‰µáˆ­áˆ³::,á‹¨áˆ„ á‰°áˆ¨á‰µ á‰°áˆ¨á‰µ áˆˆáˆ›á‹«á‹á‰… áˆ°á‹ áˆ„á‹°áˆ… áŠ•áŒˆáˆ¨á‹ á‹¨áŠ¢á‰µá‹®áŒµá‹« á–áˆˆá‰²áŠ­áˆµ á‰ á‹°áˆá‰¥ áŠ£á‹á‰€á‹‹áˆˆá‹:: á‹«áˆáŠ©áˆ…áŠ• á‰¥á‰» áŠ£á‰µáˆ­áˆ³::,áˆ„á‹°áˆ… áŠ•áŒˆáˆ¨á‹ á‹¨áŠ¢á‰µá‹®áŒµá‹« á–áˆˆá‰²áŠ­áˆµ á‰ á‹°áˆá‰¥ áŠ£á‹á‰€á‹‹áˆˆá‹:: á‹«áˆáŠ©áˆ…áŠ• á‰¥á‰»\n",
      "667,ru,Ñ‡Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ¸Ğ»Ğ¾? Ñ‡Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ? Ğ¶Ğ¾Ğ¿Ğ¾Ğ¹ Ğ¶Ğ¸Ñ€Ğ½Ğ¾Ğ¹ Ñ‡Ñ‚Ğ¾ Ğ»Ğ¸ Ğ¿Ğ¾ĞºÑ€ÑƒÑ‚Ğ¸Ğ»Ğ°? :-d :-d,Ğ§Ñ‚Ğ¾ Ñ…Ğ²Ğ°Ñ‚Ğ¸Ğ»Ğ¾? Ğ§Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ?,<extra_id_0> Ñ…Ğ²Ğ°Ñ‚Ğ¸Ğ»Ğ¾? Ñ‡Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ? Ğ¶Ğ¾Ğ¿Ğ¾Ğ¹ Ğ¶Ğ¸Ñ€Ğ½Ğ¾Ğ¹ Ñ‡Ñ‚Ğ¾ Ğ»Ğ¸ Ğ¿Ğ¾ĞºÑ€ÑƒÑ‚Ğ¸Ğ»Ğ°?\n",
      "668,uk,\"ĞÑ–Ñ‰Ğ¾ Ñ‚Ğ°Ğº Ğ½Ğµ Ğ±Ğ°Ğ´ÑŒĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ·Ñ€Ğ°Ğ½ĞºÑƒ ÑĞº \"\"Ğ±Ğ»ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾ÑĞ¿Ğ°Ğ²\"\".\",\"ĞÑ–Ñ‰Ğ¾ Ñ‚Ğ°Ğº Ğ½Ğµ Ğ±Ğ°Ğ´ÑŒĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ·Ñ€Ğ°Ğ½ĞºÑƒ ÑĞº \"\"Ğ¶Ğ°Ñ…, Ğ¿Ñ€Ğ¾ÑĞ¿Ğ°Ğ²\"\".\",\"Ğ¾Ğ½ĞµĞ· Ñ‚Ğ°Ğº Ğ½Ğµ Ğ±Ğ°Ğ´ÑŒĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ·Ñ€Ğ°Ğ½ĞºÑƒ ÑĞº \"\"Ğ±Ğ»ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾ÑĞ¿Ğ°Ğ²\"\".\"\n",
      "669,am,á‹¨á‹šáˆ… áˆ°á‹á‹¬ á‹¨áŒ…áˆáŠá‰µ á‹°áˆ¨áŒƒ á‰ á‹¨áŒŠá‹œá‹ áŠ¥á‹«á‹°áŒˆ áŠá‹á£ áŠ¥áˆ±áˆ á‰ áˆ­á‰µá‰¶ áŠ¥á‹¨áˆ°áˆ«á‰ á‰µ áŠá‹á£ á‰€áŒ¥áˆ á‰€áŒ¥áˆá¢ áˆšáŠª áˆ›á‹­áŠ® á‰¢áˆ°áˆ«á‹ áŠ á‹­áŒˆáˆ­áˆ˜áŠáˆ áŠá‰ áˆ­ á‰ á‹šáˆ… áŠ¥á‹µáˆœá‹ á‹­áŒƒáŒƒáŒƒáˆ‹áˆ áŠ¥áŠ•á‹´á¢,á‹¨áŠ¥áˆ­áˆ± á‰¥áˆµáˆˆá‰µ áŠ¨áŒŠá‹œ á‹ˆá‹° áŒŠá‹œ áŠ¨áŠ¥á‹µáˆœá‹ áŒ‹áˆ­ áŠ¥áŠ•á‹°áˆšáŒ á‰ á‰€á‹ áŠ á‰¥áˆ® á‹¨áˆšáˆ„á‹µ áŠ¥á‹¨áˆ†áŠáˆáŠ áŠ á‹­á‹°áˆˆáˆá¡á¡,áˆšáŠª áˆ›á‹­áŠ® á‰¢áˆ°áˆ«á‹ áŠ á‹­áŒˆáˆ­áˆ˜áŠáˆ áŠá‰ áˆ­ á‰ á‹šáˆ… áŠ¥á‹µáˆœá‹ á‹­áŒƒáŒƒáŒƒáˆ‹áˆ áŠ¥áŠ•á‹´á¢\n",
      "670,zh,ä¸ä¸“æ‹‰å¤–å›½äººçš®æ¡çš„çª‘å§ï¼Œäººå®¶éƒ½ä¸æ„Ÿæ¥é»‘äººçš„å®¢,ä¸ä¸“æ‹‰å¤–å›½äººçš®æ¡çš„æ€§å·¥ä½œè€…ï¼Œäººå®¶éƒ½ä¸æ„Ÿæ¥é»‘äººçš„å®¢,\"ä¸ä¸“æ‹‰å¤–å›½äººçš®æ¡çš„çª‘å§,äººå®¶éƒ½ä¸æ„Ÿæ¥é»‘äººçš„å®¢\"\n",
      "671,en,Shut your mouth and stop talking. good boy,Can you please stop talking? Thanks.,<extra_id_0> Shut your mouth and stop talking. good boy\n",
      "672,am,á‹¶/áˆ­ á‹á‰¢á‹­áŠ• á‰ áˆ˜áŒá‹°áˆ áŒ /áˆ­áŠá‰µáŠ• áˆˆáˆ˜á‰†áŠ“áŒ áŒ¥ á‹¨áˆšá‹«áˆáˆ á‹¨á‰€áŠ• áŒ…á‰¥ áŠ¥áŠ•á‹´á‰µ áŒ…áˆ áŠá‹ á‹ˆá‹³áŒ†á‰¼,á‰ áˆ€á‹­áˆ áŒ á‰…áˆ‹á‹­áˆšáŠ’áˆµá‰´áˆ­ áˆˆáˆ˜áˆ†áŠ• áˆ›áˆ°á‰¥ áŠ áˆ­á‰† áŠ áˆˆáˆ›áˆ°á‰¥áŠ• á‹«áˆ³á‹«áˆá¡á¡,<extra_id_0>áŠ• á‰ áˆ˜áŒá‹°áˆ áŒ /áˆ­áŠá‰µáŠ• áˆˆáˆ˜á‰†áŠ“áŒ áŒ¥ á‹¨áˆšá‹«áˆáˆ á‹¨á‰€áŠ• áŒ…á‰¥ áŠ¥áŠ•á‹´á‰µ áŒ…áˆ áŠá‹ á‹ˆá‹³áŒ†á‰¼\n",
      "673,ar,Ø¥Ù†Ù‡Ù… ÙŠÙƒØ±Ù‡ÙˆÙ† Ù„Ù„Ù…Ø«Ù„ÙŠÙŠÙ† Ù…Ø«Ù„ÙŠ,Ø¥Ù†Ù‡Ù… Ù…Ø¹Ø§Ø¯ÙˆÙ† Ù„Ù„Ù…Ø«Ù„ÙŠÙŠÙ† Ù…Ø«Ù„ÙŠ,<extra_id_0> Ø¥Ù†Ù‡Ù… ÙŠÙƒØ±Ù‡ÙˆÙ† Ù„Ù„Ù…Ø«Ù„ÙŠÙŠÙ† Ù…Ø«Ù„ÙŠ\n",
      "674,es,\"Lo que Cuba hace, como siempre, es propaganda mentirosa para que descerebrados como NuÃ±ez repitan como loros.\",\"Lo que Cuba estÃ¡ haciendo, como siempre, es propaganda para que gente como NÃºÃ±ez repita.\",\"<extra_id_0>, como siempre, es propaganda mentirosa para que descerebrados como NuÃ±ez repitan como loros\"\n",
      "675,ru,ĞĞ½ Ğ¶Ğµ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ±ĞµĞ¶ĞµĞ½ĞµÑ† Ğ² Ğ³ĞµĞ¹Ñ€Ğ¾Ğ¿ĞºĞµ,ĞĞ½ Ğ¶Ğµ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ±ĞµĞ¶ĞµĞ½ĞµÑ† Ğ² ĞµĞ²Ñ€Ğ¾Ğ¿Ğµ,Ğ¾Ğ½ĞµĞ· Ğ¶Ğµ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ±ĞµĞ¶ĞµĞ½ĞµÑ† Ğ² Ğ³ĞµĞ¹Ñ€Ğ¾Ğ¿ĞºĞµ\n",
      "676,ru,Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ñ‚ÑŒ Ñ Ñ‚Ğ¾Ğ±Ğ¾Ğ¹ Ğ±ĞµÑĞµĞ´Ñƒ .............Ğ²Ğ¸Ğ´Ğ½Ğ¾ Ñ‡Ğ¼Ğ¾ ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ .......,Ğ˜ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ñ‚ÑŒ Ñ Ñ‚Ğ¾Ğ±Ğ¾Ğ¹ Ğ±ĞµÑĞµĞ´Ñƒ...... Ğ’Ğ¸Ğ´Ğ½Ğ¾ Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº,Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ñ‚ÑŒ Ñ Ñ‚Ğ¾Ğ±Ğ¾Ğ¹ Ğ±ĞµÑĞµĞ´Ñƒ .............Ğ²Ğ¸Ğ´Ğ½Ğ¾ Ñ‡Ğ¼Ğ¾ ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ .......\n",
      "677,zh,æ°‘ä¸»å…šæçš„å’Œå¹³æ¼”å˜è·¯çº¿ï¼Œè¯´ç™½äº†å°±æ˜¯è‘µèŠ±å®å…¸ã€‚ç°åœ¨ç¾å›½å›½å†…ä¹Ÿæ˜¯ä¸€å †å¥³æƒä¸»ä¹‰ã€é»‘å‘½è´µçš„æ”¿æ²»æ­£ç¡®ã€‚æŠŠè‡ªå·±åˆ‡äº†è®©åˆ«äººæ­»ã€‚å…¸å‹çš„åˆè ¢åˆåã€‚ä½†æ˜¯ä½ ä¸é˜²è¿˜çœŸä¸è¡Œã€‚ä¸é˜²çœŸå®¹æ˜“æ­»ã€‚,æ°‘ä¸»å…šæçš„å’Œå¹³æ¼”å˜è·¯çº¿ï¼Œè¯´ç™½äº†å°±æ˜¯è‘µèŠ±å®å…¸ã€‚ç°åœ¨ç¾å›½å›½å†…ä¹Ÿæ˜¯ä¸€å †å¥³æƒä¸»ä¹‰ã€é‡è§†é»‘äººç”Ÿå‘½çš„æ”¿æ²»æ­£ç¡®ã€‚æŠŠè‡ªå·±åˆ‡äº†è®©åˆ«äººæ­»ã€‚å…¸å‹çš„åˆä¸æ˜æ™ºåˆåã€‚ä½†æ˜¯ä½ ä¸é˜²è¿˜çœŸä¸è¡Œã€‚ä¸é˜²çœŸå®¹æ˜“æ­»ã€‚,\"æ°‘ä¸»å…šæçš„å’Œå¹³æ¼”å˜è·¯çº¿,è¯´ç™½äº†å°±æ˜¯è‘µèŠ±å®å…¸ã€‚ç°åœ¨ç¾å›½å›½å†…ä¹Ÿæ˜¯ä¸€å †å¥³æƒä¸»ä¹‰ã€é»‘å‘½è´µçš„æ”¿æ²»æ­£ç¡®ã€‚æŠŠè‡ªå·±åˆ‡äº†è®©åˆ«äººæ­»ã€‚å…¸å‹çš„åˆè ¢åˆåã€‚ä½†æ˜¯ä½ ä¸é˜²è¿˜çœŸä¸è¡Œã€‚ä¸é˜²çœŸå®¹æ˜“æ­»ã€‚\"\n",
      "678,es,Que cabron el que publicÃ³ la foto de los finalistas de master chefğŸ˜¡ğŸ˜‚ğŸ˜‚,Que malo el que publicÃ³ la foto de los finalistas de master chefğŸ˜¡ğŸ˜‚ğŸ˜‚,<extra_id_0> Que cabron el que publicÃ³ el que publicÃ³ los finalistas de master chefğŸ˜‚ğŸ˜‚\n",
      "679,zh,å°½ç®¡æµªï¼Œè°æ¥ç›˜è°æ˜¯ç‹—ï¼Œä¸å¾—house,å°½ç®¡æµªï¼Œè°æ¥ç›˜è°æ˜¯ç‹—ï¼Œä¸å¾—å¥½ä¸‹åœº,\"ç•…,è°æ¥ç›˜è°æ˜¯ç‹—,ä¸å¾—house\"\n",
      "680,zh,çœ‹çœ‹åŸºä½¬çš„æ¶è¡Œå°±çŸ¥,çœ‹çœ‹æŸäº›ç”·åŒæ€§æ‹çš„ä¸å¥½çš„è¡Œä¸ºå°±çŸ¥,çœ‹çœ‹åŸºä½¬çš„æ¶è¡Œå°±çŸ¥\n",
      "681,am,áˆ°áˆ› áŠ áŠ•á‰° á‹°á‹°á‰¥ áˆ°á‹ áˆ˜á‰°áŠ¨áˆáŠ• áŠ¥áŠ•á‹³áˆµáŒ¨áˆ¨áˆµáŠ­ áŠ á‰µá‰€áˆ­áˆ á‹‹áŒ‹áˆ…áŠ• áŠ¥áˆ°áŒ£áˆ€áˆˆáˆ,áŠ áŠ•á‰° áŠ¥áˆ›á‰³áˆµá‰¥  áˆ˜á‰°áŠ¨áˆáŠ• áŠ¥áŠ•á‹²áˆ… áŠ¥áŠ•á‹³áˆ˜áˆµáŠ«á‰µ áŠ á‰µá‰€áˆ­áˆ  á‹‹áŒ‹ á‰µáŠ¨ááˆ‹áˆˆáˆ…,áˆ°áˆ› áŠ áŠ•á‰° á‹°á‹°á‰¥ áˆ°á‹ áˆ˜á‰°áŠ¨áˆáŠ• áŠ¥áŠ•á‹³áˆµáŒ¨áˆ¨áˆµáŠ­ áŠ á‰µá‰€áˆ­áˆ á‹‹áŒ‹áˆ…áŠ• áŠ¥áˆ°áˆ¨áˆ€áˆˆáˆ\n",
      "682,zh,ä½ æ˜¯æ²¡çœ‹è§shanhelingï¼Œé‚£ä¸ªæ›´æ¶å¿ƒ,ä½ æ˜¯æ²¡çœ‹è§shanhelingï¼Œé‚£ä¸ªæ›´è®©äººæ„Ÿåˆ°ä¸é€‚,\"ä½ æ˜¯æ²¡çœ‹è§shanheling,é‚£ä¸ªæ›´æ¶å¿ƒ\"\n",
      "683,uk,\"ĞÑƒ Ğ½Ğ°Ñ…ÑƒĞ¹, Ğ¼ÑƒĞ´Ğ°ĞºĞ¸ Ñ– Ñ” Ğ¼ÑƒĞ´Ğ°ĞºĞ¸\",\"ĞÑƒ, Ğ¿Ğ¾Ğ³Ğ°Ğ½Ñ– Ñ– Ñ” Ğ¿Ğ¾Ğ³Ğ°Ğ½Ñ–\",\"Ğ¾Ğ½ĞµĞ· Ğ½Ğ°Ñ…ÑƒĞ¹, Ğ¼ÑƒĞ´Ğ°ĞºĞ¸ Ñ– Ñ” Ğ¼ÑƒĞ´Ğ°ĞºĞ¸\"\n",
      "684,de,\"Was fÃ¼r Bullshit. Wenn er mit Wissen und UnterstÃ¼tzung von Merkel gehandelt hat, dann ist das ein Skandal der gesamten Bundesregierung!\",\"Was fÃ¼r Unsinn. Wenn er mit Wissen und UnterstÃ¼tzung von Merkel gehandelt hat, dann ist das ein Skandal der gesamten Bundesregierung!\",\"<extra_id_0> er mit Wissen und UnterstÃ¼tzung von Merkel gehandelt hat, dann ist das ein Skandal der Bundesregierung!\"\n",
      "685,hi,à¤­à¥‹à¤¸à¤¡à¤¼à¥€ à¤•à¥‡ à¤¨à¤¾à¤® à¤•à¥‡ à¤†à¤—à¥‡ à¤šà¥Œà¤•à¥€à¤¦à¤¾à¤° à¤²à¤—à¤¾à¤¨à¥‡ à¤¸à¥‡ à¤•à¥à¤› à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤—à¤¾ !,à¤¨à¤¾à¤® à¤•à¥‡ à¤†à¤—à¥‡ à¤šà¥Œà¤•à¥€à¤¦à¤¾à¤° à¤²à¤—à¤¾à¤¨à¥‡ à¤¸à¥‡ à¤•à¥à¤› à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤—à¤¾ !,<extra_id_0> à¤•à¥‡ à¤¨à¤¾à¤® à¤•à¥‡ à¤†à¤—à¥‡ à¤šà¥Œà¤•à¥€à¤¦à¤¾à¤° à¤²à¤—à¤¾à¤¨à¥‡ à¤¸à¥‡ à¤•à¥à¤› à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤—à¤¾ !\n",
      "686,uk,\"Ğ„ Ñ‚Ğ°ĞºĞ° Ñ…ÑƒĞ¹Ğ½Ñ, ĞºĞ¾Ğ»Ğ¸ Ñ‚Ğ¸ Ñ…Ğ¾Ñ‡ĞµÑˆ Ğ½Ğµ Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ñ‚Ğ¸ Ğ»ÑĞ´Ğ¸Ğ½Ñƒ, Ğ° Ñ‰Ğ¾Ğ± Ğ»ÑĞ´Ğ¸Ğ½Ğ° Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ğ»Ğ° Ñ‚ĞµĞ±Ğµ.\",\"Ğ„ Ñ‚Ğ°ĞºĞ° Ñ€Ñ–Ñ‡, ĞºĞ¾Ğ»Ğ¸ Ñ‚Ğ¸ Ñ…Ğ¾Ñ‡ĞµÑˆ Ğ½Ğµ Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ñ‚Ğ¸ Ğ»ÑĞ´Ğ¸Ğ½Ñƒ, Ğ° Ñ‰Ğ¾Ğ± Ğ»ÑĞ´Ğ¸Ğ½Ğ° Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ğ»Ğ° Ñ‚ĞµĞ±Ğµ.\",\"<extra_id_0>, ĞºĞ¾Ğ»Ğ¸ Ñ‚Ğ¸ Ñ…Ğ¾Ñ‡ĞµÑˆ Ğ½Ğµ Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ñ‚Ğ¸ Ğ»ÑĞ´Ğ¸Ğ½Ñƒ, Ğ° Ñ‰Ğ¾Ğ± Ğ»ÑĞ´Ğ¸Ğ½Ğ° Ğ¿Ğ¾Ğ±Ğ°Ñ‡Ğ¸Ğ»Ğ° Ñ‚ĞµĞ±Ğµ\"\n",
      "687,am,Good for nothing . áŠ¤áˆ­áˆšá‹«áˆµ áŠ¥áŠ•áŠ³áŠ• á‰£áˆˆá‰¤á‰µ á‹¨áˆŒáˆˆá‹ áŠ¨áŠ•á‰± áˆ°á‹ á‰ áˆšáˆ áŠ áŠáˆµá‰°áŠ› áˆ˜áŒ£áŒ¥á á‰¢á‹˜áŒ‹áŒ…áˆˆá‰µ áˆ˜áˆáŠ«áˆ áŠá‹ áŠ¥áˆ‹áˆˆáˆá¢ á‹«áŠ›á‹áŠ• á‰°á‹ˆá‹.á‰ á‰ƒáˆ¬á‹› áŠ¨áŠ¤áˆ­á–,áŠ¤áˆ­áˆšá‹«áˆµ áŠ¥áŠ•áŠ³áŠ• á‹­áˆ…áŠ• á‹¨áˆšáˆ˜áŒ¥áŠ• áŠ áŠáˆµá‰°áŠ› áˆ˜áŒ£áŒ¥á á‰¢á‹˜áŒ‹áŒ…áˆˆá‰µ áˆ˜áˆáŠ«áˆ áŠá‹ áŠ¥áˆ‹áˆˆáˆá¢ á‹«áŠ›á‹áŠ• á‰°á‹ˆá‹,áˆ°á‹ áŠ¥áŠ•áŠ³áŠ• á‰£áˆˆá‰¤á‰µ á‹¨áˆŒáˆˆá‹ áŠ¨áŠ•á‰± áˆ°á‹ á‰ áˆšáˆ áŠ áŠáˆµá‰°áŠ› áˆ˜áŒ£áŒ¥á á‰¢á‹˜áŒ‹áŒ…áˆˆá‰µ áˆ˜áˆáŠ«áˆ áŠá‹ áŠ¥áˆ‹áˆˆáˆá¢\n",
      "688,de,\"Wenn Merkel Klitschko feiert,wird es gefÃ¤hrlich. Die Sklavin Obamas feiert einen neuen Sklaven.\",\"Wenn Merkel Klitschko feiert,wird es gefÃ¤hrlich.\",\"<extra_id_0> Merkel Klitschko feiert,wird es gefÃ¤hrlich. Die Sklavin Obamas feiert einen neuen Sklaven.\"\n",
      "689,zh,ä¸ºå•¥æ€»æœ‰è™¾çˆ¬å­ä¸€æ ·çš„äºŒæç®¡ï¼Ÿæ­£å¸¸åŒ–å°±æ­£å¸¸åŒ–å‘—ï¼Œéè¦è·³å‡ºæ¥è¡¨ç¤ºåŒæ€§æ‹å°±æ˜¯é«˜è´µå°±æ˜¯å…ˆè¿›å°±æ˜¯ç°ä»£åŒ–å—ï¼Ÿæ‰¯çŠŠå­,ä¸ºå•¥æ€»æœ‰è™¾çˆ¬å­ä¸€æ ·çš„äºŒæç®¡ï¼Ÿæ­£å¸¸åŒ–å°±æ­£å¸¸åŒ–å‘—ï¼Œéè¦è·³å‡ºæ¥è¡¨ç¤ºåŒæ€§æ‹å°±æ˜¯é«˜è´µå°±æ˜¯å…ˆè¿›å°±æ˜¯ç°ä»£åŒ–å—ï¼Ÿä¹±è®²è¯,\"ä¸ºå•¥æ€»æœ‰è™¾çˆ¬å­ä¸€æ ·çš„äºŒæç®¡?æ­£å¸¸åŒ–å°±æ­£å¸¸åŒ–å‘—,éè¦è·³å‡ºæ¥è¡¨ç¤ºåŒæ€§æ‹å°±æ˜¯é«˜è´µå°±æ˜¯å…ˆè¿›å°±æ˜¯ç°ä»£åŒ–å—?æ‰¯çŠŠå­\"\n",
      "690,hi,à¤‡à¤¸ à¤¹à¤°à¤¾à¤®à¤œà¤¦à¤“ à¤•à¤¾ à¤¹à¤°à¤¾à¤®à¥€à¤ªà¤¨ à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾ à¤•à¤¿à¤¸à¥€ à¤¬à¤¾à¤¤ à¤¸à¥‡ à¤®à¤¤à¤²à¤¬ à¤¨à¤¹à¥€à¤‚ ,à¤‡à¤¸à¤•à¥‹ à¤¬à¥à¤°à¥‡ à¤•à¤¾à¤® à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾ à¤•à¤¿à¤¸à¥€ à¤¬à¤¾à¤¤ à¤¸à¥‡ à¤®à¤¤à¤²à¤¬ à¤¨à¤¹à¥€à¤‚ ,à¤¹à¤°à¤¾à¤®à¤œà¤¦à¤“ à¤•à¤¾ à¤¹à¤°à¤¾à¤®à¥€à¤ªà¤¨ à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾ à¤•à¤¿à¤¸à¥€ à¤¬à¤¾à¤¤ à¤¸à¥‡ à¤®à¤¤à¤²à¤¬ à¤¨à¤¹à¥€à¤‚\n",
      "691,hi,à¤ªà¤¿à¤›à¤µà¤¾à¤¡à¤¼à¥‡ à¤®à¥‡à¤‚ à¤¦à¤® à¤¹à¥‹à¤¤à¤¾ à¤¤à¥‹ à¤­à¤¾à¤—à¤¤à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤°,à¤²à¤¡à¤¼à¤¨à¥‡ à¤•à¥€ à¤¹à¤¿à¤®à¥à¤®à¤¤ à¤¹à¥‹à¤¤à¥€ à¤¤à¥‹ à¤­à¤¾à¤—à¤¤à¤¾ à¤¨à¤¹à¥€à¤‚ à¤§à¥‹à¤–à¥‡à¤¬à¤¾à¤œ ,à¤¦à¤® à¤¹à¥‹à¤¤à¤¾ à¤¤à¥‹ à¤­à¤¾à¤—à¤¤à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¤°à¤¾à¤®à¤–à¥‹à¤°\n",
      "692,en,what the hell question are you talking about ?,what is the question are you talking about?,<extra_id_0> what the hell question are you talking about?\n",
      "693,uk,\"Ğ”Ğ¾ Ğ´ĞµÑĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğµ Ğ²Ğ¸Ğ½Ğ¸ĞºĞ°Ñ” Ñ–Ğ½ÑˆĞ¸Ñ… Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ, Ğ¾ĞºÑ€Ñ–Ğ¼ \"\"Ğ¢Ğ¸ Ğ±Ğ»ÑÑ‚ÑŒ ÑƒĞ¶Ğµ Ñ€Ğ¾Ğ´Ğ¸Ğ²ÑÑ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼ÑƒĞ´Ğ°ĞºĞ¾Ğ¼?\"\".\",\"Ğ”Ğ¾ Ğ´ĞµÑĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğµ Ğ²Ğ¸Ğ½Ğ¸ĞºĞ°Ñ” Ñ–Ğ½ÑˆĞ¸Ñ… Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ, Ğ¾ĞºÑ€Ñ–Ğ¼ \"\"Ğ¢Ğ¸ ÑƒĞ¶Ğµ Ñ€Ğ¾Ğ´Ğ¸Ğ²ÑÑ Ñ‚Ğ°ĞºĞ¸Ğ¼?\"\".\",\"<extra_id_0> Ğ½Ğµ Ğ²Ğ¸Ğ½Ğ¸ĞºĞ°Ñ” Ñ–Ğ½ÑˆĞ¸Ñ… Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ, Ğ¾ĞºÑ€Ñ–Ğ¼ \"\"Ğ¢Ğ¸ Ğ±Ğ»ÑÑ‚ÑŒ Ñ€Ğ¾Ğ´Ğ¸Ğ²ÑÑ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼ÑƒĞ´Ğ°ĞºĞ¾Ğ¼?\"\".\"\n",
      "694,uk,\"Ğ Ñ‡Ğ¸ Ğ½Ğµ Ğ¼Ñ€Ğ°Ğ·ÑŒ Ñ‚Ğ¸, Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¾?\",\"Ñ‡Ğ¸ Ğ½Ğµ Ñ€ĞµĞ´Ğ¸ÑĞºĞ° Ñ‚Ğ¸, Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¾?\",<extra_id_0>. Ğ Ñ‡Ğ¸ Ğ¼Ñ€Ğ°Ğ·ÑŒ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¾\n",
      "695,es,JAJAJAJAJJAJA que vas a eclipsar tÃº y el otro subnormal xd,Jajajajaja quÃ© vas a eclipse tÃº y el otro xd,<extra_id_0> que vas a eclipsar tÃº y el otro subnormal xd\n",
      "696,es,\"Te gustan los niÃ±os,fascista de mierda?\",Te gustan los niÃ±os?,\"<extra_id_0>,fascista de mierda?\"\n",
      "697,de,\"Trump ist nicht eingeknickt und schon gar nicht vor diesen dummen Politikern, LÃ¼genpresse und Nazimedien in Deutschland!!\",\"Trump ist nicht eingeknickt und schon gar nicht vor diesen Politikern, Presse und Medien in Deutschland!!\",\"<extra_id_0> Trump ist nicht eingeknickt und schon gar nicht vor diesen dummen Politikern, LÃ¼genpresse und Nazimedien in Deutschland!!\"\n",
      "698,zh,æ¥¼ä¸»è¿™è®½åˆºå¤ªæœ‰åŠ›åº¦æŠŠè·ªè™«æ—å’Œå›½é™…é¬¼å­è™«ç±»åŠå…¶å®ƒä»¬åä»£è™«æ··æå†™æ·‹æ¼“å°½è‡´,æ¥¼ä¸»è¿™è®½åˆºå¤ªæœ‰åŠ›åº¦æŠŠå´‡æ´‹æ—å’Œå¤–å›½äººåŠå…¶å®ƒä»¬åä»£æ··è¡€å„¿æå†™æ·‹æ¼“å°½è‡´,<extra_id_0>ä¸»è¿™è®½åˆºå¤ªæœ‰åŠ›åº¦æŠŠè·ªè™«æ—å’Œå›½é™…é¬¼å­è™«ç±»åŠå…¶å®ƒä»¬åä»£æ··æå†™æ·‹æ¼“å°½è‡´\n",
      "699,de,Das darf man bei den GrÃ¼nen Kinderfi....n nicht so eng sehen! ğŸ˜,Das darf man bei den GrÃ¼nen nicht so eng sehen! ğŸ˜,<extra_id_0> das darf man bei den GrÃ¼nen Kinderfi....n nicht so eng sehen! ğŸ˜\n",
      "700,hi,à¤¦à¥‹ à¤šà¤¾à¤° à¤•à¥‹ à¤¶à¥‚à¤Ÿ à¤•à¤°à¥‹ à¤¯à¥‡ à¤¹à¤°à¤¾à¤®à¥€ à¤²à¥‹à¤— à¤…à¤ªà¤¨à¥‡ à¤†à¤ª à¤šà¤¿à¤²à¥à¤²à¤¾à¤¨à¤¾ à¤¬à¤¨à¥à¤¦ à¤•à¤° à¤¦à¥‡à¤‚à¤—à¥‡à¤‚à¥¤,à¤¦à¥‹ à¤šà¤¾à¤° à¤ªà¤° à¤•à¤¡à¤¼à¥€ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤•à¤°à¥‹ à¤¯à¥‡ à¤¬à¥à¤°à¥‡ à¤²à¥‹à¤— à¤…à¤ªà¤¨à¥‡ à¤†à¤ª à¤šà¤¿à¤²à¥à¤²à¤¾à¤¨à¤¾ à¤¬à¤¨à¥à¤¦ à¤•à¤° à¤¦à¥‡à¤‚à¤—à¥‡à¤‚à¥¤,à¤¬à¤¿à¤¨à¤¾ à¤¦à¥‹ à¤šà¤¾à¤° à¤•à¥‹ à¤¶à¥‚à¤Ÿ à¤•à¤° à¤¦à¥‡à¤‚à¤—à¥‡à¤‚à¥¤\n",
      "701,ar,ÙˆÙ„Ø§ ØµÙ„Ø¹Øª Ù…Ù† ÙƒØ«Ø± Ù…Ø§ØªØ­Ù„Ù Ø§Ù† Ø²ÙŠØª Ø§Ù„Ø³Ù„Ø·Ø§Ù† ÙŠØ²ÙŠÙ† Ø§Ù„Ø´Ø¹Ø± ğŸ˜‚ğŸ˜‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙˆÙ… Ù„Ùˆ Ø²Ù†Ø¬ÙŠ ÙƒØ³Ø±Ù‡Ø§ Ø¨Ù†Ù‚ÙˆÙ„ Ù„Ù‡ Ø§Ù„Ù„Ø­Ù… ÙˆØ§Ù„Ø¹Ø¸Ù… Ù„Ø§ ØªØ¨Ù‚ÙŠ Ù„Ù†Ø§ Ø´ÙŠ,ÙˆÙ„Ø§ ØµÙ„Ø¹Øª Ù…Ù† ÙƒØ«Ø± Ù…Ø§ØªØ­Ù„Ù Ø§Ù† Ø²ÙŠØª Ø§Ù„Ø³Ù„Ø·Ø§Ù† ÙŠØ²ÙŠÙ† Ø§Ù„Ø´Ø¹Ø± ğŸ˜‚ğŸ˜‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙˆÙ… Ù„Ùˆ Ø¢Ø®Ø± ÙƒØ³Ø±Ù‡Ø§ Ø¨Ù†Ù‚ÙˆÙ„ Ù„Ù‡ Ø§Ù„Ù„Ø­Ù… ÙˆØ§Ù„Ø¹Ø¸Ù… Ù„Ø§ ØªØ¨Ù‚ÙŠ Ù„Ù†Ø§ Ø´ÙŠ,Ø¹Ù„Ù‰ Ø¹Ù…ÙˆÙ… Ù„Ùˆ Ø²Ù†Ø¬ÙŠ ÙƒØ³Ø±Ù‡Ø§ Ø¨Ù†Ù‚ÙˆÙ„ Ù„Ù‡ Ø§Ù„Ù„Ø­Ù… ÙˆØ§Ù„Ø¹Ø¸Ù… Ù„Ø§ ØªØ¨Ù‚ÙŠ Ù„Ù†Ø§ Ø´ÙŠ\n",
      "702,uk,ĞŸÑ€Ğ¸ Ğ¼ĞµĞ½Ñ– Ñ‚Ğ°ĞºĞ¾Ñ— Ñ…ÑƒĞ¹Ğ½Ñ– Ğ± Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾.,ĞŸÑ€Ğ¸ Ğ¼ĞµĞ½Ñ– Ñ‚Ğ°ĞºĞ¾Ñ— Ğ½Ñ–ÑĞµĞ½Ñ–Ñ‚Ğ½Ğ¸Ñ†Ñ– Ğ± Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾.,Ğ¼ĞµĞ½Ñ– Ñ‚Ğ°ĞºĞ¸Ğ¹ Ñ…ÑƒĞ¹Ğ½Ñ– Ğ± Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾.\n",
      "703,ru,\"Ğ¿Ğ¸Ğ·Ğ´ÑƒĞ¹Ñ‚Ğµ Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ½Ğ° Ğ²Ğ°Ñ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¾, Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ°Ñ Ğ½Ğµ Ñ…Ğ¾Ñ‡ĞµÑ‚ÑÑ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ´Ğ¾Ğ¼ 2\",Ğ£Ñ…Ğ¾Ğ´Ğ¸Ñ‚Ğµ Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ²Ğ°Ñ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ· Ğ·Ğ° Ğ²Ğ°Ñ Ğ´Ğ¾Ğ¼ 2 ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ½Ğµ Ñ…Ğ¾Ñ‡ĞµÑ‚ÑÑ,\"<extra_id_0>, Ğ½Ğ° Ğ²Ğ°Ñ Ğ½Ğµ Ñ…Ğ¾Ñ‡ĞµÑ‚ÑÑ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ´Ğ¾Ğ¼ 2\"\n",
      "704,de,Es gibt deutsche Weiber die sind geil drauf...deswegen wÃ¤hlen sie SPDund GrÃ¼nen..!,Es gibt deutsche Frauen denen das gefÃ¤llt...deswegen wÃ¤hlen sie SPDund GrÃ¼nen..!,<extra_id_0> gibt deutsche Weiber die sind geil draufdeswegen wÃ¤hlen sie SPDund GrÃ¼nen..!\n",
      "705,ar,Ø¯ÙŠØ§Ù†Ø¬.....Ø³ÙˆÙ„ÙŠÙ‡......Ù‚ÙØ´Ù‡ ÙˆÙ…Ù‡Ø²ÙˆÙ…ÙŠÙ† 1/0 Ø³ÙˆÙŠØ³Ø±ÙŠ Ù…ØªØ®Ù„Ù Ø§Ù…Ù‡ ÙƒØ§Ù†Øª Ø¨ØªØ¨ÙŠØ¹ Ø²Ø¨Ø¯Ø© ÙÙŠ Ù„ÙˆØ²Ø§Ù†,Ø¯ÙŠØ§Ù†Ø¬.....Ø³ÙˆÙ„ÙŠÙ‡......Ù‚ÙØ´Ù‡ ÙˆÙ…Ù‡Ø²ÙˆÙ…ÙŠÙ† 1/0 Ø³ÙˆÙŠØ³Ø±ÙŠ Ø§Ù…Ù‡ ÙƒØ§Ù†Øª Ø¨ØªØ¨ÙŠØ¹ Ø²Ø¨Ø¯Ø© ÙÙŠ Ù„ÙˆØ²Ø§Ù†,Ø¯ÙŠØ§Ù†Ø¬.....Ø³ÙˆÙ„ÙŠÙ‡......Ù‚ÙØ´Ù‡ ÙˆÙ…Ù‡Ø²ÙˆÙ…ÙŠÙ† 1/0 Ø³ÙˆÙŠØ³Ø±ÙŠ Ù…ØªØ®Ù„Ù Ø§Ù…Ù‡ ÙƒØ§Ù†Øª Ø¨ØªØ¨ÙŠØ¹ Ø²Ø¨Ø¯Ø© ÙÙŠ Ù„ÙˆØ²Ø§Ù†\n",
      "706,ru,\"Ğ·Ğ° ĞµÑ€? Ğ¿Ñ€Ğ¾ÑˆÑƒ Ğ²ĞºĞ¾Ğ»Ğ¾Ñ‚ÑŒÑÑ:d Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¸ Ğ·Ğ° Ğ¿Ğ¸Ğ´Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑÑƒÑÑ‚, Ñ…Ğ¾Ñ‚Ñ Ñ„Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞµĞºÑ‚Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚\",\"Ğ—Ğ° ĞµÑ€? ĞœĞ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑÑƒÑÑ‚, Ñ…Ğ¾Ñ‚Ñ Ñ„Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞµĞºÑ‚Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚\",\"<extra_id_0> Ğ·Ğ° Ğ¿Ğ¸Ğ´Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑÑƒÑÑ‚, Ñ…Ğ¾Ñ‚Ñ Ñ„Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞµĞºÑ‚Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚\"\n",
      "707,de,Martin Schulz ist 2x sitzen geblieben und hat keinen Schulabschluss.. Wie kann denn so ein Nulltipper als Kanzlerkandidat aufgestellt werden?,Martin Schulz ist 2x sitzen geblieben und hat keinen Schulabschluss.. Wie kann denn so eie Person als Kanzlerkandidat aufgestellt werden?,<extra_id_0> Martin Schulz ist 2x sitzen geblieben und hat keinen Schulabschluss.. Wie kann denn so ein Nulltipper als Kanzlerkandidat aufgestellt werden?\n",
      "708,zh,æ˜¯å•Šï¼é‚£ä¸ªéƒ½æœ‰è¿™äº›è„‘ç˜«å¥‡äººè·ªè™«æ—,æ˜¯å•Šï¼é‚£ä¸ªéƒ½æœ‰è¿™äº›æ„šæ˜§çš„å¥‡äººå´‡æ‹œé»‘äººæ—,æ˜¯å•Š!é‚£ä¸ªéƒ½æœ‰è¿™äº›è„‘ç˜«å¥‡äººè·ªè™«æ—\n",
      "709,zh,ä¸€çœ¼çœŸï¼Œä¹Ÿå°±æ¯ç•œèƒ½å¹²å¾—å‡ºæ¥è¿™ç§äº‹,ä¸€çœ¼çœŸï¼Œä¹Ÿå°±ä»¤æˆ‘ä¸æ»¡çš„å¥³æ€§èƒ½å¹²å¾—å‡ºæ¥è¿™ç§äº‹,\"ä¸€çœ¼çœŸ,ä¹Ÿå°±æ¯ç•œèƒ½å¹²å¾—å‡ºæ¥è¿™ç§äº‹\"\n",
      "710,am,áˆáˆªá‹á‰½ áŠ“á‰¸á‹ áŠ¥áŠ•á‹°áŠ¥áŠ•áˆ± áˆáˆª á‹¨áˆˆáˆ á‰²áŠ’áˆ½ á‰‚áŒ£á‰¸á‹áŠ• á‰ áŒ¥á‹­á‰µ áˆ˜á‹áŒ‹á‰µ áŠá‹ á‹«áŠ•áŠ• áŠá‹ á‹¨áˆˆáˆ˜á‹±á‰µ,ááˆ­áˆ€á‰µ áŠ áˆˆá‰£á‰¸á‹á¤ á‰µáŠ•áˆ½ áŠ¥áˆ­áˆáŒƒ áˆ˜á‹ˆáˆ°á‹µ áŠ áˆˆá‰ á‰µ á‹«áŠ•áŠ• áŠá‹ á‹¨áˆˆáˆ˜á‹±á‰µ,áˆ°á‹á‰½ áŠ“á‰¸á‹ áŠ¥áŠ•á‹°áŠ¥áŠ•áˆ± áˆáˆª á‹¨áˆˆáˆ á‰²áŠ’áˆ½ á‰‚áŒ£á‰¸á‹áŠ• á‰ áŒ¥á‹­á‰µ áˆ˜á‹áŒ‹á‰µ áŠá‹ á‹«áŠ•áŠ• áŠá‹ á‹¨áˆˆáˆ˜á‹±á‰µ\n",
      "711,uk,ĞšĞ¾Ğ»Ğ¸ Ğ¿Ğ°Ñ†Ğ°Ğ½ - Ğ¿Ğ¸Ğ·Ğ´ĞµÑ†ÑŒ ÑĞº Ğ²Ñ–Ğ´Ñ€Ğ°Ğ·Ğ»Ğ¸Ğ²Ğ¾.,ĞšĞ¾Ğ»Ğ¸ Ğ¿Ğ°Ñ†Ğ°Ğ½ - Ğ³ĞµÑ‚ÑŒ ÑĞº Ğ²Ñ–Ğ´Ñ€Ğ°Ğ·Ğ»Ğ¸Ğ²Ğ¾.,ĞšĞ¾Ğ»Ğ¸ Ğ¿Ğ°Ñ†Ğ°Ğ½ - Ğ¿Ğ¸Ğ·Ğ´ĞµÑ†ÑŒ ÑĞº Ğ²Ñ–Ğ´Ñ€Ğ°Ğ·Ğ»Ğ¸Ğ²Ğ¾\n",
      "712,de,ich finde gut dass #Trump #Jerusalem als Hauptstadt anerkennt nun bekommen die #Israelis erst richtig auf die Fresse,ich finde gut dass #Trump #Jerusalem als Hauptstadt anerkennt nun bekommen die #Israelis erst richtig Probleme,<extra_id_0> ich finde gut dass #Trump #Jerusalem als Hauptstadt anerkennt nun bekommen die #Israelis erst richtig auf die Fresse\n",
      "713,uk,ĞºĞµĞºÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ°Ğ¼Ğ½Ğ° Ğ¼ĞµĞ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ğ¿Ğ¾Ñ…ÑƒĞ¹ Ğ±ÑƒĞ»Ğ¾,ĞºĞµĞºÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ°Ğ¼Ğ½Ğ° Ğ¼ĞµĞ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ğ²ÑĞµ Ğ¾Ğ´Ğ½Ğ¾ Ğ±ÑƒĞ»Ğ¾,Ğ¾Ğ½ĞµĞ· ĞºĞµĞºÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ°Ğ¼Ğ½Ğ° Ğ¼ĞµĞ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ğ¿Ğ¾Ñ…ÑƒĞ¹ Ğ±ÑƒĞ»Ğ¾\n",
      "714,ru,Ğ¢Ğ°Ğ¼ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾ Ğ¾Ğ½ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» Ğ¸ ÑƒĞ±ĞµĞ´Ğ¸Ğ»ÑÑ Ñ‚ÑƒĞ¿Ğ¸Ñ†Ğ° Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¸ÑÑŒ ÑƒĞ¶Ğµ,Ğ¢Ğ°Ğ¼ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾ Ğ¾Ğ½ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» Ğ¸ ÑƒĞ±ĞµĞ´Ğ¸Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¸ÑÑŒ ÑƒĞ¶Ğµ,<extra_id_0> Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾ Ğ¾Ğ½ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» Ğ¸ ÑƒĞ±ĞµĞ´Ğ¸Ğ»ÑÑ Ñ‚ÑƒĞ¿Ğ¸Ñ†Ğ° Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¸ÑÑŒ ÑƒĞ¶Ğµ\n",
      "715,ru,Ğ½ĞµĞ½Ğ°Ğ²Ğ¸Ğ¶Ñƒ Ğ¿Ñ€ĞµĞ·Ğ¸Ñ€Ğ°Ñ Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ€Ğ°Ğ³Ğ°Ğ¼ Ğ½Ğµ Ğ²ĞµÑ€Ğ¸Ğ»Ğ° Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ²ĞµÑ€ÑĞ»Ğ° Ñ‡Ñ‚Ğ¾Ğ± Ğ¾Ğ½ Ğ¸Ğ·Ğ´Ğ¾Ñ… !!! Ñ‡Ğ¼Ğ¾ Ğ±Ğ¾Ñ‚Ğ¾ĞºÑĞ½Ğ¾Ğµ,\"ĞĞµĞ½Ğ°Ğ²Ğ¸Ğ¶Ñƒ, Ğ¿Ñ€ĞµĞ·Ğ¸Ñ€Ğ°Ñ, Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ€Ğ°Ğ³Ğ°Ğ¼ Ğ½Ğµ Ğ²ĞµÑ€Ğ¸Ğ»Ğ° Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ²ĞµÑ€ÑĞ»Ğ°.\",Ğ½ĞµĞ½Ğ°Ğ²Ğ¸Ğ¶Ñƒ Ğ¿Ñ€ĞµĞ·Ğ¸Ñ€Ğ°Ñ Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ€Ğ°Ğ³Ğ°Ğ¼ Ğ½Ğµ Ğ²ĞµÑ€Ğ¸Ğ»Ğ° Ñ‡Ñ‚Ğ¾Ğ± Ğ¾Ğ½ Ğ¸Ğ·Ğ´Ğ¾Ñ… !!!\n",
      "716,ru,\"Ğ¡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ğ¹ĞºÑƒ, ÑĞ°Ğ¼Ğ¸ Ğ¶Ğµ Ğ·Ğ° ÑÑ‚Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚Ğµ.\",\"Ğ¡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ² Ğ½ĞµĞ¿Ğ¾Ğ¹Ğ¼Ğ¸ Ñ‡Ñ‚Ğ¾, ÑĞ°Ğ¼Ğ¼ Ğ¶Ğµ Ğ·Ğ° ÑÑ‚Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚Ğµ\",ÑĞ°Ğ¼Ğ¸ Ğ¶Ğµ Ğ·Ğ° ÑÑ‚Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚Ğµ\n",
      "717,hi,à¤­à¤¡à¤¼à¤µà¤¾ à¤—à¤¿à¤°à¥€ à¤•à¥€ à¤®à¥‚à¤°à¥à¤¤à¤¿ à¤¶à¥à¤°à¥€ à¤•à¥‡à¤œà¤°à¥€à¤µà¤¾à¤² à¤œà¥€,à¤—à¤²à¤¤ à¤•à¤¾à¤®à¥‹à¤‚ à¤•à¥€ à¤®à¥‚à¤°à¥à¤¤à¤¿ à¤¶à¥à¤°à¥€ à¤•à¥‡à¤œà¤°à¥€à¤µà¤¾à¤² à¤œà¥€,<extra_id_0> à¤­à¤¡à¤¼à¤µà¤¾ à¤—à¤¿à¤°à¥€ à¤®à¥‚à¤°à¥à¤¤à¤¿ à¤¶à¥à¤°à¥€ à¤¶à¥à¤°à¥€à¤œà¤°à¥€ à¤œà¥€\n",
      "718,es,Pobre tarado que no sabes lo que dices .... pobre tarado,no sabes lo que dices,<extra_id_0> no sabes lo que tienes\n",
      "719,es,\"Me gustarÃ­a saber lo que tÃº harÃ­as, si te llegarÃ¡ a pasar a ti. Eres el tÃ­pico charlatÃ¡n de feria.\",Me gustarÃ­a saber quÃ© harÃ­as si te sucediera a ti. A veces pareces muy persuasivo en tus argumentos.,\"<extra_id_0> me gustarÃ­a saber lo que tÃº harÃ­as, si te llegarÃ¡ a pasar a ti.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ğŸ”® FULL DATASET INFERENCE & CSV EXPORT\n",
    "\n",
    "\n",
    "print(\"ğŸ”® Starting full dataset inference...\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model_lora.eval()\n",
    "\n",
    "# Get the full test dataset (all languages)\n",
    "full_test_dataset = dataset['test']\n",
    "print(f\"ğŸ“Š Full test dataset size: {len(full_test_dataset)} samples\")\n",
    "\n",
    "# Function to generate detoxified text (reuse from previous cell)\n",
    "def detoxify_text(toxic_text, max_length=512):\n",
    "    \"\"\"Generate detoxified version of toxic text\"\"\"\n",
    "    input_text = f\"{toxic_text}\"\n",
    "    \n",
    "    inputs = TOKENIZER(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(model_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_lora.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=TOKENIZER.pad_token_id,\n",
    "            eos_token_id=TOKENIZER.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Prepare results list\n",
    "all_results = []\n",
    "\n",
    "# Process all samples with progress bar\n",
    "print(f\"\\nğŸ§ª Processing {len(full_test_dataset)} samples...\")\n",
    "for i in tqdm(range(len(full_test_dataset)), desc=\"Generating detoxified text\"):\n",
    "    sample = full_test_dataset[i]\n",
    "    \n",
    "    try:\n",
    "        # Extract sample data\n",
    "        toxic_sentence = sample['toxic_sentence']\n",
    "        expected_neutral = sample['neutral_sentence']\n",
    "        language = sample['language']\n",
    "        \n",
    "        # Generate detoxified version\n",
    "        generated_neutral = detoxify_text(toxic_sentence)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'sample_id': i,\n",
    "            'language': language,\n",
    "            'toxic_input': toxic_sentence,\n",
    "            'expected_output': expected_neutral,\n",
    "            'generated_output': generated_neutral\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Print progress every 100 samples\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"âœ… Processed {i + 1}/{len(full_test_dataset)} samples\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing sample {i}: {e}\")\n",
    "        # Add error entry\n",
    "        all_results.append({\n",
    "            'sample_id': i,\n",
    "            'language': sample.get('language', 'unknown'),\n",
    "            'toxic_input': sample.get('toxic_sentence', ''),\n",
    "            'expected_output': sample.get('neutral_sentence', ''),\n",
    "            'generated_output': f'ERROR: {str(e)}'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Add timestamp for unique filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"mt5_detoxification_results_{timestamp}.csv\"\n",
    "# Save to CSV\n",
    "results_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Inference completed!\")\n",
    "print(f\"ğŸ“Š Results summary:\")\n",
    "print(f\"  Total samples processed: {len(results_df)}\")\n",
    "print(f\"  Languages in dataset: {results_df['language'].value_counts().to_dict()}\")\n",
    "print(f\"  Successful generations: {len(results_df[~results_df['generated_output'].str.contains('ERROR:')])}\")\n",
    "print(f\"  Errors: {len(results_df[results_df['generated_output'].str.contains('ERROR:')])}\")\n",
    "print(f\"  ğŸ“ Results saved to: {csv_filename}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\nğŸ¯ Sample results:\")\n",
    "print(\"=\" * 100)\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\nSample {i+1} ({row['language']}):\")\n",
    "    print(f\"ğŸ”´ Toxic:     {row['toxic_input'][:80]}{'...' if len(row['toxic_input']) > 80 else ''}\")\n",
    "    print(f\"ğŸŸ¢ Expected:  {row['expected_output'][:80]}{'...' if len(row['expected_output']) > 80 else ''}\")\n",
    "    print(f\"ğŸ”µ Generated: {row['generated_output'][:80]}{'...' if len(row['generated_output']) > 80 else ''}\")\n",
    "\n",
    "# Basic evaluation metrics\n",
    "print(f\"\\nğŸ“ˆ Quick evaluation metrics:\")\n",
    "\n",
    "# Language-wise statistics\n",
    "lang_stats = results_df.groupby('language').agg({\n",
    "    'sample_id': 'count',\n",
    "    'generated_output': lambda x: (~x.str.contains('ERROR:')).sum()\n",
    "}).rename(columns={'sample_id': 'total_samples', 'generated_output': 'successful_generations'})\n",
    "\n",
    "print(\"\\nğŸ“Š Per-language statistics:\")\n",
    "for lang, stats in lang_stats.iterrows():\n",
    "    success_rate = (stats['successful_generations'] / stats['total_samples']) * 100\n",
    "    print(f\"  {lang}: {stats['successful_generations']}/{stats['total_samples']} ({success_rate:.1f}% success)\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ CSV file '{csv_filename}' contains all results and can be opened in Excel or imported for further analysis.\")\n",
    "\n",
    "with open(csv_filename) as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170bec6",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7be87f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAVING FINE-TUNED mT5 MODEL ===\n",
      "ğŸ“ Storage type: LOCAL\n",
      "ğŸ’¾ Save path: .\\model_backups\\/mt5-detox-lora-20251122_161504\n",
      "ğŸ“ Created save directory: .\\model_backups\\/mt5-detox-lora-20251122_161504\n",
      "\n",
      "ğŸ”§ Option 1: Save LoRA adapter to Google Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRA adapter saved to: .\\model_backups\\/mt5-detox-lora-20251122_161504\n",
      "ğŸ“ Saving tokenizer...\n",
      "âœ… Tokenizer saved alongside the model\n",
      "ğŸ“„ Model info saved to: .\\model_backups\\/mt5-detox-lora-20251122_161504/model_info.txt\n",
      "\n",
      "â˜ï¸ Option 2: Backup to Hugging Face Hub (optional)\n",
      "To save to HF Hub, uncomment and run:\n",
      "# model_lora_stable.push_to_hub('your-username/mt5-detox-model')\n",
      "\n",
      "ğŸ“Š Model info:\n",
      "  Model type: <class 'peft.peft_model.PeftModelForSeq2SeqLM'>\n",
      "  Base model: google/mt5-base\n",
      "  LoRA rank: 8\n",
      "  Target modules: {'q', 'v'}\n",
      "  Training precision: bfloat16\n",
      "  Save location: .\\model_backups\\/mt5-detox-lora-20251122_161504\n",
      "\n",
      "ğŸ¯ Model successfully saved to Google Drive with timestamp: 20251122_161504\n",
      "ğŸ’¡ The timestamped folder helps you keep track of different training runs!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ SAVE TRAINED MODEL - Multiple Options (with Cloud Storage support)\n",
    "\n",
    "print(\"=== SAVING FINE-TUNED mT5 MODEL ===\")\n",
    "\n",
    "# Configure save path with automatic cloud storage detection\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped folder name\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_folder_name = f\"mt5-detox-lora-{timestamp}\"\n",
    "\n",
    "# Smart path detection function\n",
    "def get_best_save_path():\n",
    "    \"\"\"Automatically detect the best cloud storage path available\"\"\"\n",
    "    username = os.getenv('USERNAME', 'user')  # Windows\n",
    "    \n",
    "    # Priority order: OneDrive -> Google Drive -> Local\n",
    "    possible_paths = [\n",
    "        # OneDrive paths (since you're using OneDrive)\n",
    "        f\"C:\\\\Users\\\\{username}\\\\OneDrive\\\\ML_Models\\\\mt5-detox\\\\\",\n",
    "        f\"C:\\\\Users\\\\{username}\\\\OneDrive\\\\Documents\\\\ML_Models\\\\mt5-detox\\\\\",\n",
    "        \n",
    "        # Google Drive Desktop paths\n",
    "        f\"C:\\\\Users\\\\{username}\\\\Google Drive\\\\ML_Models\\\\mt5-detox\\\\\",\n",
    "        \"G:\\\\My Drive\\\\ML_Models\\\\mt5-detox\\\\\",\n",
    "        \n",
    "        # Local fallback\n",
    "        \".\\\\model_backups\\\\\",\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            # Test if parent directory exists or can be created\n",
    "            parent = os.path.dirname(path)\n",
    "            if os.path.exists(parent) or path.startswith('.\\\\'):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                return path, \"cloud\" if not path.startswith('.\\\\') else \"local\"\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Final fallback\n",
    "    fallback = \".\\\\model_backups\\\\\"\n",
    "    os.makedirs(fallback, exist_ok=True)\n",
    "    return fallback, \"local\"\n",
    "\n",
    "# Get the best available path\n",
    "base_path, storage_type = get_best_save_path()\n",
    "save_path = os.path.join(base_path, model_folder_name)\n",
    "\n",
    "print(f\"ğŸ“ Storage type: {storage_type.upper()}\")\n",
    "print(f\"ğŸ’¾ Save path: {save_path}\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print(f\"ğŸ“ Created save directory: {save_path}\")\n",
    "\n",
    "# OPTION 1: Save LoRA adapter to Google Drive (Recommended)\n",
    "print(\"\\nğŸ”§ Option 1: Save LoRA adapter to Google Drive\")\n",
    "try:\n",
    "    model_lora_stable.save_pretrained(save_path)\n",
    "    print(f\"âœ… LoRA adapter saved to: {save_path}\")\n",
    "    \n",
    "    # Save tokenizer as well (important!)\n",
    "    print(\"ğŸ“ Saving tokenizer...\")\n",
    "    TOKENIZER.save_pretrained(save_path)\n",
    "    print(\"âœ… Tokenizer saved alongside the model\")\n",
    "    \n",
    "    # Create a model info file\n",
    "    info_file = os.path.join(save_path, \"model_info.txt\")\n",
    "    with open(info_file, \"w\") as f:\n",
    "        f.write(f\"Model Training Information\\n\")\n",
    "        f.write(f\"========================\\n\")\n",
    "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Base Model: google/mt5-base\\n\")\n",
    "        f.write(f\"Task: Text Detoxification\\n\")\n",
    "        f.write(f\"LoRA Rank: {model_lora_stable.peft_config['default'].r}\\n\")\n",
    "        f.write(f\"LoRA Alpha: {model_lora_stable.peft_config['default'].lora_alpha}\\n\")\n",
    "        f.write(f\"Target Modules: {model_lora_stable.peft_config['default'].target_modules}\\n\")\n",
    "        f.write(f\"Training Precision: bfloat16\\n\")\n",
    "        f.write(f\"Dataset Size: {len(tokenized_dataset)} samples\\n\")\n",
    "    print(f\"ğŸ“„ Model info saved to: {info_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving to Google Drive: {e}\")\n",
    "    print(\"ğŸ’¡ Falling back to local save...\")\n",
    "    fallback_path = f\"./mt5-detox-lora-{timestamp}\"\n",
    "    model_lora_stable.save_pretrained(fallback_path)\n",
    "    TOKENIZER.save_pretrained(fallback_path)\n",
    "    print(f\"âœ… Model saved locally to: {fallback_path}\")\n",
    "\n",
    "# OPTION 2: Additional backup to Hugging Face Hub\n",
    "print(\"\\nâ˜ï¸ Option 2: Backup to Hugging Face Hub (optional)\")\n",
    "print(\"To save to HF Hub, uncomment and run:\")\n",
    "print(\"# model_lora_stable.push_to_hub('your-username/mt5-detox-model')\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Model info:\")\n",
    "print(f\"  Model type: {type(model_lora_stable)}\")\n",
    "print(f\"  Base model: google/mt5-base\")\n",
    "print(f\"  LoRA rank: {model_lora_stable.peft_config['default'].r}\")\n",
    "print(f\"  Target modules: {model_lora_stable.peft_config['default'].target_modules}\")\n",
    "print(f\"  Training precision: bfloat16\")\n",
    "print(f\"  Save location: {save_path}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Model successfully saved to Google Drive with timestamp: {timestamp}\")\n",
    "print(\"ğŸ’¡ The timestamped folder helps you keep track of different training runs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e76a5d",
   "metadata": {},
   "source": [
    "# ğŸ“‚ **Model Saving Options Explained**\n",
    "\n",
    "## ğŸ”§ **Option 1: LoRA Adapter Only (Recommended)**\n",
    "- **Size**: ~10-50 MB (very small!)\n",
    "- **Contains**: Only the fine-tuned LoRA weights\n",
    "- **Requires**: Original base model (google/mt5-base) to load\n",
    "- **Best for**: Sharing, version control, experimentation\n",
    "\n",
    "## ğŸ—ï¸ **Option 2: Complete Model** \n",
    "- **Size**: ~2-4 GB (full model size)\n",
    "- **Contains**: Entire model with merged weights\n",
    "- **Standalone**: Can be loaded independently\n",
    "- **Issues**: May not work with quantized models\n",
    "\n",
    "## â˜ï¸ **Option 3: Hugging Face Hub**\n",
    "- **Benefits**: Easy sharing, version control, public access\n",
    "- **Size**: Same as Option 1 (just LoRA adapter)\n",
    "- **Usage**: Perfect for model deployment and collaboration\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ **Why LoRA Adapter is Best**\n",
    "\n",
    "1. **Tiny file size** - Only stores the adaptation, not the full model\n",
    "2. **Version control friendly** - Easy to track changes in Git\n",
    "3. **Multiple adapters** - Can train different adapters for different tasks\n",
    "4. **Memory efficient** - Load base model once, swap adapters quickly\n",
    "5. **Quantization compatible** - Works perfectly with your 4-bit setup\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ **File Structure After Saving**\n",
    "\n",
    "```\n",
    "mt5-detox-lora-adapter/\n",
    "â”œâ”€â”€ adapter_config.json       # LoRA configuration\n",
    "â”œâ”€â”€ adapter_model.safetensors  # LoRA weights (~10-50MB)\n",
    "â”œâ”€â”€ tokenizer.json            # Tokenizer files\n",
    "â”œâ”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ special_tokens_map.json\n",
    "â””â”€â”€ spiece.model\n",
    "```\n",
    "\n",
    "**Total size**: Usually under 100MB vs ~2GB for full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ GOOGLE DRIVE SETUP HELPER\n",
    "\n",
    "def setup_google_drive_path():\n",
    "    \"\"\"Helper function to set up cloud storage path based on local environment\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    username = os.getenv('USERNAME', 'user')  # Windows\n",
    "    \n",
    "    # Priority order paths for local Windows environment\n",
    "    possible_paths = [\n",
    "        # OneDrive paths (highest priority since you're using OneDrive)\n",
    "        f\"C:\\\\Users\\\\{username}\\\\OneDrive\\\\ML_Models\\\\\",\n",
    "        f\"C:\\\\Users\\\\{username}\\\\OneDrive\\\\Documents\\\\ML_Models\\\\\",\n",
    "        \n",
    "        # Google Drive Desktop paths\n",
    "        f\"C:\\\\Users\\\\{username}\\\\Google Drive\\\\ML_Models\\\\\",\n",
    "        \"G:\\\\My Drive\\\\ML_Models\\\\\",\n",
    "        \n",
    "        # Dropbox alternative\n",
    "        f\"C:\\\\Users\\\\{username}\\\\Dropbox\\\\ML_Models\\\\\",\n",
    "        \n",
    "        # Local fallback in your workspace\n",
    "        \".\\\\model_backups\\\\\",\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” Searching for cloud storage paths...\")\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            # Check if parent directory exists\n",
    "            parent = os.path.dirname(path) if not path.endswith('\\\\') else path[:-1]\n",
    "            \n",
    "            if os.path.exists(parent) or path.startswith('.\\\\'):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                path_type = \"OneDrive\" if \"OneDrive\" in path else (\"Google Drive\" if \"Google Drive\" in path else \"Local\")\n",
    "                print(f\"âœ… Found {path_type}: {path}\")\n",
    "                return path\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Could not access {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Final fallback\n",
    "    fallback = \".\\\\model_backups\\\\\"\n",
    "    os.makedirs(fallback, exist_ok=True)\n",
    "    print(f\"âš ï¸  Using local fallback: {fallback}\")\n",
    "    return fallback\n",
    "\n",
    "# Test the setup\n",
    "recommended_path = setup_google_drive_path()\n",
    "print(f\"\\nğŸ¯ Recommended save path: {recommended_path}\")\n",
    "print(\"\\nğŸ’¡ Tips:\")\n",
    "print(\"  â€¢ For Google Colab: Enable 'Mount Drive' in the sidebar\")\n",
    "print(\"  â€¢ For local use: Install Google Drive Desktop app\")\n",
    "print(\"  â€¢ For OneDrive: The path should be automatically detected\")\n",
    "print(\"  â€¢ Manual setup: Edit GDRIVE_PATH variable in the save code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166f86",
   "metadata": {},
   "source": [
    "# ğŸ”§ Key Parameters for Experimenting with Different Configurations\n",
    "\n",
    "## 1. LoRA Configuration Parameters\n",
    "\n",
    "### **LoRA Rank (r)** - Most Important\n",
    "- **Current**: `r=8` (conservative)\n",
    "- **Options**: `4, 8, 16, 32, 64`\n",
    "- **Impact**: Higher rank = more parameters = better adaptation but more memory\n",
    "- **Recommendation**: Start with 8-16, increase if underfitting\n",
    "\n",
    "### **LoRA Alpha** - Second Most Important  \n",
    "- **Current**: `lora_alpha=16`\n",
    "- **Formula**: Usually `lora_alpha = 2 * r` or `lora_alpha = r`\n",
    "- **Impact**: Controls LoRA scaling, higher = stronger adaptation\n",
    "- **Options**: `8, 16, 32, 64`\n",
    "\n",
    "### **Target Modules** - Architecture Dependent\n",
    "- **Current**: `[\"q\", \"v\"]` (conservative)\n",
    "- **Options for T5**: \n",
    "  - Minimal: `[\"q\", \"v\"]`\n",
    "  - Standard: `[\"q\", \"v\", \"k\", \"o\"]` \n",
    "  - Comprehensive: `[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]`\n",
    "- **Impact**: More modules = better adaptation but more parameters\n",
    "\n",
    "### **LoRA Dropout**\n",
    "- **Current**: `0.05`\n",
    "- **Options**: `0.0, 0.05, 0.1, 0.2`\n",
    "- **Impact**: Regularization, higher = less overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quantization Parameters\n",
    "\n",
    "### **Compute Dtype** - Stability vs Speed\n",
    "- **Current**: `torch.bfloat16` (most stable)\n",
    "- **Options**: \n",
    "  - `torch.float16` (faster, less stable)\n",
    "  - `torch.bfloat16` (balanced)\n",
    "  - `torch.float32` (most stable, slower)\n",
    "\n",
    "### **Double Quantization**\n",
    "- **Current**: `False` (for stability)\n",
    "- **Options**: `True/False`\n",
    "- **Impact**: `True` saves memory but can cause instability\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training Arguments\n",
    "\n",
    "### **Learning Rate** - Critical for Convergence\n",
    "- **Current**: `1e-4`\n",
    "- **Options**: `5e-5, 1e-4, 2e-4, 3e-4, 5e-4`\n",
    "- **Rule**: Lower for stable training, higher for faster convergence\n",
    "\n",
    "### **Batch Size & Gradient Accumulation**\n",
    "- **Current**: `batch_size=2, grad_accum=8` (effective=16)\n",
    "- **Options**:\n",
    "  - Small GPU: `batch=1, accum=16`\n",
    "  - Medium GPU: `batch=2, accum=8` \n",
    "  - Large GPU: `batch=4, accum=4`\n",
    "\n",
    "### **Learning Rate Scheduler**\n",
    "- **Current**: Default (linear decay)\n",
    "- **Options**: `\"linear\", \"cosine\", \"polynomial\", \"constant\"`\n",
    "- **Add**: `lr_scheduler_type=\"cosine\"` to TrainingArguments\n",
    "\n",
    "### **Warmup Steps**\n",
    "- **Current**: Not set\n",
    "- **Options**: `50, 100, 200` steps\n",
    "- **Add**: `warmup_steps=100` to TrainingArguments\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Processing Parameters\n",
    "\n",
    "### **Max Length**\n",
    "- **Current**: `512` tokens\n",
    "- **Options**: `256, 384, 512, 768`\n",
    "- **Impact**: Longer = more context but more memory\n",
    "\n",
    "### **Preprocessing Prompt**\n",
    "- **Current**: `\"detoxify: {text}\"`\n",
    "- **Options**:\n",
    "  - `\"Rewrite to be non-toxic: {text}\"`\n",
    "  - `\"Make this text neutral: {text}\"`\n",
    "  - `\"Remove toxicity: {text}\"`\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Advanced Training Parameters\n",
    "\n",
    "### **Gradient Clipping**\n",
    "- **Current**: `max_grad_norm=1.0`\n",
    "- **Options**: `0.5, 1.0, 2.0`\n",
    "\n",
    "### **Weight Decay**\n",
    "- **Current**: Default (0.01)\n",
    "- **Options**: `0.0, 0.01, 0.1`\n",
    "- **Add**: `weight_decay=0.1` to TrainingArguments\n",
    "\n",
    "### **Adam Optimizer Parameters**\n",
    "- **Add to TrainingArguments**:\n",
    "  - `adam_beta1=0.9`\n",
    "  - `adam_beta2=0.999`\n",
    "  - `adam_epsilon=1e-8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1464a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª CONFIGURATION TEMPLATES - Ready to Copy & Modify\n",
    "\n",
    "# ========================================\n",
    "# TEMPLATE 1: AGGRESSIVE CONFIGURATION\n",
    "# ========================================\n",
    "def create_aggressive_config():\n",
    "    \"\"\"Higher capacity, faster learning - for underfitting issues\"\"\"\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,  # Re-enable for memory efficiency\n",
    "    )\n",
    "    \n",
    "    # LoRA config - More parameters\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Increased rank\n",
    "        lora_alpha=32,  # Increased alpha\n",
    "        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],  # All modules\n",
    "        lora_dropout=0.1,  # Higher dropout\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "    )\n",
    "    \n",
    "    # Training args - Faster learning\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./mt5-detoxify-aggressive\",\n",
    "        per_device_train_batch_size=4,  # Larger batch if GPU allows\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,  # Higher learning rate\n",
    "        bf16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=500,\n",
    "        eval_strategy=\"no\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=1.0,\n",
    "        warmup_steps=100,  # Add warmup\n",
    "        lr_scheduler_type=\"cosine\",  # Cosine scheduler\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    return bnb_config, lora_config, training_args\n",
    "\n",
    "# ========================================\n",
    "# TEMPLATE 2: CONSERVATIVE CONFIGURATION  \n",
    "# ========================================\n",
    "def create_conservative_config():\n",
    "    \"\"\"Lower capacity, stable training - for overfitting or instability\"\"\"\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,  # Disabled for stability\n",
    "    )\n",
    "    \n",
    "    # LoRA config - Fewer parameters\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,  # Very low rank\n",
    "        lora_alpha=8,  # Low alpha\n",
    "        target_modules=[\"q\", \"v\"],  # Minimal modules\n",
    "        lora_dropout=0.05,  # Low dropout\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "    )\n",
    "    \n",
    "    # Training args - Slow and stable\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./mt5-detoxify-conservative\",\n",
    "        per_device_train_batch_size=1,  # Small batch\n",
    "        gradient_accumulation_steps=16,  # More accumulation\n",
    "        num_train_epochs=5,  # More epochs\n",
    "        learning_rate=5e-5,  # Lower learning rate\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        eval_strategy=\"no\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=0.5,  # Stricter clipping\n",
    "        warmup_steps=200,  # Longer warmup\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        weight_decay=0.1,  # Higher regularization\n",
    "    )\n",
    "    \n",
    "    return bnb_config, lora_config, training_args\n",
    "\n",
    "# ========================================\n",
    "# TEMPLATE 3: BALANCED CONFIGURATION\n",
    "# ========================================\n",
    "def create_balanced_config():\n",
    "    \"\"\"Middle ground - good starting point\"\"\"\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "    \n",
    "    # LoRA config - Moderate parameters\n",
    "    lora_config = LoraConfig(\n",
    "        r=12,  # Medium rank\n",
    "        lora_alpha=24,  # Proportional alpha\n",
    "        target_modules=[\"q\", \"v\", \"k\", \"o\"],  # Core attention modules\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "    )\n",
    "    \n",
    "    # Training args - Balanced\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./mt5-detoxify-balanced\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1.5e-4,  # Medium learning rate\n",
    "        bf16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=500,\n",
    "        eval_strategy=\"no\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=1.0,\n",
    "        warmup_steps=100,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        weight_decay=0.05,\n",
    "    )\n",
    "    \n",
    "    return bnb_config, lora_config, training_args\n",
    "\n",
    "print(\"âœ… Configuration templates created!\")\n",
    "print(\"ğŸ“‹ Available templates:\")\n",
    "print(\"  - create_aggressive_config(): High capacity, fast learning\")\n",
    "print(\"  - create_conservative_config(): Low capacity, stable training\") \n",
    "print(\"  - create_balanced_config(): Middle ground approach\")\n",
    "print(\"\\nğŸ”„ To use a template:\")\n",
    "print(\"  bnb_config, lora_config, training_args = create_balanced_config()\")\n",
    "print(\"  # Then recreate model and trainer with new configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ QUICK EXPERIMENT HELPER\n",
    "def create_model_with_config(config_name=\"balanced\"):\n",
    "    \"\"\"Quick way to create a new model with different configurations\"\"\"\n",
    "    \n",
    "    # Clear previous model\n",
    "    import gc\n",
    "    if 'model_lora_stable' in globals():\n",
    "        del model_lora_stable\n",
    "    if 'model_quantized_stable' in globals():\n",
    "        del model_quantized_stable\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Get configuration\n",
    "    if config_name == \"aggressive\":\n",
    "        bnb_config, lora_config, training_args = create_aggressive_config()\n",
    "    elif config_name == \"conservative\":\n",
    "        bnb_config, lora_config, training_args = create_conservative_config()\n",
    "    elif config_name == \"balanced\":\n",
    "        bnb_config, lora_config, training_args = create_balanced_config()\n",
    "    else:\n",
    "        raise ValueError(\"config_name must be 'aggressive', 'conservative', or 'balanced'\")\n",
    "    \n",
    "    print(f\"ğŸ”§ Creating model with {config_name} configuration...\")\n",
    "    \n",
    "    # Load model with new config\n",
    "    model_quantized = T5ForConditionalGeneration.from_pretrained(\n",
    "        \"google/mt5-base\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Prepare for training\n",
    "    model_quantized = prepare_model_for_kbit_training(model_quantized)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model_lora = get_peft_model(model_quantized, lora_config)\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        TOKENIZER, \n",
    "        model=model_lora,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model_lora,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[debug_callback]\n",
    "    )\n",
    "    \n",
    "    # Test model stability\n",
    "    print(\"ğŸ§ª Testing model stability...\")\n",
    "    test_batch = data_collator([tokenized_dataset[0]])\n",
    "    model_device = next(model_lora.parameters()).device\n",
    "    test_batch = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n",
    "    \n",
    "    model_lora.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_lora(\n",
    "            input_ids=test_batch['input_ids'],\n",
    "            attention_mask=test_batch['attention_mask'],\n",
    "            labels=test_batch['labels']\n",
    "        )\n",
    "        \n",
    "        loss_value = outputs.loss.item()\n",
    "        has_nan = torch.isnan(outputs.logits).any().item()\n",
    "        \n",
    "        print(f\"âœ… Model created successfully!\")\n",
    "        print(f\"ğŸ“Š Test loss: {loss_value:.4f}\")\n",
    "        print(f\"ğŸ” Contains NaN: {has_nan}\")\n",
    "        \n",
    "        if has_nan or loss_value != loss_value:  # NaN check\n",
    "            print(\"âš ï¸  Warning: Model shows instability\")\n",
    "        else:\n",
    "            print(\"âœ… Model is stable and ready for training\")\n",
    "    \n",
    "    model_lora.train()\n",
    "    \n",
    "    # Print configuration summary\n",
    "    trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Configuration Summary ({config_name}):\")\n",
    "    print(f\"  LoRA rank: {lora_config.r}\")\n",
    "    print(f\"  LoRA alpha: {lora_config.lora_alpha}\")\n",
    "    print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "    print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    \n",
    "    return model_lora, trainer\n",
    "\n",
    "# Example usage:\n",
    "print(\"ğŸ¯ Usage examples:\")\n",
    "print(\"  model, trainer = create_model_with_config('balanced')\")\n",
    "print(\"  model, trainer = create_model_with_config('aggressive')\")  \n",
    "print(\"  model, trainer = create_model_with_config('conservative')\")\n",
    "print(\"  trainer.train()  # Start training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7879c9d",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Float16 vs BFloat16: Complete Technical Comparison\n",
    "\n",
    "## ğŸ“Š **Bit Representation & Precision**\n",
    "\n",
    "### **Float16 (Half Precision)**\n",
    "- **Total bits**: 16\n",
    "- **Sign bit**: 1\n",
    "- **Exponent**: 5 bits (bias = 15)\n",
    "- **Mantissa**: 10 bits\n",
    "- **Range**: Â±65,504 (limited)\n",
    "- **Precision**: ~3-4 decimal digits\n",
    "\n",
    "### **BFloat16 (Brain Float)**\n",
    "- **Total bits**: 16  \n",
    "- **Sign bit**: 1\n",
    "- **Exponent**: 8 bits (bias = 127) - **Same as Float32!**\n",
    "- **Mantissa**: 7 bits\n",
    "- **Range**: Â±3.4Ã—10Â³â¸ (same as Float32)\n",
    "- **Precision**: ~2-3 decimal digits\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ **Key Trade-offs**\n",
    "\n",
    "| Aspect | Float16 | BFloat16 | Winner |\n",
    "|--------|---------|----------|---------|\n",
    "| **Numerical Range** | Limited (Â±65K) | Wide (Â±3.4Ã—10Â³â¸) | ğŸ† **BFloat16** |\n",
    "| **Precision** | Higher (10-bit mantissa) | Lower (7-bit mantissa) | ğŸ† **Float16** |\n",
    "| **Overflow Risk** | High | Low | ğŸ† **BFloat16** |\n",
    "| **Underflow Risk** | High | Low | ğŸ† **BFloat16** |\n",
    "| **Hardware Support** | Limited | Growing (TPUs, modern GPUs) | ğŸ† **BFloat16** |\n",
    "| **Memory Usage** | Same (16 bits) | Same (16 bits) | ğŸ¤ **Tie** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  **Why BFloat16 is Better for Deep Learning**\n",
    "\n",
    "### **1. Dynamic Range**\n",
    "```\n",
    "Float16:  Can represent values from Â±6Ã—10â»â¸ to Â±65,504\n",
    "BFloat16: Can represent values from Â±1Ã—10â»Â³â¸ to Â±3.4Ã—10Â³â¸\n",
    "```\n",
    "\n",
    "### **2. Gradient Stability**\n",
    "- **Float16**: Gradients often underflow to zero (vanishing gradients)\n",
    "- **BFloat16**: Maintains gradient magnitudes better during backpropagation\n",
    "\n",
    "### **3. Loss Landscape**\n",
    "- **Float16**: Can cause loss spikes due to overflow\n",
    "- **BFloat16**: More stable loss curves, fewer NaN issues\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” **In Your mT5 Training Context**\n",
    "\n",
    "### **Why You Switched from Float16 to BFloat16:**\n",
    "\n",
    "1. **NaN Loss Issue**: Your original `float16` config caused NaN logits\n",
    "2. **Quantization Stability**: 4-bit quantization + float16 = numerical instability\n",
    "3. **LoRA Adaptation**: Low-rank adaptations are sensitive to precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b91562",
   "metadata": {},
   "source": [
    "# ğŸ¯ **When to Use Each Format**\n",
    "\n",
    "## ğŸŸ¢ **Use BFloat16 When:**\n",
    "- Training large language models (like your mT5)\n",
    "- Using quantization (QLoRA, 4-bit, 8-bit)\n",
    "- Working with transformers/attention mechanisms\n",
    "- Need stable gradients\n",
    "- Using modern GPUs (A100, H100, RTX 40xx series)\n",
    "- Training is producing NaN losses with Float16\n",
    "\n",
    "## ğŸŸ¡ **Use Float16 When:**\n",
    "- Training smaller models (CNNs, simple MLPs)\n",
    "- Maximum precision is critical\n",
    "- Working with older hardware\n",
    "- Memory is extremely constrained\n",
    "- Your training is already stable\n",
    "\n",
    "## ğŸ”´ **Avoid Float16 When:**\n",
    "- Training very deep networks\n",
    "- Using aggressive optimizations (quantization + low precision)\n",
    "- Gradients are vanishing\n",
    "- Getting overflow/underflow errors\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ˆ **Performance Impact in Your Case**\n",
    "\n",
    "## **Memory Usage**: \n",
    "- Both use exactly **16 bits per parameter**\n",
    "- No difference in memory consumption\n",
    "\n",
    "## **Speed**:\n",
    "- **Modern GPUs**: BFloat16 often faster (hardware optimized)\n",
    "- **Older GPUs**: Float16 might be slightly faster\n",
    "- **TPUs**: BFloat16 significantly faster (native support)\n",
    "\n",
    "## **Training Stability**:\n",
    "- **Your original issue**: Float16 + 4-bit quantization = NaN disaster\n",
    "- **Current solution**: BFloat16 + 4-bit quantization = stable training âœ…\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”§ **Configuration Impact**\n",
    "\n",
    "```python\n",
    "# âŒ PROBLEMATIC (what you had before)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Caused NaN\n",
    "    bnb_4bit_use_double_quant=True,        # Added instability\n",
    ")\n",
    "training_args = TrainingArguments(fp16=True)  # Amplified the problem\n",
    "\n",
    "# âœ… STABLE (what you have now) \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Stable\n",
    "    bnb_4bit_use_double_quant=False,        # Conservative\n",
    ")\n",
    "training_args = TrainingArguments(bf16=True)   # Consistent\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ† **Bottom Line for Your mT5 Training**\n",
    "\n",
    "**BFloat16 is the superior choice because:**\n",
    "1. **Wider dynamic range** prevents overflow/underflow\n",
    "2. **Better gradient preservation** during backpropagation  \n",
    "3. **More stable with quantization** (your 4-bit setup)\n",
    "4. **Industry standard** for large language model training\n",
    "5. **Hardware optimized** on modern accelerators\n",
    "\n",
    "Your switch from `float16` to `bfloat16` was the key fix that solved your NaN loss problem! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
